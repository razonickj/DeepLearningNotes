\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathrsfs} 
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{bm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{enumitem}  

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
% === MATH COMMANDS ===
% Define bold symbols for vectors and matrices consistently
\newcommand{\vect}[1]{\bm{#1}} % Vector (bold lowercase)
\newcommand{\mat}[1]{\bm{#1}}  % Matrix (bold uppercase)
\newcommand{\R}{\mathbb{R}}    % Real numbers symbol
% Define bold theta
\newcommand{\btheta}{\bm{\theta}}
% Define bold symbols

% Define bold symbols

\newcommand{\bphi}{\bm{\phi}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bz}{\bm{z}}
\newcommand{\ba}{\bm{a}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bc}{\bm{c}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bV}{\mathbf{V}}

% Math operators
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Tr}{Tr}


\newcommand{\bdelta}{\bm{\delta}}



\newcommand{\bSigma}{\bm{\Sigma}}


% Define operators

\DeclareMathOperator{\diag}{diag}  % Diagonal
\DeclareMathOperator{\rank}{rank}  % Rank
% === MATH COMMANDS ===
% Define probability, expectation, variance, covariance
\newcommand{\Prob}{\mathbb{P}} % Probability symbol
\newcommand{\E}{\mathbb{E}}    % Expectation symbol
\DeclareMathOperator{\Var}{Var} % Variance operator
\DeclareMathOperator{\Cov}{Cov} % Covariance operator
\DeclareMathOperator{\KL}{KL}  % KL Divergence operator
\newcommand{\Lik}{\mathcal{L}} % Likelihood symbol
\newcommand{\Indep}{\perp \!\!\! \perp} % Independence symbol

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{Machine Learning Notes (Deep Learning)}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Holehouse Lab} \vspace*{10\baselineskip}}
		}
\date{}
\author{\textbf{Nicholas Razo}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------


\part{Linear Algebra}

\section{Introduction}
This document provides a brief overview of fundamental concepts in linear algebra, covering scalars, vectors, matrices, and various operations and decompositions. Each section introduces the topic, its common symbolism, interpretations, and simple examples.

% ====================
% === SCALARS ===
% ====================
\section{Scalars}

You have \textbf{definitely} seen a scalar before, even in your daily life. When the cashier asks you for $\$71.62$ in exchange for $12$ eggs, both of those numbers are scalars. Most of our intuition for math is based around scalars, and you start out learning how to count and focusing on only scalars. 

\subsection*{Symbolism}
Scalars are typically denoted by lowercase italic letters, often from the Latin alphabet (e.g., $a, b, k, x$) or Greek alphabet (e.g., $\alpha, \beta, \lambda$). They belong to a \textbf{field} (which is just some set of numbers that you can add, subtract, multiply, and divide), most commonly the real numbers ($\R$ - $1, 2, 3, 4, 5.78654, -12.035,$ etc.) or complex numbers ($\mathbb{C}$ - which are the "imaginary" numbers).

\subsection*{Interpretation}
A scalar represents a single numerical quantity, essentially a magnitude without direction (in the context of distinguishing from vectors). In linear algebra, they are used to scale vectors and matrices, meaning they change the magnitude but not the direction (unless the scalar is negative, which reverses the direction). They can also be viewed as the components of a vector or a matrix.

\subsection*{Examples}
\begin{enumerate}
    \item If $a = 5$ and $b = -3.14$, then $a$ and $b$ are scalars.
    \item Scalar addition/subtraction: $5 + (-3.14) = 1.86$. We can think of subtraction as just adding a negative number. We can also go the reverse direction and think of adding a negative number as simply subtraction.
    \item Scalar multiplication: $5 \times (-3.14) = -15.7$. If you really want to do this by hand you can, but calculators are the way to go for this kind of problem in my opinion.
\end{enumerate}

% ====================
% === VECTORS ===
% ====================
\section{Vectors}

\subsection*{Symbolism}
Vectors are often denoted by lowercase bold letters (e.g., $\vect{v}, \vect{u}, \vect{x}$) or sometimes with an arrow above ($\vec{v}$). They are typically represented as column matrices (column vectors):
\[ \vect{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} \]
Here, $v_1, v_2, \dots, v_n$ are the components of the vector, and $n$ is its dimension. A vector in $\R^n$ has $n$ real-valued components. Row vectors are represented as $\vect{v}^T = \begin{bmatrix} v_1 & v_2 & \dots & v_n \end{bmatrix}$.

\subsection*{Interpretation}
Vectors represent quantities that have both magnitude and direction. Geometrically, they can be visualized as arrows originating from the origin (or any point) in a coordinate system. They can also represent points in space or simply an ordered list of numbers. Vectors are fundamental elements of vector spaces.

\subsection*{Examples}
Let $\vect{v} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix}$, $\vect{u} = \begin{bmatrix} 0 \\ 4 \\ 1 \end{bmatrix}$, and $k = 3$.
\begin{enumerate}
    \item Vector Addition:
    \[ \vect{v} + \vect{u} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix} + \begin{bmatrix} 0 \\ 4 \\ 1 \end{bmatrix} = \begin{bmatrix} 1+0 \\ -2+4 \\ 3+1 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \]
    \item Scalar Multiplication:
    \[ k\vect{v} = 3 \vect{v} = 3 \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix} = \begin{bmatrix} 3 \times 1 \\ 3 \times (-2) \\ 3 \times 3 \end{bmatrix} = \begin{bmatrix} 3 \\ -6 \\ 9 \end{bmatrix} \]
\end{enumerate}
% ====================
% === Inner Product ===
% ====================
\section{Inner Product}
You probably have encountered an inner product before in your travels across the math kingdom. The one you might have seen is called the "dot" product. It takes in two vectors and produces a scalar.

\subsection*{Symbolism}
The classic way to symbolize an inner product is simply:
\[
<\cdot, \cdot> : V \times V \rightarrow S
\]
All this means is that you take two vectors out of a vector space $V$ and perform some operation that maps to a scalar. If we have two particular vectors $\vect{x},\vect{y} \in V$, then taking this operator looks like this:
\[
<\vect{x},\vect{y}>
\]
Notice how we have not yet talked about how this value is computed. This is often the case in math. Instead, we focus on what the operation takes in (two vectors), what it produces (a scalar), and any properties that the operation must have. The reason for this, is that proofs are done abstractly, and we can show anything that has the following properties can be used in place of our typical dot product that we might have seen before:
\begin{itemize}
    \item Positive semi-definite: This just means that inner products can not have negative values produced from any two vectors. This is important as you can use this sometimes to bound a result!
    \[
    <\vect{x},\vect{y}> \geq 0
    \]
    \item Symmetric for real-valued vectors. This just means you can flip the order of the input vectors and it will still produce the exact same value!
    \[
    <\vect{x},\vect{y}> = <\vect{y},\vect{x}>
    \]
    \item Linear Operator. This one is the most abstract and really only provides two important insights. The first is that it gives you a way of uncoupling aspects of your equations, meaning that you can often rearrange the symbols in a way that makes them more meaningful to you. The second is very conceptual in that, this statement gets at the idea of superposition (an often used physics principle). This means that the output for some sum of inputs is equal to the sum of the individual outputs.
    \[
    <\vect{x}+\alpha \vect{z},\vect{y}> = <\vect{x},\vect{y}> + \alpha<\vect{z},\vect{y}>
    \]
\end{itemize}

Okay, now that we have covered the overview, lets cover the dot products symbolism (remember that this is just taking the individual values of each element and multiplying them before adding them)
\[
\vect{x} \cdot \vect{y} = \vect{x}^{T}\vect{y}=\sum_{i=1}^{N}x_{i}y_{i}= x_{1}y_{1} + x_{2}y_{2}+...+x_{N}y_{N} =|\vect{x}| |\vect{y}|cos(\theta)
\]

To contrast the dot product, let me show you another inner product that is not the dot product. All I am going to do is modify the formula a little by weighting the pairs of products before summing them:
\[
\vect{x} \cdot \vect{y} = \sum_{i=1}^{N} w_{i}x_{i}y_{i}= w_{1}x_{1}y_{1} + w_{2}x_{2}y_{2}+...+w_{N}x_{N}y_{N}
\]
As long as the weights ($w_{i}$) are all positive, this still maintains its properties (which means any proof for inner products will still hold!), so it is still an inner product.

\subsection*{Interpretation}
There are several different ways to interpret the inner product of two vectors:
\begin{itemize}
    \item The amount that the two vectors point in the same direction
    \item The angle difference between the two vectors
\end{itemize}


\subsection*{Examples}
Let $\vect{x}^{T} = [3,4]; \vect{y}^{T} = [1,2]$. Compute the dot product between them:
\[
\vect{x} \cdot \vect{y} = \sum_{i=1}^{N}x_{i}y_{i} = (3)1 + (4)2 = 3+8 = 11
\]


% ====================
% === MATRICES ===
% ====================
\section{Matrices}

\subsection*{Symbolism}
Matrices are denoted by uppercase bold letters (e.g., $\mat{A}, \mat{B}, \mat{X}$). A matrix with $m$ rows and $n$ columns is called an $m \times n$ matrix. Its elements (entries) are denoted by $A_{ij}$ or $a_{ij}$, where $i$ is the row index and $j$ is the column index.
\[ \mat{A} = \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    a_{21} & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \]

\subsection*{Interpretation}
Matrices are \textbf{rectangular arrays} of numbers (meaning the total number of elements in the matrix can be specified by the number of rows and columns). They can represent:
\begin{itemize}
    \item Systems of linear equations. Each value in a row of the matrix can represent the coefficients of a polynomial. A simple example of a polynomial system is $y=1+3x+2x^{2}$ and $y=2+x+x^{2}$. We can make this a matrix by taking the coefficients and placing them in the rows.
    \[ \begin{bmatrix}
        1 & 3 & 2 \\
        2 & 1 & 1 \\
    \end{bmatrix} \]
    
    \item Linear transformations (functions between \textbf{vector spaces} - a set of vectors with certain vectors included -  that preserve vector addition and scalar multiplication).
    \item Data tables.
    \item Connections in graphs (adjacency matrix).
\end{itemize}
The dimensions ($m \times n$) are crucial for operations like matrix multiplication.

\subsection*{Examples}
Let $\mat{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $\mat{B} = \begin{bmatrix} 0 & -1 \\ 5 & 2 \end{bmatrix}$, and $k = 2$.
\begin{enumerate}
    \item Matrix Addition (must have same dimensions):
    \[ \mat{A} + \mat{B} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} + \begin{bmatrix} 0 & -1 \\ 5 & 2 \end{bmatrix} = \begin{bmatrix} 1+0 & 2+(-1) \\ 3+5 & 4+2 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 8 & 6 \end{bmatrix} \]
    \item Scalar Multiplication:
    \[ k\mat{A} = 2 \mat{A} = 2 \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 2 \times 1 & 2 \times 2 \\ 2 \times 3 & 2 \times 4 \end{bmatrix} = \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix} \]
\end{enumerate}

% ====================
% === TENSORS ===
% ====================
\section{Tensors}

\subsection*{Symbolism}
Tensors generalize scalars, vectors, and matrices to potentially higher dimensions. They are often denoted by bold uppercase letters (like matrices, e.g., $\mat{T}$) or sometimes script ($\mathcal{T}$) or Fraktur letters. Individual elements are accessed using multiple indices, e.g., $T_{ijk...}$. The number of indices is the \emph{rank} (or order) of the tensor.

\subsection*{Interpretation}
\begin{itemize}
    \item A rank-0 tensor is a scalar (0 indices).
    \item A rank-1 tensor is a vector (1 index).
    \item A rank-2 tensor is a matrix (2 indices).
    \item A rank-3 tensor can be thought of as a cube of numbers, and so on.
\end{itemize}
Tensors are used in physics (e.g., stress tensor, electromagnetic tensor), continuum mechanics, general relativity, and machine learning (e.g., representing data with multiple features over time). They represent \textbf{multilinear} relationships between vectors.

\subsection*{Examples}
\begin{enumerate}
    \item A scalar $s$ is a rank-0 tensor.
    \item A vector $\vect{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$ is a rank-1 tensor with elements $v_i$.
    \item A matrix $\mat{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$ is a rank-2 tensor with elements $a_{ij}$.
    \item A rank-3 tensor $\mathcal{T}$ with dimensions $2 \times 2 \times 2$ would have elements $T_{ijk}$ where $i, j, k \in \{1, 2\}$. It can be visualized as two $2 \times 2$ matrices stacked.
\end{enumerate}
Calculations with higher-rank tensors involve tensor contractions and products (e.g., tensor dot product, outer product), which are generalizations of matrix operations.

% ==========================
% === MATRIX MULTIPLICATION ===
% ==========================
\section{Matrix Multiplication}

\subsection*{Symbolism}
The product of an $m \times n$ matrix $\mat{A}$ and an $n \times p$ matrix $\mat{B}$ is an $m \times p$ matrix $\mat{C} = \mat{A}\mat{B}$. The element $C_{ij}$ is calculated as the dot product of the $i$-th row of $\mat{A}$ and the $j$-th column of $\mat{B}$:
\[ C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj} \]
Note: The number of columns in $\mat{A}$ must equal the number of rows in $\mat{B}$. Matrix multiplication is generally not commutative ($\mat{A}\mat{B} \neq \mat{B}\mat{A}$).

\subsection*{Interpretation}
Matrix multiplication corresponds to the composition of linear transformations. If $\mat{A}$ represents transformation $T_A$ and $\mat{B}$ represents $T_B$, then $\mat{A}\mat{B}$ represents applying $T_B$ first, followed by $T_A$.

\subsection*{Examples}
Let $\mat{A} = \begin{bmatrix} 1 & 2 \\ 3 & 0 \end{bmatrix}$ ($2 \times 2$) and $\mat{B} = \begin{bmatrix} 4 & 1 & 0 \\ -1 & 2 & 3 \end{bmatrix}$ ($2 \times 3$). The product $\mat{C} = \mat{A}\mat{B}$ will be a $2 \times 3$ matrix. Picture plucking out the columns of the second matrix and multiplying them by the rows in the first matrix.
\[ \mat{C} = \begin{bmatrix} (1)(4)+(2)(-1) & (1)(1)+(2)(2) & (1)(0)+(2)(3) \\ (3)(4)+(0)(-1) & (3)(1)+(0)(2) & (3)(0)+(0)(3) \end{bmatrix} \]
\[ \mat{C} = \begin{bmatrix} 4-2 & 1+4 & 0+6 \\ 12+0 & 3+0 & 0+0 \end{bmatrix} = \begin{bmatrix} 2 & 5 & 6 \\ 12 & 3 & 0 \end{bmatrix} \]
The product $\mat{B}\mat{A}$ is not defined because the inner dimensions do not match (3 columns in $\mat{B}$, 2 rows in $\mat{A}$).

% ==========================
% === LINEAR OPERATORS ===
% ==========================
\section{Linear Operators (Transformations)}

\subsection*{Symbolism}
A linear operator (or linear map, linear transformation) $T$ is a function between two vector spaces, $T: V \to W$, that satisfies two properties:
\begin{enumerate}
    \item Additivity: $T(\vect{u} + \vect{v}) = T(\vect{u}) + T(\vect{v})$ for all $\vect{u}, \vect{v} \in V$.
    \item Homogeneity: $T(k\vect{v}) = k T(\vect{v})$ for all $\vect{v} \in V$ and scalar $k$.
\end{enumerate}
If $V = \R^n$ and $W = \R^m$, then $T$ can be represented by an $m \times n$ matrix $\mat{A}$ such that $T(\vect{x}) = \mat{A}\vect{x}$.

\subsection*{Interpretation}
Linear operators describe transformations of vectors that preserve the structure of the vector space (lines remain lines, the origin maps to the origin). Geometrically, in $\R^2$ or $\R^3$, they represent operations like rotations, reflections, scaling, shearing, and projections.

\subsection*{Examples}
\begin{enumerate}
    \item Rotation in $\R^2$: The matrix $\mat{R} = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}$ represents a counter-clockwise rotation by angle $\theta$. Let $\theta = 90^\circ (\pi/2)$, so $\cos \theta = 0, \sin \theta = 1$. $\mat{R}_{90} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$. Applying this to $\vect{v} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$:
    \[ T(\vect{v}) = \mat{R}_{90}\vect{v} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} (0)(2)+(-1)(1) \\ (1)(2)+(0)(1) \end{bmatrix} = \begin{bmatrix} -1 \\ 2 \end{bmatrix} \]
    The vector $\begin{bmatrix} 2 \\ 1 \end{bmatrix}$ is rotated 90 degrees counter-clockwise to $\begin{bmatrix} -1 \\ 2 \end{bmatrix}$.

    \item Scaling in $\R^2$: The matrix $\mat{S} = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix}$ scales vectors by a factor of 2 in the x-direction and 0.5 in the y-direction. Applying this to $\vect{v} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$:
     \[ T(\vect{v}) = \mat{S}\vect{v} = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} (2)(2)+(0)(1) \\ (0)(2)+(0.5)(1) \end{bmatrix} = \begin{bmatrix} 4 \\ 0.5 \end{bmatrix} \]
\end{enumerate}

% ==========================
% === IDENTITY MATRICES ===
% ==========================
\section{Identity Matrices}

\subsection*{Symbolism}
The identity matrix of size $n \times n$ is denoted by $\mat{I}_n$ or simply $\mat{I}$ if the size is clear from context. It is a square matrix with ones on the main diagonal and zeros elsewhere. $I_{ij} = \delta_{ij}$ (Kronecker delta).
\[ \mat{I}_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad \mat{I}_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \]

\subsection*{Interpretation}
The identity matrix represents the "identity" transformation, meaning it leaves any vector unchanged when applied: $\mat{I}\vect{v} = \vect{v}$. It is the multiplicative identity element in matrix multiplication: for any $m \times n$ matrix $\mat{A}$, $\mat{I}_m \mat{A} = \mat{A}$ and $\mat{A} \mat{I}_n = \mat{A}$.

\subsection*{Examples}
Let $\mat{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ and $\vect{v} = \begin{bmatrix} 5 \\ -1 \end{bmatrix}$.
\begin{enumerate}
    \item $\mat{A}\mat{I}_2 = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} (1)(1)+(2)(0) & (1)(0)+(2)(1) \\ (3)(1)+(4)(0) & (3)(0)+(4)(1) \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \mat{A}$.
    \item $\mat{I}_2\mat{A} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} (1)(1)+(0)(3) & (1)(2)+(0)(4) \\ (0)(1)+(1)(3) & (0)(2)+(1)(4) \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \mat{A}$.
    \item $\mat{I}_2\vect{v} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 5 \\ -1 \end{bmatrix} = \begin{bmatrix} (1)(5)+(0)(-1) \\ (0)(5)+(1)(-1) \end{bmatrix} = \begin{bmatrix} 5 \\ -1 \end{bmatrix} = \vect{v}$.
\end{enumerate}

% ======================================
% === MATRIX MULTIPLICATIVE INVERSES ===
% ======================================
\section{Matrix Multiplicative Inverses}

\subsection*{Symbolism}
The inverse of a square matrix $\mat{A}$ is denoted by $\mat{A}^{-1}$. It is defined as the matrix such that:
\[ \mat{A}\mat{A}^{-1} = \mat{A}^{-1}\mat{A} = \mat{I} \]
A matrix must be square to have an inverse, but not all square matrices have one. Matrices that have an inverse are called \emph{invertible} or \emph{non-singular}.

\subsection*{Interpretation}
If $\mat{A}$ represents a linear transformation, $\mat{A}^{-1}$ represents the transformation that "undoes" the effect of $\mat{A}$. If $T(\vect{x}) = \mat{A}\vect{x}$, then $T^{-1}(\vect{y}) = \mat{A}^{-1}\vect{y}$. The inverse exists if and only if the transformation does not collapse the space into a lower dimension (i.e., $\det(\mat{A}) \neq 0$). Inverses are crucial for solving systems of linear equations of the form $\mat{A}\vect{x} = \vect{b}$, for which the solution is $\vect{x} = \mat{A}^{-1}\vect{b}$ (if $\mat{A}$ is invertible).

\subsection*{Examples}
\begin{enumerate}
    \item Inverse of a $2 \times 2$ matrix: For $\mat{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, the inverse is
    \[ \mat{A}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \]
    provided the determinant $ad-bc \neq 0$.
    Let $\mat{A} = \begin{bmatrix} 4 & 7 \\ 2 & 6 \end{bmatrix}$. The determinant is $\det(\mat{A}) = (4)(6) - (7)(2) = 24 - 14 = 10$. Since $10 \neq 0$, the inverse exists.
    \[ \mat{A}^{-1} = \frac{1}{10} \begin{bmatrix} 6 & -7 \\ -2 & 4 \end{bmatrix} = \begin{bmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{bmatrix} \]
    Check: $\mat{A}\mat{A}^{-1} = \begin{bmatrix} 4 & 7 \\ 2 & 6 \end{bmatrix} \begin{bmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{bmatrix} = \begin{bmatrix} (2.4-1.4) & (-2.8+2.8) \\ (1.2-1.2) & (-1.4+2.4) \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \mat{I}$.

    \item A singular matrix: Let $\mat{B} = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}$. The determinant is $\det(\mat{B}) = (1)(4) - (2)(2) = 4 - 4 = 0$. $\mat{B}$ is singular and does not have an inverse.
\end{enumerate}

% ====================
% === NORMS ===
% ====================
\section{Norms}
Norms are mostly just a fancy way of saying magnitude. When you think of the magnitude of a vector (length) you are thinking of a norm. Much like inner products took two different vectors and produced scalar number, norms take in a single vector and produce a number. A simple way to form norms is to take an inner product (like the dot product) and make both of the vectors be the same one. It should be no surprise that many norms are based on taking an inner product.

\subsection*{Symbolism}
A norm is a function that assigns a strictly positive length or size (magnitude) to each vector in a vector space (just think of this as a set of vectors that you then have a few restrictions on), except for the zero vector ($[0,0,0,...,0]$), which has zero length. The norm of a vector $\vect{v}$ is denoted by $||\vect{v}||$. These restrictions can be written out more mathematically as:
\begin{itemize}
    \item $||\vect{x}|| = 0 \implies \vect{x} = \vect{0}$
    \item Non negative: $||\vect{x}|| \geq 0$
    \item Triangle Inequality/Subaddative. This one is best visualize. Draw out a right triangle and make its hypotenuse be $\vect{x} + \vect{y}$. You will quickly see that the sum of the length of the other two lines is longer than the hypotenuse.
    \[
    ||\vect{x} + \vect{y}|| \leq ||\vect{x}|| + ||\vect{y}||
    \]
\end{itemize}
Common vector norms include:
\begin{itemize}
    \item $L_p$ norm: $||\vect{v}||_p = \left( \sum_{i=1}^n |v_i|^p \right)^{1/p}$ for $p \ge 1$. This is by far the most common. When you typically talk about the magnitude of a vector you are talking about $L_{2}$ ($L_{p}$ when $p=2$).
    \item $L_2$ norm (Euclidean norm): $||\vect{v}||_2 = \sqrt{\sum_{i=1}^n v_i^2} = \sqrt{\vect{v}^T \vect{v}}$. Often written as just $||\vect{v}||$. This one is so important in most applications that I decided to make it its own bullet point
    \item $L_1$ norm (Manhattan norm): $||\vect{v}||_1 = \sum_{i=1}^n |v_i|$.
    \item $L_\infty$ norm (Max norm): $||\vect{v}||_\infty = \max_{i} |v_i|$. Yes it is this simple. 
\end{itemize}
Matrix norms also exist, denoted $||\mat{A}||$, measuring the "size" of a matrix, often interpreted as the maximum factor by which the matrix can stretch a vector. Example: Frobenius norm $||\mat{A}||_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}$.

\subsection*{Interpretation}
Norms provide a notion of distance or length. The $L_2$ norm corresponds to the standard Euclidean distance from the origin. The $L_1$ norm represents the distance travelled along grid lines (like city blocks). The $L_\infty$ norm gives the maximum absolute component value. Norms are fundamental in analysis, optimization (measuring error), and machine learning (regularization).

\subsection*{Examples}
Let $\vect{v} = \begin{bmatrix} 3 \\ -4 \\ 0 \end{bmatrix}$.
\begin{enumerate}
    \item $L_2$ norm: $||\vect{v}||_2 = \sqrt{3^2 + (-4)^2 + 0^2} = \sqrt{9 + 16 + 0} = \sqrt{25} = 5$.
    \item $L_1$ norm: $||\vect{v}||_1 = |3| + |-4| + |0| = 3 + 4 + 0 = 7$.
    \item $L_\infty$ norm: $||\vect{v}||_\infty = \max(|3|, |-4|, |0|) = \max(3, 4, 0) = 4$.
\end{enumerate}
Let $\mat{A} = \begin{bmatrix} 1 & -2 \\ 0 & 3 \end{bmatrix}$.
\begin{enumerate}
    \item Frobenius norm: $||\mat{A}||_F = \sqrt{1^2 + (-2)^2 + 0^2 + 3^2} = \sqrt{1 + 4 + 0 + 9} = \sqrt{14}$.
\end{enumerate}

% ====================
% === DETERMINANTS ===
% ====================
\section{Determinants}
we briefly touched on norms for matrices, but an arguably more important quantification is the determinant. Since we can often view matrices as transformations/operations on vector spaces, we sometimes want to quantify how this transformation transforms space. 

\subsection*{Symbolism}
The determinant is a scalar value associated with a square matrix $\mat{A}$, denoted $\det(\mat{A})$ or $|\mat{A}|$. PLEASE note how this is a single bar rather than the norm which was a double bar.

\subsection*{Interpretation}
The determinant has several interpretations:
\begin{itemize}
    \item Geometric: It represents the signed scaling factor by which the linear transformation associated with $\mat{A}$ changes area (in 2D), volume (in 3D), or hypervolume (in higher dimensions). A negative determinant indicates a change in orientation (e.g., reflection). In the special case when the determinant is $0$, this means we have lost all hypervolume. In 3D, this might mean that the matrix maps all points onto a plane. It could also map 3D onto a line or a single point. Having a determinant of $0$ does not tell you how many dimensions were lost in the transformation only that there was at least one dimension lost.
    \item Algebraic: $\det(\mat{A}) \neq 0$ if and only if (necessary and sufficient) the matrix $\mat{A}$ is invertible (non-singular). If $\det(\mat{A}) = 0$, the matrix is singular, its columns (and rows) are linearly dependent, and the transformation collapses the space onto a lower dimension. This means this transformation loses information as it is not invertible. Basically determinants are a convenient way to get a binary (yes or no/true or false/1 or 0) about if a transformation conserves all information in the input vector space.
\end{itemize}
When working with matrices, sometimes we will want to work with matrix factors (think of this as a sort of prime factorization of a real number but for matrixes). This means that taking the determinant of a product of matrixes is sometimes needed. Similarly, this is also true for inverses and transposes. So we may need the following algebraic identities to work with these:

\[
    \det(\mat{A}\mat{B}) = \det(\mat{A})\det(\mat{B})
\]
\[
\det(\mat{A}^{-1}) = 1/\det(\mat{A})
\]
\[
\det(\mat{A}^T) = \det(\mat{A})
\]

\subsection*{Examples}
To be frank, I do not see knowing how to compute these as being very useful. It is basically a formula called \textbf{cofactor expansion} (there are a couple of other methods as well), but I will not cover that here. These calculations are long and error prone - great place for a computer!
\begin{enumerate}
    \item $2 \times 2$ matrix: For $\mat{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, $\det(\mat{A}) = ad - bc$.
    Let $\mat{A} = \begin{bmatrix} 4 & 7 \\ 2 & 6 \end{bmatrix}$. $\det(\mat{A}) = (4)(6) - (7)(2) = 24 - 14 = 10$. (Invertible)
    Let $\mat{B} = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}$. $\det(\mat{B}) = (1)(4) - (2)(2) = 4 - 4 = 0$. (Singular)

    \item $3 \times 3$ matrix (using cofactor expansion along first row): For $\mat{C} = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}$,
    \[ \det(\mat{C}) = a \begin{vmatrix} e & f \\ h & i \end{vmatrix} - b \begin{vmatrix} d & f \\ g & i \end{vmatrix} + c \begin{vmatrix} d & e \\ g & h \end{vmatrix} \]
    \[ = a(ei - fh) - b(di - fg) + c(dh - eg) \]
    Let $\mat{C} = \begin{bmatrix} 1 & 2 & 0 \\ 3 & -1 & 2 \\ 2 & 0 & 1 \end{bmatrix}$.
    \[ \det(\mat{C}) = 1 \begin{vmatrix} -1 & 2 \\ 0 & 1 \end{vmatrix} - 2 \begin{vmatrix} 3 & 2 \\ 2 & 1 \end{vmatrix} + 0 \begin{vmatrix} 3 & -1 \\ 2 & 0 \end{vmatrix} \]
    \[ = 1((-1)(1) - (2)(0)) - 2((3)(1) - (2)(2)) + 0 \]
    \[ = 1(-1 - 0) - 2(3 - 4) = -1 - 2(-1) = -1 + 2 = 1 \]
\end{enumerate}

% ====================
% === TRACE ===
% ====================
\section{Trace}

\subsection*{Symbolism}
The trace of a square matrix $\mat{A}$ (denoted $\Tr(\mat{A})$) is the sum of the elements on its main diagonal.
\[ \Tr(\mat{A}) = \sum_{i=1}^{n} A_{ii} \]

\subsection*{Interpretation}
The trace is a simple linear operator on the space of square matrices ($\Tr(\mat{A}+\mat{B}) = \Tr(\mat{A}) + \Tr(\mat{B})$, $\Tr(c\mat{A}) = c\Tr(\mat{A})$). It is invariant under cyclic permutations: $\Tr(\mat{A}\mat{B}\mat{C}) = \Tr(\mat{C}\mat{A}\mat{B}) = \Tr(\mat{B}\mat{C}\mat{A})$. A key property is $\Tr(\mat{A}\mat{B}) = \Tr(\mat{B}\mat{A})$. The trace is also equal to the sum of the eigenvalues of the matrix.

\subsection*{Examples}
Let $\mat{A} = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$.
\[ \Tr(\mat{A}) = 1 + 5 + 9 = 15 \]
Let $\mat{B} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$.
\[ \Tr(\mat{B}) = 0 + 0 = 0 \]

% ==========================
% === DIAGONAL MATRICES ===
% ==========================
\section{Diagonal Matrices}

\subsection*{Symbolism}
A square matrix $\mat{D}$ is called diagonal if all its off-diagonal elements are zero ($D_{ij} = 0$ for $i \neq j$). It is often written as $\mat{D} = \diag(d_1, d_2, \dots, d_n)$.
\[ \mat{D} = \begin{bmatrix} d_1 & 0 & \dots & 0 \\ 0 & d_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & d_n \end{bmatrix} \]

\subsection*{Interpretation}
A diagonal matrix represents a linear transformation that scales the space along the coordinate axes. This is so important that I want to say this again: \textbf{Diagonal matrices only scale vectors!} The $i$-th basis vector is scaled by the factor $d_i$. Another important property is that computations involving diagonal matrices are very simple. The following will become useful when you think about the implementation of machine learning algorithms:
\begin{itemize}
    \item Multiplication: $\diag(a_1, \dots, a_n) \diag(b_1, \dots, b_n) = \diag(a_1 b_1, \dots, a_n b_n)$.
    \item Inverse: $\diag(d_1, \dots, d_n)^{-1} = \diag(1/d_1, \dots, 1/d_n)$ (if all $d_i \neq 0$).
    \item Determinant: $\det(\diag(d_1, \dots, d_n)) = d_1 d_2 \dots d_n$.
    \item Powers: $\diag(d_1, \dots, d_n)^k = \diag(d_1^k, \dots, d_n^k)$.
\end{itemize}

\subsection*{Examples}
Let $\mat{D} = \diag(2, -1, 3) = \begin{bmatrix} 2 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$. Let $\vect{v} = \begin{bmatrix} 1 \\ 4 \\ -2 \end{bmatrix}$.
\begin{enumerate}
    \item Action on a vector:
    \[ \mat{D}\vect{v} = \begin{bmatrix} 2 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \\ -2 \end{bmatrix} = \begin{bmatrix} 2(1) \\ -1(4) \\ 3(-2) \end{bmatrix} = \begin{bmatrix} 2 \\ -4 \\ -6 \end{bmatrix} \]
    Each component of $\vect{v}$ is scaled by the corresponding diagonal entry of $\mat{D}$.
    \item Determinant: $\det(\mat{D}) = 2 \times (-1) \times 3 = -6$.
    \item Inverse: $\mat{D}^{-1} = \diag(1/2, 1/(-1), 1/3) = \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix}$.
\end{enumerate}

% ===========================
% === SYMMETRIC MATRICES ===
% ===========================
\section{Symmetric Matrices}

\subsection*{Symbolism}
A square matrix $\mat{A}$ is symmetric if it is equal to its transpose, i.e., $\mat{A} = \mat{A}^T$. This means $A_{ij} = A_{ji}$ for all $i, j$.

\subsection*{Interpretation}
Symmetric matrices arise naturally in many areas, such as in quadratic forms ($\vect{x}^T \mat{A} \vect{x}$ - notice how this is similar to $x^{2}$ which is often called a quadratic term), covariance matrices in statistics, and adjacency matrices of undirected graphs. They have very important properties, most notably:
\begin{itemize}
    \item All eigenvalues of a real symmetric matrix are real.
    \item Eigenvectors corresponding to distinct eigenvalues are orthogonal.
    \item A real symmetric matrix is always orthogonally diagonalizable (Spectral Theorem).
\end{itemize}

\subsection*{Examples}
\begin{enumerate}
    \item $\mat{A} = \begin{bmatrix} 1 & 2 & -1 \\ 2 & 5 & 0 \\ -1 & 0 & 3 \end{bmatrix}$ is symmetric because $\mat{A}^T = \begin{bmatrix} 1 & 2 & -1 \\ 2 & 5 & 0 \\ -1 & 0 & 3 \end{bmatrix} = \mat{A}$.
    \item $\mat{B} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ is not symmetric because $\mat{B}^T = \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} \neq \mat{B}$.
    \item Any diagonal matrix is symmetric.
\end{enumerate}

% ======================
% === ORTHOGONALITY ===
% ======================
\section{Orthogonality (Vectors)}

\subsection*{Symbolism}
Two vectors $\vect{u}$ and $\vect{v}$ in an inner product space (like $\R^n$ with the dot product) are orthogonal if their inner product is zero. For real vectors using the standard dot product, this is written as $\vect{u} \cdot \vect{v} = 0$ or $\vect{u}^T \vect{v} = 0$.

\subsection*{Interpretation}
Orthogonal vectors are geometrically perpendicular to each other (meet at a 90-degree angle). A set of vectors $\{\vect{v}_1, \dots, \vect{v}_k\}$ is called an orthogonal set if every pair of distinct vectors in the set is orthogonal ($\vect{v}_i \cdot \vect{v}_j = 0$ for $i \neq j$). Orthogonal sets of non-zero vectors are always linearly independent.

\subsection*{Examples}
Let $\vect{u} = \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}$, $\vect{v} = \begin{bmatrix} 3 \\ -1 \\ 1 \end{bmatrix}$, $\vect{w} = \begin{bmatrix} 1 \\ 1 \\ 3 \end{bmatrix}$.
\begin{enumerate}
    \item Check $\vect{u}$ and $\vect{v}$:
    \[ \vect{u}^T \vect{v} = (1)(3) + (2)(-1) + (-1)(1) = 3 - 2 - 1 = 0 \]
    So, $\vect{u}$ and $\vect{v}$ are orthogonal.
    \item Check $\vect{u}$ and $\vect{w}$:
    \[ \vect{u}^T \vect{w} = (1)(1) + (2)(1) + (-1)(3) = 1 + 2 - 3 = 0 \]
    So, $\vect{u}$ and $\vect{w}$ are orthogonal.
    \item Check $\vect{v}$ and $\vect{w}$:
    \[ \vect{v}^T \vect{w} = (3)(1) + (-1)(1) + (1)(3) = 3 - 1 + 3 = 5 \]
    So, $\vect{v}$ and $\vect{w}$ are not orthogonal.
\end{enumerate}
The set $\{\vect{u}, \vect{v}\}$ is an orthogonal set. The set $\{\vect{u}, \vect{v}, \vect{w}\}$ is not.

% ====================
% === NORMALITY ===
% ====================
\section{Normality (Unit Vectors)}

\subsection*{Symbolism}
A vector $\vect{v}$ is called a normal vector or, more commonly, a \emph{unit vector} if its norm (usually the $L_2$ norm) is equal to 1.
\[ ||\vect{v}|| = ||\vect{v}||_2 = 1 \]
Any non-zero vector $\vect{u}$ can be normalized to create a unit vector $\hat{\vect{u}}$ pointing in the same direction:
\[ \hat{\vect{u}} = \frac{\vect{u}}{||\vect{u}||} \]

\subsection*{Interpretation}
Unit vectors specify direction without magnitude. They form the basis for coordinate systems (e.g., $\vect{i}, \vect{j}, \vect{k}$ in $\R^3$). Normalizing vectors is common when only the direction is relevant.

\subsection*{Examples}
\begin{enumerate}
    \item Let $\vect{u} = \begin{bmatrix} 3 \\ 0 \\ -4 \end{bmatrix}$.
    Calculate its norm: $||\vect{u}||_2 = \sqrt{3^2 + 0^2 + (-4)^2} = \sqrt{9 + 0 + 16} = \sqrt{25} = 5$.
    Normalize $\vect{u}$:
    \[ \hat{\vect{u}} = \frac{1}{5} \begin{bmatrix} 3 \\ 0 \\ -4 \end{bmatrix} = \begin{bmatrix} 3/5 \\ 0 \\ -4/5 \end{bmatrix} \]
    Check the norm of $\hat{\vect{u}}$: $||\hat{\vect{u}}||_2 = \sqrt{(3/5)^2 + 0^2 + (-4/5)^2} = \sqrt{9/25 + 16/25} = \sqrt{25/25} = 1$.

    \item The standard basis vector $\vect{e}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ is a unit vector since $||\vect{e}_1||_2 = \sqrt{1^2+0^2+0^2} = 1$.
\end{enumerate}

% ==========================
% === ORTHOGONAL MATRICES ===
% ==========================
\section{Orthogonal Matrices}

\subsection*{Symbolism}
A square matrix $\mat{Q}$ is called an orthogonal matrix if its columns form an \emph{orthonormal set} (i.e., they are mutually orthogonal unit vectors). Equivalently, a matrix $\mat{Q}$ is orthogonal if its transpose is equal to its inverse:
\[ \mat{Q}^T \mat{Q} = \mat{Q} \mat{Q}^T = \mat{I} \quad \iff \quad \mat{Q}^{-1} = \mat{Q}^T \]

\subsection*{Interpretation}
Orthogonal matrices represent linear transformations that preserve lengths (isometries) and angles between vectors. Geometrically, these transformations correspond to rotations and reflections (or combinations thereof).
Properties:
\begin{itemize}
    \item $||\mat{Q}\vect{x}||_2 = ||\vect{x}||_2$ for any vector $\vect{x}$.
    \item $(\mat{Q}\vect{x}) \cdot (\mat{Q}\vect{y}) = \vect{x} \cdot \vect{y}$ for any vectors $\vect{x}, \vect{y}$.
    \item The determinant of an orthogonal matrix is always $\det(\mat{Q}) = \pm 1$. (+1 for pure rotations, -1 for reflections).
    \item The rows of $\mat{Q}$ also form an orthonormal set.
\end{itemize}

\subsection*{Examples}
\begin{enumerate}
    \item Rotation matrix in $\R^2$: $\mat{R}_{90} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$.
    Columns are $\vect{q}_1 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ and $\vect{q}_2 = \begin{bmatrix} -1 \\ 0 \end{bmatrix}$.
    $||\vect{q}_1|| = 1$, $||\vect{q}_2|| = 1$.
    $\vect{q}_1 \cdot \vect{q}_2 = (0)(-1) + (1)(0) = 0$.
    The columns form an orthonormal set. Check $\mat{Q}^T \mat{Q} = \mat{I}$:
    \[ \mat{R}_{90}^T \mat{R}_{90} = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} (0)(0)+(1)(1) & (0)(-1)+(1)(0) \\ (-1)(0)+(0)(1) & (-1)(-1)+(0)(0) \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \mat{I} \]
    So $\mat{R}_{90}$ is orthogonal. $\det(\mat{R}_{90}) = (0)(0) - (-1)(1) = 1$.

    \item Permutation matrix: $\mat{P} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$. Columns are $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$, which are orthonormal.
    \[ \mat{P}^T \mat{P} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \mat{I} \]
    So $\mat{P}$ is orthogonal. $\det(\mat{P}) = (0)(0) - (1)(1) = -1$ (represents a reflection across the line $y=x$).
\end{enumerate}

% ======================================
% === EIGENVALUE DECOMPOSITION (EVD) ===
% ======================================
\section{Eigenvalue Decomposition (Eigendecomposition)}

\subsection*{Symbolism}
For a square matrix $\mat{A}$, a non-zero vector $\vect{v}$ is an eigenvector and a scalar $\lambda$ is its corresponding eigenvalue if they satisfy the equation:
\[ \mat{A}\vect{v} = \lambda \vect{v} \]
The eigendecomposition of a diagonalizable matrix $\mat{A}$ is given by:
\[ \mat{A} = \mat{Q} \mat{\Lambda} \mat{Q}^{-1} \]
where $\mat{Q}$ is an invertible matrix whose columns are the eigenvectors of $\mat{A}$, and $\mat{\Lambda}$ (Lambda) is a diagonal matrix whose diagonal entries are the corresponding eigenvalues $\Lambda_{ii} = \lambda_i$.
If $\mat{A}$ is symmetric, then $\mat{Q}$ can be chosen to be an orthogonal matrix ($\mat{Q}^{-1} = \mat{Q}^T$), and the decomposition is $\mat{A} = \mat{Q} \mat{\Lambda} \mat{Q}^T$.

\subsection*{Interpretation}
Eigenvectors represent directions that are unchanged (only scaled) by the linear transformation $\mat{A}$. The eigenvalue $\lambda$ is the factor by which the eigenvector $\vect{v}$ is scaled. The eigendecomposition breaks down the matrix $\mat{A}$ into its fundamental directional scaling actions. It simplifies matrix powers ($\mat{A}^k = \mat{Q} \mat{\Lambda}^k \mat{Q}^{-1}$) and understanding the matrix's behavior. Not all square matrices are diagonalizable (i.e., have a full set of linearly independent eigenvectors). Symmetric matrices are always orthogonally diagonalizable.

\subsection*{Examples}
Let $\mat{A} = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$ (symmetric).
\begin{enumerate}
    \item Find eigenvalues: Solve $\det(\mat{A} - \lambda \mat{I}) = 0$.
    \[ \det \begin{bmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{bmatrix} = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 4 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda - 3)(\lambda - 1) = 0 \]
    The eigenvalues are $\lambda_1 = 3$ and $\lambda_2 = 1$. $\mat{\Lambda} = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}$.
    \item Find eigenvectors:
    For $\lambda_1 = 3$: $(\mat{A} - 3\mat{I})\vect{v} = \vect{0} \implies \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$. This gives $-v_1 + v_2 = 0$, so $v_1 = v_2$. An eigenvector is $\vect{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
    For $\lambda_2 = 1$: $(\mat{A} - 1\mat{I})\vect{v} = \vect{0} \implies \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$. This gives $v_1 + v_2 = 0$, so $v_1 = -v_2$. An eigenvector is $\vect{v}_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
    \item Normalize eigenvectors: $||\vect{v}_1|| = \sqrt{1^2+1^2} = \sqrt{2}$. $\hat{\vect{v}}_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
    $||\vect{v}_2|| = \sqrt{1^2+(-1)^2} = \sqrt{2}$. $\hat{\vect{v}}_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
    \item Form orthogonal matrix $\mat{Q}$: $\mat{Q} = \begin{bmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{bmatrix}$.
    \item Decomposition: $\mat{A} = \mat{Q}\mat{\Lambda}\mat{Q}^T$.
    \[ \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} = \begin{bmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{bmatrix} \]
    (Verification left as an exercise).
\end{enumerate}

% ===========================================
% === SINGULAR VALUE DECOMPOSITION (SVD) ===
% ===========================================
\section{Singular Value Decomposition (SVD)}

\subsection*{Symbolism}
Any $m \times n$ matrix $\mat{A}$ can be factorized as:
\[ \mat{A} = \mat{U} \mat{\Sigma} \mat{V}^T \]
where:
\begin{itemize}
    \item $\mat{U}$ is an $m \times m$ orthogonal matrix. Its columns ($\vect{u}_i$) are the left singular vectors (eigenvectors of $\mat{A}\mat{A}^T$).
    \item $\mat{\Sigma}$ (Sigma) is an $m \times n$ diagonal matrix (padded with zeros if not square). The diagonal entries $\sigma_i = \Sigma_{ii}$ are the singular values of $\mat{A}$, usually sorted in descending order ($\sigma_1 \ge \sigma_2 \ge \dots \ge 0$). They are the square roots of the non-zero eigenvalues of both $\mat{A}\mat{A}^T$ and $\mat{A}^T\mat{A}$.
    \item $\mat{V}$ is an $n \times n$ orthogonal matrix. Its columns ($\vect{v}_i$) are the right singular vectors (eigenvectors of $\mat{A}^T\mat{A}$). $\mat{V}^T$ is its transpose.
\end{itemize}
We have $\mat{A}\vect{v}_i = \sigma_i \vect{u}_i$ and $\mat{A}^T\vect{u}_i = \sigma_i \vect{v}_i$.

\subsection*{Interpretation}
SVD provides a powerful decomposition applicable to *any* matrix, square or rectangular. It decomposes the action of $\mat{A}$ into three steps:
1. A rotation/reflection ($\mat{V}^T$ changes basis in the domain).
2. A scaling along the new axes ($\mat{\Sigma}$ scales the dimensions).
3. Another rotation/reflection ($\mat{U}$ changes basis in the codomain).
The singular values $\sigma_i$ represent the "magnitude" of the action of $\mat{A}$ along the principal directions defined by the singular vectors. SVD is fundamental in many applications, including dimensionality reduction (PCA), image compression, noise reduction, recommender systems, and calculating the pseudoinverse.

\subsection*{Examples}
Calculating SVD manually is often complex. Here's a conceptual example and a simple numeric case.
\begin{enumerate}
    \item Conceptual: Consider a transformation $\mat{A}$ that squashes a circle into an ellipse. $\mat{V}^T$ rotates the space so the ellipse's major/minor axes align with the coordinate axes. $\mat{\Sigma}$ scales along these axes (the singular values are the lengths of the semi-axes). $\mat{U}$ rotates the resulting ellipse into its final position.

    \item Simple Diagonal Case: Let $\mat{A} = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}$.
    $\mat{A}^T\mat{A} = \begin{bmatrix} 9 & 0 \\ 0 & 4 \end{bmatrix}$. Eigenvalues 9, 4. Singular values $\sigma_1 = \sqrt{9}=3$, $\sigma_2 = \sqrt{4}=2$. $\mat{\Sigma} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}$.
    Eigenvectors of $\mat{A}^T\mat{A}$ are $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ ($\lambda=9$) and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ ($\lambda=4$). So $\mat{V} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
    $\mat{A}\mat{A}^T = \begin{bmatrix} 9 & 0 \\ 0 & 4 \end{bmatrix}$. Eigenvectors are $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ ($\lambda=9$) and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ ($\lambda=4$).
    Need to find $\mat{U}$ such that $\mat{A}\vect{v}_i = \sigma_i \vect{u}_i$.
    $\mat{A}\vect{v}_1 = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \end{bmatrix}$. We need $3 \vect{u}_1 = \begin{bmatrix} 3 \\ 0 \end{bmatrix}$, so $\vect{u}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
    $\mat{A}\vect{v}_2 = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ -2 \end{bmatrix}$. We need $2 \vect{u}_2 = \begin{bmatrix} 0 \\ -2 \end{bmatrix}$, so $\vect{u}_2 = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$.
    So $\mat{U} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$.
    The SVD is $\mat{A} = \mat{U}\mat{\Sigma}\mat{V}^T = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^T$.
    Note: The negative sign in $\mat{A}$ is absorbed into $\mat{U}$, keeping $\mat{\Sigma}$ non-negative.
\end{enumerate}

% ======================================
% === MOORE-PENROSE PSEUDOINVERSE ===
% ======================================
\section{Moore-Penrose Pseudoinverse}

\subsection*{Symbolism}
The Moore-Penrose pseudoinverse of a matrix $\mat{A}$ (which can be non-square or singular) is denoted by $\mat{A}^{+}$.

\subsection*{Interpretation}
The pseudoinverse generalizes the concept of a matrix inverse to matrices that are not invertible in the usual sense. It provides a unique matrix $\mat{A}^{+}$ satisfying four criteria (Penrose conditions). Its most common use is finding the best approximate solution (in a least-squares sense) to a system of linear equations $\mat{A}\vect{x} = \vect{b}$ that may not have an exact solution. The vector $\vect{x}^{+} = \mat{A}^{+}\vect{b}$ minimizes the squared $L_2$ norm of the residual $||\mat{A}\vect{x} - \vect{b}||_2^2$. If multiple solutions achieve this minimum error, $\vect{x}^{+}$ is the one with the smallest $L_2$ norm $||\vect{x}||_2^2$. If $\mat{A}$ is invertible, then $\mat{A}^{+} = \mat{A}^{-1}$.

\subsection*{Calculation using SVD}
The most stable way to compute the pseudoinverse is via SVD. If $\mat{A} = \mat{U} \mat{\Sigma} \mat{V}^T$ is the SVD of $\mat{A}$, then the pseudoinverse is:
\[ \mat{A}^{+} = \mat{V} \mat{\Sigma}^{+} \mat{U}^T \]
where $\mat{\Sigma}^{+}$ is obtained from $\mat{\Sigma}$ (which is $m \times n$) by:
1. Taking the reciprocal of all non-zero singular values on the diagonal.
2. Transposing the resulting matrix (making it $n \times m$).

\subsection*{Examples}
\begin{enumerate}
    \item Singular Square Matrix: Let $\mat{A} = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}$. $\det(\mat{A}) = 0$.
    SVD (calculations omitted, typically done numerically):
    $\mat{A} \approx \begin{bmatrix} 0.447 & -0.894 \\ 0.894 & 0.447 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0.447 & 0.894 \\ -0.894 & 0.447 \end{bmatrix}^T$.
    So $\mat{U} \approx \begin{bmatrix} 0.447 & -0.894 \\ 0.894 & 0.447 \end{bmatrix}$, $\mat{\Sigma} = \begin{bmatrix} 5 & 0 \\ 0 & 0 \end{bmatrix}$, $\mat{V} \approx \begin{bmatrix} 0.447 & -0.894 \\ 0.894 & 0.447 \end{bmatrix}$.
    Calculate $\mat{\Sigma}^{+}$: Reciprocal of non-zero singular values is $1/5$. Transpose $\begin{bmatrix} 1/5 & 0 \\ 0 & 0 \end{bmatrix}$ is itself. $\mat{\Sigma}^{+} = \begin{bmatrix} 0.2 & 0 \\ 0 & 0 \end{bmatrix}$.
    Calculate $\mat{A}^{+} = \mat{V} \mat{\Sigma}^{+} \mat{U}^T$:
    \[ \mat{A}^{+} \approx \begin{bmatrix} 0.447 & -0.894 \\ 0.894 & 0.447 \end{bmatrix} \begin{bmatrix} 0.2 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0.447 & 0.894 \\ -0.894 & 0.447 \end{bmatrix} \]
    \[ \approx \begin{bmatrix} 0.0894 & 0 \\ 0.1788 & 0 \end{bmatrix} \begin{bmatrix} 0.447 & 0.894 \\ -0.894 & 0.447 \end{bmatrix} = \begin{bmatrix} 0.04 & 0.08 \\ 0.08 & 0.16 \end{bmatrix} = \frac{1}{25} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix} \]
    So $\mat{A}^{+} = \begin{bmatrix} 0.04 & 0.08 \\ 0.08 & 0.16 \end{bmatrix}$.

    \item Rectangular Matrix: Let $\mat{B} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}$ ($2 \times 1$).
    SVD: $\mat{U} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, $\mat{\Sigma} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}$, $\mat{V} = [1]$ ($1 \times 1$).
    $\mat{B} = \mat{U}\mat{\Sigma}\mat{V}^T = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 0 \end{bmatrix} [1]^T$.
    Calculate $\mat{\Sigma}^{+}$: Reciprocal of non-zero singular value is $1/2$. Transpose $\begin{bmatrix} 1/2 \\ 0 \end{bmatrix}$ to get $\begin{bmatrix} 1/2 & 0 \end{bmatrix}$ ($1 \times 2$). $\mat{\Sigma}^{+} = \begin{bmatrix} 0.5 & 0 \end{bmatrix}$.
    Calculate $\mat{B}^{+} = \mat{V} \mat{\Sigma}^{+} \mat{U}^T$:
    \[ \mat{B}^{+} = [1] \begin{bmatrix} 0.5 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^T = [1] \begin{bmatrix} 0.5 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = [1] \begin{bmatrix} 0.5 & 0 \end{bmatrix} = \begin{bmatrix} 0.5 & 0 \end{bmatrix} \]
    So $\mat{B}^{+} = \begin{bmatrix} 0.5 & 0 \end{bmatrix}$.
\end{enumerate}

\newpage
\part{Probability and Random Variables}

% ====================
% === INTRODUCTION ===
% ====================
\section{Introduction}
This document provides a brief overview of fundamental concepts in probability theory and related areas like information theory and graphical models. Each section introduces the topic, its common symbolism, interpretations, and simple examples.

% ====================
% === RANDOM VARIABLES ===
% ====================
\section{Random Variables}

\subsection*{Symbolism}
Random variables (RVs) are typically denoted by uppercase letters (e.g., $X, Y, Z$). The specific values (realizations or outcomes) of a random variable are denoted by corresponding lowercase letters (e.g., $x, y, z$). The set of possible values for $X$ is its sample space, often denoted $\Omega$ or $\mathcal{X}$.

\subsection*{Interpretation}
A random variable is a variable whose value is a numerical outcome determined by a random phenomenon or experiment. Formally, it's a function that maps outcomes from a sample space $\Omega$ to a measurable space (often the real numbers $\R$).
\begin{itemize}
    \item \textbf{Discrete Random Variable:} Takes on a finite or countably infinite number of distinct values (e.g., number of heads in 3 coin flips, result of a dice roll).
    \item \textbf{Continuous Random Variable:} Can take on any value within a given range or interval (e.g., height of a person, temperature).
\end{itemize}

\subsection*{Examples}
\begin{enumerate}
    \item Let $X$ be the outcome of a single coin flip: $\mathcal{X} = \{ \text{Heads}, \text{Tails} \}$. We might map this to numbers: $X \in \{0, 1\}$, where $X=1$ represents Heads. (Discrete)
    \item Let $Y$ be the height of a randomly chosen adult student in centimeters. $Y$ can take any value in a range, e.g., $Y \in [140, 220]$. (Continuous)
    \item Let $Z$ be the result of rolling a standard six-sided die: $\mathcal{Z} = \{1, 2, 3, 4, 5, 6\}$. (Discrete)
\end{enumerate}

% ====================
% === PROBABILITY ===
% ====================
\section{Probability}

\subsection*{Symbolism}
The probability of an event $A$ occurring is denoted by $\Prob(A)$, $P(A)$, or $\Pr(A)$. Probability is a value between 0 and 1, inclusive: $0 \le \Prob(A) \le 1$.
A probability function $\Prob$ assigns probabilities to events in an event space $\mathcal{F}$ (a set of subsets of the sample space $\Omega$) and must satisfy the Kolmogorov axioms:
\begin{enumerate}
    \item Non-negativity: $\Prob(A) \ge 0$ for all $A \in \mathcal{F}$.
    \item Normalization: $\Prob(\Omega) = 1$.
    \item Additivity: For any countable collection of mutually exclusive (disjoint) events $A_1, A_2, \dots$, $\Prob(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \Prob(A_i)$.
\end{enumerate}
For a random variable $X$, we write $\Prob(X=x)$ for the probability that $X$ takes the value $x$.

\subsection*{Interpretation}
Probability quantifies the likelihood or chance of an event occurring. $\Prob(A)=0$ means event $A$ is impossible. $\Prob(A)=1$ means event $A$ is certain. Higher values indicate higher likelihood. This can be interpreted from a frequentist perspective (long-run frequency of occurrence) or a Bayesian perspective (degree of belief).

\subsection*{Examples}
Let $Z$ be the result of rolling a fair six-sided die. $\Omega = \{1, 2, 3, 4, 5, 6\}$.
\begin{enumerate}
    \item Probability of rolling a 3: $\Prob(Z=3) = 1/6$.
    \item Probability of rolling an even number: Let $E = \{2, 4, 6\}$. $\Prob(E) = \Prob(Z=2) + \Prob(Z=4) + \Prob(Z=6) = 1/6 + 1/6 + 1/6 = 3/6 = 1/2$.
    \item Probability of rolling a number less than or equal to 2: Let $L = \{1, 2\}$. $\Prob(L) = \Prob(Z=1) + \Prob(Z=2) = 1/6 + 1/6 = 2/6 = 1/3$.
\end{enumerate}

% ====================
% === LIKELIHOOD ===
% ====================
\section{Likelihood}

\subsection*{Symbolism}
The likelihood function is often denoted by $\Lik(\theta | x)$ or $L(\theta | x)$, representing the likelihood of a parameter (or vector of parameters) $\theta$ given observed data $x$. Mathematically, it is often defined using the probability of the observed data given the parameter: $\Lik(\theta | x) = P(x | \theta)$ (or $f(x | \theta)$ for continuous data). Crucially, likelihood is interpreted as a function of $\theta$ with the data $x$ held fixed.

\subsection*{Interpretation}
Likelihood measures how well a particular parameter value $\theta$ explains the observed data $x$. It is *not* a probability distribution over $\theta$ (it doesn't necessarily integrate/sum to 1 over $\theta$). Comparing the likelihood of different parameter values helps determine which value is more plausible given the data. This is the basis for Maximum Likelihood Estimation (MLE), where we seek the $\theta$ that maximizes $\Lik(\theta | x)$.

\subsection*{Examples}
Suppose we flip a coin once and observe Heads ($x=1$). Let $p$ be the unknown probability of Heads. The probability mass function is $P(x|p) = p^x (1-p)^{1-x}$.
\begin{enumerate}
    \item The likelihood function given $x=1$ is $\Lik(p | x=1) = p^1 (1-p)^{1-1} = p$.
    \item Likelihood of $p=0.5$: $\Lik(0.5 | 1) = 0.5$.
    \item Likelihood of $p=0.8$: $\Lik(0.8 | 1) = 0.8$.
    The parameter value $p=0.8$ has a higher likelihood given the observed data (Heads).
    \item If we observed three flips: Heads, Tails, Heads ($x=(1, 0, 1)$). Assuming independence, $\Lik(p | x=(1,0,1)) = P(1|p)P(0|p)P(1|p) = p(1-p)p = p^2(1-p)$. The $p$ where the likelihood would be the highest is: $\hat{p}=2/3$.
\end{enumerate}

% ============================
% === PROBABILITY DISTRIBUTIONS ===
% ============================
\section{Probability Distributions}

\subsection*{Symbolism}
A probability distribution describes the probabilities of different outcomes for a random variable.
\begin{itemize}
    \item \textbf{Probability Mass Function (PMF):} For a discrete RV $X$, $P(x)$ or $p_X(x)$ denotes $P(X=x)$. Requires $P(x) \ge 0$ and $\sum_x P(x) = 1$.
    \item \textbf{Probability Density Function (PDF):} For a continuous RV $X$, $f(x)$ or $f_X(x)$ denotes the density. Requires $f(x) \ge 0$ and $\int_{-\infty}^{\infty} f(x) dx = 1$. Note: $f(x)$ is *not* a probability; $\Prob(a \le X \le b) = \int_a^b f(x) dx$.
    \item \textbf{Cumulative Distribution Function (CDF):} For any RV $X$, $F(x)$ or $F_X(x)$ denotes $\Prob(X \le x)$. $F(x)$ is non-decreasing, $F(-\infty)=0$, $F(\infty)=1$.
\end{itemize}

\subsection*{Interpretation}
Distributions provide a complete description of the probabilistic behavior of a random variable. PMFs give probabilities directly, while PDFs give densities whose integrals yield probabilities over intervals. CDFs give the cumulative probability up to a certain value.

\subsection*{Examples}
\begin{enumerate}
    \item Discrete Uniform (Fair Die): $X \in \{1..6\}$. PMF: $P(X=k) = 1/6$ for $k=1..6$. CDF: $F(x) = \lfloor x \rfloor / 6$ for $x \in [1, 6)$, $F(x)=0$ for $x<1$, $F(x)=1$ for $x \ge 6$. For example, $F(3.5) = \lfloor 3.5 \rfloor / 6 = 3/6 = 0.5$.
    \item Continuous Uniform $U(0, 1)$: $X \in [0, 1]$. PDF: $f(x) = 1$ if $0 \le x \le 1$, and $0$ otherwise. CDF: $F(x) = x$ if $0 \le x \le 1$, $0$ if $x < 0$, $1$ if $x > 1$. Probability over an interval: $\Prob(0.2 \le X \le 0.7) = \int_{0.2}^{0.7} 1 dx = 0.7 - 0.2 = 0.5$.
\end{enumerate}

% ============================
% === MARGINAL PROBABILITY ===
% ============================
\section{Marginal Probability}

\subsection*{Symbolism}
Given a joint probability distribution of multiple random variables, e.g., $P(X, Y)$, the marginal probability of a single variable, e.g., $X$, is denoted $P(X)$ or $p_X(x)$. It is obtained by summing or integrating the joint distribution over the other variables:
\begin{itemize}
    \item Discrete: $P(X=x) = \sum_{y} P(X=x, Y=y)$
    \item Continuous: $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$
\end{itemize}
This process is called \emph{marginalization}.

\subsection*{Interpretation}
Marginal probability gives the probability distribution of one variable without regard to the values of the other variables in the system. It effectively ignores (by summing/integrating over all possibilities) the information about the other variables. Another way to view this is that you can go from a probability in one variable and expand it into two in order to simplify a relationship you might have in your data.

\subsection*{Examples}
Consider a joint PMF for discrete variables $X, Y \in \{0, 1\}$:
\begin{center}
\begin{tabular}{c|cc|c}
$P(X,Y)$ & $Y=0$ & $Y=1$ & $P(X)$ \\ \hline
$X=0$ & 0.1 & 0.2 & ? \\
$X=1$ & 0.4 & 0.3 & ? \\ \hline
$P(Y)$ & ? & ? & 1.0
\end{tabular}
\end{center}
\begin{enumerate}
    \item Marginal probability $P(X=0)$: $P(X=0) = P(X=0, Y=0) + P(X=0, Y=1) = 0.1 + 0.2 = 0.3$.
    \item Marginal probability $P(X=1)$: $P(X=1) = P(X=1, Y=0) + P(X=1, Y=1) = 0.4 + 0.3 = 0.7$.
    (Check: $0.3 + 0.7 = 1.0$)
    \item Marginal probability $P(Y=0)$: $P(Y=0) = P(X=0, Y=0) + P(X=1, Y=0) = 0.1 + 0.4 = 0.5$.
    \item Marginal probability $P(Y=1)$: $P(Y=1) = P(X=0, Y=1) + P(X=1, Y=1) = 0.2 + 0.3 = 0.5$.
    (Check: $0.5 + 0.5 = 1.0$)
\end{enumerate}
Completed table:
\begin{center}
\begin{tabular}{c|cc|c}
$P(X,Y)$ & $Y=0$ & $Y=1$ & $P(X)$ \\ \hline
$X=0$ & 0.1 & 0.2 & 0.3 \\
$X=1$ & 0.4 & 0.3 & 0.7 \\ \hline
$P(Y)$ & 0.5 & 0.5 & 1.0
\end{tabular}
\end{center}

% ==============================
% === CONDITIONAL PROBABILITY ===
% ==============================
\section{Conditional Probability}

\subsection*{Symbolism}
The conditional probability of event $A$ occurring given that event $B$ has occurred is denoted $P(A|B)$ (read "probability of A given B"). It is defined as:
\[ P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{provided } P(B) > 0 \]
where $P(A \cap B)$ is the probability that both $A$ and $B$ occur (joint probability).
For random variables: $P(Y=y | X=x) = \frac{P(X=x, Y=y)}{P(X=x)}$ (discrete) or $f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}$ (continuous).

\subsection*{Interpretation}
Conditional probability represents the updated probability of an event based on new information (knowledge that another event occurred). It essentially restricts the sample space to the outcomes where the conditioning event $B$ is true, and then calculates the probability of $A$ within that reduced space.

\subsection*{Examples}
Using the joint PMF table from the previous section:
\begin{enumerate}
    \item Probability of $Y=1$ given $X=0$:
    \[ P(Y=1 | X=0) = \frac{P(X=0, Y=1)}{P(X=0)} = \frac{0.2}{0.3} = \frac{2}{3} \]
    \item Probability of $X=0$ given $Y=1$:
    \[ P(X=0 | Y=1) = \frac{P(X=0, Y=1)}{P(Y=1)} = \frac{0.2}{0.5} = \frac{2}{5} = 0.4 \]
\end{enumerate}
Drawing two cards from a standard 52-card deck without replacement.
\begin{enumerate}
    \item $A=$ 1st card is Ace, $B=$ 2nd card is Ace. $P(A) = 4/52$.
    \item $P(B|A) = \frac{P(A \cap B)}{P(A)}$. $P(A \cap B)$ is P(Ace, Ace) = $(4/52) \times (3/51)$.
    \[ P(B|A) = \frac{(4/52) \times (3/51)}{4/52} = \frac{3}{51} = \frac{1}{17} \]
    Given the first card was an Ace, there are 3 Aces left in the remaining 51 cards.
\end{enumerate}

% =========================================
% === CHAIN RULE OF CONDITIONAL PROBABILITY ===
% =========================================
\section{Chain Rule of Conditional Probability}

\subsection*{Symbolism}
The chain rule allows calculating the joint probability of multiple events or random variables using conditional probabilities. For events $A_1, A_2, \dots, A_n$:
\[ P(A_1 \cap A_2 \cap \dots \cap A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1, A_2) \dots P(A_n|A_1, \dots, A_{n-1}) \]
For random variables $X_1, X_2, \dots, X_n$:
\[ P(X_1=x_1, \dots, X_n=x_n) = P(x_1) P(x_2|x_1) P(x_3|x_1, x_2) \dots P(x_n|x_1, \dots, x_{n-1}) \]
(Using shorthand $P(x_i)$ for $P(X_i=x_i)$ etc.)

\subsection*{Interpretation}
This rule decomposes a complex joint probability into a product of simpler conditional probabilities. It follows directly from the definition of conditional probability: $P(A, B) = P(A|B)P(B) = P(B|A)P(A)$. It's fundamental for Bayesian networks and models involving sequences (like Hidden Markov Models).

\subsection*{Examples}
\begin{enumerate}
    \item Three events $A, B, C$: $P(A, B, C) = P(A) P(B|A) P(C|A, B)$.
    \item Drawing 3 cards without replacement (Ace, King, Queen in that order):
    Let $A_1$=1st is Ace, $A_2$=2nd is King, $A_3$=3rd is Queen.
    \[ P(A_1, A_2, A_3) = P(A_1) P(A_2|A_1) P(A_3|A_1, A_2) \]
    \[ = \left(\frac{4}{52}\right) \times \left(\frac{4}{51}\right) \times \left(\frac{4}{50}\right) \]
\end{enumerate}

% ====================
% === INDEPENDENCE ===
% ====================
\section{Independence}

\subsection*{Symbolism}
Two events $A$ and $B$ are statistically independent if the occurrence of one does not affect the probability of the other. Symbolically:
\[ A \Indep B \iff P(A \cap B) = P(A) P(B) \]
Equivalent conditions (if denominators are non-zero):
\[ P(A|B) = P(A) \]
\[ P(B|A) = P(B) \]
Two random variables $X$ and $Y$ are independent ($X \Indep Y$) if their joint distribution factors into the product of their marginal distributions:
\begin{itemize}
    \item Discrete: $P(X=x, Y=y) = P(X=x) P(Y=y)$ for all $x, y$.
    \item Continuous: $f_{X,Y}(x,y) = f_X(x) f_Y(y)$ for all $x, y$.
\end{itemize}

\subsection*{Interpretation}
Independence signifies a lack of probabilistic relationship between events or variables. Knowing the outcome of one provides no information about the outcome of the other. This simplifies calculations greatly, as the joint probability is just the product of marginals.

\subsection*{Examples}
\begin{enumerate}
    \item Two fair coin flips: $X$=result of 1st, $Y$=result of 2nd. Assume independent.
    $P(X=\text{H}, Y=\text{T}) = P(X=\text{H}) P(Y=\text{T}) = (1/2) \times (1/2) = 1/4$.
    \item Rolling a die ($X$) and flipping a coin ($Y$). Assume independent.
    $P(X=3, Y=\text{H}) = P(X=3) P(Y=\text{H}) = (1/6) \times (1/2) = 1/12$.
    \item Consider the joint PMF from the Marginal Probability section. Are $X$ and $Y$ independent?
    Check if $P(X=x, Y=y) = P(X=x)P(Y=y)$ for all $x, y$.
    Take $x=0, y=0$: $P(X=0, Y=0) = 0.1$. $P(X=0)P(Y=0) = (0.3)(0.5) = 0.15$.
    Since $0.1 \neq 0.15$, $X$ and $Y$ are *not* independent.
\end{enumerate}

% ==============================
% === CONDITIONAL INDEPENDENCE ===
% ==============================
\section{Conditional Independence}

\subsection*{Symbolism}
Two events $A$ and $B$ are conditionally independent given a third event $C$ if:
\[ P(A \cap B | C) = P(A|C) P(B|C) \]
Equivalent conditions (if denominators non-zero):
\[ P(A | B, C) = P(A|C) \]
\[ P(B | A, C) = P(B|C) \]
Random variables $X$ and $Y$ are conditionally independent given $Z$ (denoted $X \Indep Y | Z$) if:
\[ P(X, Y | Z) = P(X|Z) P(Y|Z) \quad \text{for all } x, y, z \]
(Using appropriate PMFs or PDFs).

\subsection*{Interpretation}
Conditional independence means that once the outcome of $Z$ is known, $X$ and $Y$ become independent. Any statistical dependence between $X$ and $Y$ is fully explained by or mediated through $Z$. This concept is crucial for understanding graphical models (Bayesian networks and Markov Random Fields).

\subsection*{Examples}
\begin{enumerate}
    \item Let $F$ = Electrical Fault, $A$ = Alarm Sounds, $S$ = Sprinkler Activates.
    Assume Fault causes Alarm ($F \to A$) and Fault causes Sprinkler ($F \to S$).
    Alarm and Sprinkler are likely dependent: $P(S|A) > P(S)$. Hearing the alarm increases belief the sprinkler will activate.
    However, they are conditionally independent given Fault: $A \Indep S | F$.
    $P(A, S | F) = P(A|F) P(S|F)$. If we know there's a fault, hearing the alarm gives no *additional* information about the sprinkler (its probability is determined by $P(S|F)$). Similarly $P(A, S | \neg F) = P(A|\neg F) P(S|\neg F)$ (likely both zero or very small).
    \item Height ($H$), Age ($A$), Vocabulary Size ($V$) in children.
    $H$ and $V$ are dependent (taller children tend to have larger vocabularies).
    But they might be conditionally independent given Age: $H \Indep V | A$. Age is the common cause explaining the correlation. Knowing a child's age makes height less informative about vocabulary, and vice versa.
\end{enumerate}

% ========================
% === EXPECTATION VALUES ===
% ========================
\section{Expectation Values (Expected Value)}

\subsection*{Symbolism}
The expected value (or mean) of a random variable $X$ is denoted $\E[X]$ or $\mu_X$.
\begin{itemize}
    \item Discrete: $\E[X] = \sum_x x P(X=x)$
    \item Continuous: $\E[X] = \int_{-\infty}^{\infty} x f(x) dx$
\end{itemize}
The expected value of a function $g(X)$ is $\E[g(X)] = \sum_x g(x) P(X=x)$ or $\int g(x) f(x) dx$.
A key property is linearity: $\E[aX + bY] = a\E[X] + b\E[Y]$ for constants $a, b$ and RVs $X, Y$.

\subsection*{Interpretation}
The expected value represents the long-run average outcome of the random variable over many independent trials. It's the probability-weighted average of all possible values. Geometrically, it's the center of mass of the probability distribution.

\subsection*{Examples}
\begin{enumerate}
    \item Fair six-sided die $X \in \{1..6\}$, $P(X=k)=1/6$.
    \[ \E[X] = \sum_{k=1}^6 k \cdot \frac{1}{6} = \frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5 \]
    \item Bernoulli trial $X \in \{0, 1\}$ with $P(X=1)=p$.
    \[ \E[X] = 1 \cdot P(X=1) + 0 \cdot P(X=0) = 1 \cdot p + 0 \cdot (1-p) = p \]
    \item Expected value of $X^2$ for the die roll:
    \[ \E[X^2] = \sum_{k=1}^6 k^2 \cdot \frac{1}{6} = \frac{1^2+2^2+3^2+4^2+5^2+6^2}{6} = \frac{1+4+9+16+25+36}{6} = \frac{91}{6} \]
\end{enumerate}

% ====================
% === VARIANCE ===
% ====================
\section{Variance}

\subsection*{Symbolism}
The variance of a random variable $X$ measures the spread of its distribution. It's denoted $\Var(X)$ or $\sigma^2_X$ or simply $\sigma^2$.
\[ \Var(X) = \E[(X - \E[X])^2] = \E[(X - \mu_X)^2] \]
A common computational formula is:
\[ \Var(X) = \E[X^2] - (\E[X])^2 = \E[X^2] - \mu_X^2 \]
The standard deviation $\sigma_X$ is the square root of the variance: $\sigma_X = \sqrt{\Var(X)}$.
Property: $\Var(aX + b) = a^2 \Var(X)$ for constants $a, b$.

\subsection*{Interpretation}
Variance quantifies the expected squared deviation of a random variable from its mean. A low variance indicates values cluster tightly around the mean, while high variance indicates values are spread out. Standard deviation provides a measure of spread in the same units as the random variable itself.

\subsection*{Examples}
\begin{enumerate}
    \item Fair six-sided die $X$. We found $\E[X]=3.5$ and $\E[X^2]=91/6$.
    \[ \Var(X) = \E[X^2] - (\E[X])^2 = \frac{91}{6} - (3.5)^2 = \frac{91}{6} - (7/2)^2 = \frac{91}{6} - \frac{49}{4} \]
    \[ = \frac{182}{12} - \frac{147}{12} = \frac{35}{12} \approx 2.917 \]
    Standard deviation $\sigma_X = \sqrt{35/12} \approx 1.708$.
    \item Bernoulli trial $X$ with $P(X=1)=p$. $\E[X]=p$. $X^2=X$ since $X$ is 0 or 1. So $\E[X^2] = \E[X] = p$.
    \[ \Var(X) = \E[X^2] - (\E[X])^2 = p - p^2 = p(1-p) \]
\end{enumerate}

% ====================
% === COVARIANCE ===
% ====================
\section{Covariance}

\subsection*{Symbolism}
The covariance between two random variables $X$ and $Y$ measures how they vary together. It's denoted $\Cov(X, Y)$ or $\sigma_{XY}$.
\[ \Cov(X, Y) = \E[(X - \E[X])(Y - \E[Y])] = \E[(X - \mu_X)(Y - \mu_Y)] \]
Computational formula:
\[ \Cov(X, Y) = \E[XY] - \E[X]\E[Y] \]
Properties:
\begin{itemize}
    \item $\Cov(X, X) = \Var(X)$
    \item $\Cov(X, Y) = \Cov(Y, X)$
    \item $\Cov(aX+b, cY+d) = ac \Cov(X, Y)$
    \item $\Var(X+Y) = \Var(X) + \Var(Y) + 2\Cov(X, Y)$
    \item If $X, Y$ are independent, then $\Cov(X, Y) = 0$. (Converse is not always true).
\end{itemize}
The Pearson correlation coefficient normalizes covariance: $\rho_{XY} = \frac{\Cov(X, Y)}{\sigma_X \sigma_Y}$, where $-1 \le \rho_{XY} \le 1$.

\subsection*{Interpretation}
\begin{itemize}
    \item $\Cov(X, Y) > 0$: $X$ and $Y$ tend to move in the same direction (if $X$ is above its mean, $Y$ tends to be above its mean).
    \item $\Cov(X, Y) < 0$: $X$ and $Y$ tend to move in opposite directions.
    \item $\Cov(X, Y) = 0$: No linear relationship between $X$ and $Y$.
\end{itemize}
Correlation $\rho$ measures the strength and direction of the *linear* relationship.

\subsection*{Examples}
Let $X$ be the result of a fair die roll, $Y = X^2$. Are they independent? No. Let's calculate covariance.
$\E[X] = 3.5$, $\E[Y] = \E[X^2] = 91/6$.
We need $\E[XY] = \E[X \cdot X^2] = \E[X^3]$.
$\E[X^3] = \sum_{k=1}^6 k^3 \cdot \frac{1}{6} = \frac{1^3+..+6^3}{6} = \frac{1+8+27+64+125+216}{6} = \frac{441}{6} = 73.5$.
\[ \Cov(X, Y) = \E[XY] - \E[X]\E[Y] = 73.5 - (3.5) \times (91/6) \]
\[ = 73.5 - (7/2) \times (91/6) = 73.5 - 637/12 \approx 73.5 - 53.083 = 20.417 \]
Since $\Cov(X, Y) \neq 0$, $X$ and $Y$ are not independent (and positively correlated).

% =======================================
% === COMMON PROBABILITY DISTRIBUTIONS ===
% =======================================
\section{Common Probability Distributions}

\subsection*{Symbolism and Properties}
Standard distributions modelling common random processes.

\begin{enumerate}
    \item \textbf{Bernoulli Distribution:} $X \sim \text{Bernoulli}(p)$
        \begin{itemize}
            \item Parameter: $p \in [0, 1]$ (probability of success).
            \item Sample Space: $\mathcal{X} = \{0, 1\}$ (0=failure, 1=success).
            \item PMF: $P(X=1) = p$, $P(X=0) = 1-p$. Or $P(X=k) = p^k (1-p)^{1-k}$.
            \item Mean: $\E[X] = p$.
            \item Variance: $\Var(X) = p(1-p)$.
            \item Interpretation: Single trial with two outcomes.
        \end{itemize}

    \item \textbf{Binomial Distribution:} $X \sim \text{Binomial}(n, p)$
        \begin{itemize}
            \item Parameters: $n \in \{1, 2, ...\}$ (number of trials), $p \in [0, 1]$ (success prob per trial).
            \item Sample Space: $\mathcal{X} = \{0, 1, \dots, n\}$.
            \item PMF: $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$, where $\binom{n}{k} = \frac{n!}{k!(n-k)!}$.
            \item Mean: $\E[X] = np$.
            \item Variance: $\Var(X) = np(1-p)$.
            \item Interpretation: Number of successes in $n$ independent Bernoulli trials.
        \end{itemize}

    \item \textbf{Poisson Distribution:} $X \sim \text{Poisson}(\lambda)$
        \begin{itemize}
            \item Parameter: $\lambda > 0$ (average rate or count).
            \item Sample Space: $\mathcal{X} = \{0, 1, 2, \dots\}$.
            \item PMF: $P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!}$.
            \item Mean: $\E[X] = \lambda$.
            \item Variance: $\Var(X) = \lambda$.
            \item Interpretation: Count of events in a fixed interval of time/space, given average rate $\lambda$. Approximation to Binomial for large $n$, small $p$ with $\lambda=np$.
        \end{itemize}

    \item \textbf{Uniform Distribution (Continuous):} $X \sim U(a, b)$
        \begin{itemize}
            \item Parameters: $a, b \in \R$, $a < b$ (interval endpoints).
            \item Sample Space: $\mathcal{X} = [a, b]$.
            \item PDF: $f(x) = \frac{1}{b-a}$ if $a \le x \le b$, and $0$ otherwise.
            \item Mean: $\E[X] = \frac{a+b}{2}$.
            \item Variance: $\Var(X) = \frac{(b-a)^2}{12}$.
            \item Interpretation: All values in the interval $[a, b]$ are equally likely.
        \end{itemize}

    \item \textbf{Normal (Gaussian) Distribution:} $X \sim \mathcal{N}(\mu, \sigma^2)$
        \begin{itemize}
            \item Parameters: $\mu \in \R$ (mean), $\sigma^2 > 0$ (variance).
            \item Sample Space: $\mathcal{X} = \R$.
            \item PDF: $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$.
            \item Mean: $\E[X] = \mu$.
            \item Variance: $\Var(X) = \sigma^2$.
            \item Interpretation: Symmetric bell-shaped curve. Arises frequently due to the Central Limit Theorem (sum of many independent RVs tends towards Normal).
        \end{itemize}

    \item \textbf{Exponential Distribution:} $X \sim \text{Exponential}(\lambda)$
        \begin{itemize}
            \item Parameter: $\lambda > 0$ (rate parameter). Sometimes parameterized by scale $\beta=1/\lambda$.
            \item Sample Space: $\mathcal{X} = [0, \infty)$.
            \item PDF: $f(x) = \lambda e^{-\lambda x}$ if $x \ge 0$, and $0$ otherwise.
            \item Mean: $\E[X] = 1/\lambda$.
            \item Variance: $\Var(X) = 1/\lambda^2$.
            \item Interpretation: Waiting time until the next event in a Poisson process with rate $\lambda$. Has the memoryless property: $P(X > s+t | X > s) = P(X > t)$.
        \end{itemize}
\end{enumerate}

% =====================
% === LATENT VARIABLES ===
% =====================
\section{Latent Variables}

\subsection*{Symbolism}
Latent variables are often denoted by $Z$ or $H$ (for hidden). Observed variables are often denoted by $X$ or $O$. A model might relate them, e.g., via $P(X|Z)$ and $P(Z)$.

\subsection*{Interpretation}
Latent variables are random variables that are not directly observed but are inferred from the observed variables. They are introduced in statistical models to represent underlying structures, hidden causes, unobserved states, or clusters in the data. Models incorporating latent variables can often capture complex dependencies between observed variables more parsimoniously. Examples include Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), Latent Dirichlet Allocation (LDA) for topic modeling, and Variational Autoencoders (VAEs). Inference typically involves estimating the posterior distribution of latent variables given observed data, $P(Z|X)$.

\subsection*{Examples}
\begin{enumerate}
    \item Gaussian Mixture Model (GMM): Observed data points $X = \{x_1, \dots, x_n\}$ are assumed to be generated from a mixture of $K$ Gaussian components. For each data point $x_i$, there is a latent variable $z_i \in \{1, \dots, K\}$ indicating which component generated it. The model specifies $P(z_i)$ (mixture weights) and $P(x_i | z_i)$ (component Gaussians).
    \item Hidden Markov Model (HMM): Used for sequential data $X = (x_1, \dots, x_T)$. Assumes each observation $x_t$ depends on a hidden state $z_t$, and the hidden state sequence $Z=(z_1, \dots, z_T)$ follows a Markov chain ($P(z_t | z_{t-1}, \dots, z_1) = P(z_t | z_{t-1})$). The model defines transition probabilities $P(z_t|z_{t-1})$ and emission probabilities $P(x_t|z_t)$.
\end{enumerate}

% ==================
% === BAYES' RULE ===
% ==================
\section{Bayes' Rule}

\subsection*{Symbolism}
For events $A$ and $B$ (with $P(B)>0$):
\[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]
In the context of inference with parameters $\theta$ and data $x$:
\[ P(\theta|x) = \frac{P(x|\theta) P(\theta)}{P(x)} \]
where:
\begin{itemize}
    \item $P(\theta|x)$ is the \textbf{posterior} probability of parameters given data.
    \item $P(x|\theta)$ is the \textbf{likelihood} of data given parameters.
    \item $P(\theta)$ is the \textbf{prior} probability of parameters (belief before seeing data).
    \item $P(x)$ is the \textbf{evidence} or marginal likelihood of data, $P(x) = \int P(x|\theta) P(\theta) d\theta$ (or sum). Acts as a normalizing constant.
\end{itemize}
Often written as: Posterior $\propto$ Likelihood $\times$ Prior.

\subsection*{Interpretation}
Bayes' rule provides a way to update beliefs (prior) in light of new evidence (data) to obtain revised beliefs (posterior). It mathematically connects the probability of $A$ given $B$ to the probability of $B$ given $A$. It's the foundation of Bayesian statistics and inference.

\subsection*{Examples}
Medical Diagnosis: Let $D$ be the event "patient has Disease", $T$ be "patient tests Positive".
Suppose:
\begin{itemize}
    \item Prior prevalence: $P(D) = 0.01$ (1% of population has the disease). So $P(\neg D) = 0.99$.
    \item Test sensitivity: $P(T|D) = 0.95$ (95% of sick people test positive).
    \item Test specificity: $P(\neg T|\neg D) = 0.90$. So false positive rate is $P(T|\neg D) = 1 - 0.90 = 0.10$ (10% of healthy people test positive).
\end{itemize}
What is $P(D|T)$, the probability of having the disease given a positive test?
First, find the total probability of testing positive, $P(T)$ (the evidence):
\[ P(T) = P(T|D)P(D) + P(T|\neg D)P(\neg D) \]
\[ P(T) = (0.95)(0.01) + (0.10)(0.99) = 0.0095 + 0.0990 = 0.1085 \]
Now apply Bayes' Rule:
\[ P(D|T) = \frac{P(T|D) P(D)}{P(T)} = \frac{(0.95)(0.01)}{0.1085} = \frac{0.0095}{0.1085} \approx 0.0876 \]
Even with a positive test, the probability of actually having this rare disease is only about 8.8%, because the false positive rate is relatively high compared to the disease prevalence.

% ======================
% === SELF-INFORMATION ===
% ======================
\section{Self-Information}

\subsection*{Symbolism}
The self-information (or surprisal) of an outcome $x$ is denoted $I(x)$. It is defined based on the probability $P(x)$ of the outcome:
\[ I(x) = -\log P(x) \]
The base of the logarithm determines the units:
\begin{itemize}
    \item Base 2: units are bits.
    \item Base $e$ (natural log): units are nats.
    \item Base 10: units are Hartleys.
\end{itemize}

\subsection*{Interpretation}
Self-information quantifies the amount of surprise or information content associated with observing a specific outcome. Outcomes with low probability are more surprising and thus have higher information content. Certain outcomes ($P(x)=1$) have zero information content ($-\log 1 = 0$).

\subsection*{Examples}
Using base 2 logarithm (bits):
\begin{enumerate}
    \item Fair coin flip: $P(\text{Heads}) = 0.5$. $I(\text{Heads}) = -\log_2(0.5) = -\log_2(2^{-1}) = 1$ bit.
    \item Fair die roll: $P(X=3) = 1/6$. $I(X=3) = -\log_2(1/6) = \log_2(6) \approx 2.58$ bits.
    \item A very likely event: $P(Y=y) = 0.99$. $I(Y=y) = -\log_2(0.99) \approx 0.014$ bits (low information/surprise).
\end{enumerate}

% =====================
% === SHANNON ENTROPY ===
% =====================
\section{Shannon Entropy}

\subsection*{Symbolism}
The Shannon entropy of a discrete random variable $X$ with PMF $P(x)$ is denoted $H(X)$ or $H(P)$. It is the expected value of the self-information:
\[ H(X) = \E[I(X)] = \E[-\log P(X)] = - \sum_{x \in \mathcal{X}} P(x) \log P(x) \]
For continuous RVs with PDF $f(x)$, this is called \emph{differential entropy}:
\[ h(X) = - \int_{-\infty}^{\infty} f(x) \log f(x) dx \]
(Note: Differential entropy doesn't share all properties of discrete entropy, e.g., can be negative). Units depend on the logarithm base (bits for log base 2). Convention: $0 \log 0 = 0$.

\subsection*{Interpretation}
Entropy measures the average amount of information, uncertainty, or surprise associated with the outcomes of a random variable.
\begin{itemize}
    \item Maximum entropy occurs for a uniform distribution over the possible outcomes (maximum uncertainty).
    \item Minimum entropy (zero for discrete) occurs when the outcome is deterministic (no uncertainty).
    \item It provides a lower bound on the average number of bits required to encode samples from the distribution (source coding theorem).
\end{itemize}

\subsection*{Examples}
Using base 2 logarithm (bits):
\begin{enumerate}
    \item Fair coin flip $X$, $P(H)=0.5, P(T)=0.5$.
    $H(X) = - [P(H)\log_2 P(H) + P(T)\log_2 P(T)] = - [0.5(-1) + 0.5(-1)] = 1$ bit.
    \item Biased coin flip $Y$, $P(H)=0.9, P(T)=0.1$.
    $H(Y) = - [0.9 \log_2 0.9 + 0.1 \log_2 0.1] \approx - [0.9(-0.152) + 0.1(-3.322)] \approx 0.1368 + 0.3322 = 0.469$ bits. (Less uncertainty than the fair coin).
    \item Fair die roll $Z$, $P(k)=1/6$ for $k=1..6$.
    $H(Z) = - \sum_{k=1}^6 \frac{1}{6} \log_2 \frac{1}{6} = - 6 \times (\frac{1}{6} \log_2 \frac{1}{6}) = - \log_2 \frac{1}{6} = \log_2 6 \approx 2.58$ bits.
\end{enumerate}

% ====================================
% === KULLBACK-LEIBLER (KL) DIVERGENCE ===
% ====================================
\section{Kullback-Leibler (KL) Divergence}

\subsection*{Symbolism}
The KL divergence measures how one probability distribution $P$ diverges from a second expected probability distribution $Q$. It is denoted $D_{KL}(P || Q)$.
\begin{itemize}
    \item Discrete: $D_{KL}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}$
    \item Continuous: $D_{KL}(P || Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx$
\end{itemize}
It can be written using expectation: $D_{KL}(P || Q) = \E_{X \sim P} \left[ \log \frac{P(X)}{Q(X)} \right]$. Units depend on log base. Convention: $P(x) \log \frac{P(x)}{0} = \infty$, $0 \log \frac{0}{Q(x)} = 0$. Requires $P$ to be absolutely continuous wrt $Q$ (i.e., $Q(x)=0 \implies P(x)=0$).

\subsection*{Interpretation}
KL divergence quantifies the "distance" (though not a true metric as it's not symmetric) from distribution $Q$ to distribution $P$.
\begin{itemize}
    \item $D_{KL}(P || Q) \ge 0$.
    \item $D_{KL}(P || Q) = 0$ if and only if $P = Q$.
    \item It's asymmetric: $D_{KL}(P || Q) \neq D_{KL}(Q || P)$ generally.
    \item It represents the expected extra information (average additional bits/nats) required to encode samples from $P$ using a code optimized for $Q$, compared to using a code optimized for $P$.
    \item Used in variational inference (approximating a complex posterior $P$ with a simpler $Q$) and model comparison.
\end{itemize}

\subsection*{Examples}
Let $P = \text{Bernoulli}(0.5)$ and $Q = \text{Bernoulli}(0.8)$. Base 2 logs.
$P(0)=0.5, P(1)=0.5$. $Q(0)=0.2, Q(1)=0.8$.
\begin{align*} D_{KL}(P || Q) &= P(0) \log_2 \frac{P(0)}{Q(0)} + P(1) \log_2 \frac{P(1)}{Q(1)} \\ &= 0.5 \log_2 \frac{0.5}{0.2} + 0.5 \log_2 \frac{0.5}{0.8} \\ &= 0.5 \log_2 (2.5) + 0.5 \log_2 (0.625) \\ &\approx 0.5(1.322) + 0.5(-0.678) \approx 0.661 - 0.339 = 0.322 \text{ bits} \end{align*}
How much $Q$ diverges from $P$?
\begin{align*} D_{KL}(Q || P) &= Q(0) \log_2 \frac{Q(0)}{P(0)} + Q(1) \log_2 \frac{Q(1)}{P(1)} \\ &= 0.2 \log_2 \frac{0.2}{0.5} + 0.8 \log_2 \frac{0.8}{0.5} \\ &= 0.2 \log_2 (0.4) + 0.8 \log_2 (1.6) \\ &\approx 0.2(-1.322) + 0.8(0.678) \approx -0.264 + 0.542 = 0.278 \text{ bits} \end{align*}
Note $D_{KL}(P || Q) \neq D_{KL}(Q || P)$.

% =====================
% === CROSS-ENTROPY ===
% =====================
\section{Cross-Entropy}

\subsection*{Symbolism}
The cross-entropy between two probability distributions $P$ and $Q$ over the same sample space is denoted $H(P, Q)$.
\begin{itemize}
    \item Discrete: $H(P, Q) = - \sum_{x \in \mathcal{X}} P(x) \log Q(x)$
    \item Continuous: $H(P, Q) = - \int p(x) \log q(x) dx$ (less common)
\end{itemize}
Using expectation: $H(P, Q) = \E_{X \sim P} [-\log Q(X)]$.

\subsection*{Interpretation}
Cross-entropy measures the average number of bits/nats needed to identify an event drawn from distribution $P$, when the coding scheme used is optimized for distribution $Q$.
It relates to Shannon entropy and KL divergence:
\[ H(P, Q) = H(P) + D_{KL}(P || Q) \]
Since $H(P)$ is fixed (for a given $P$) and $D_{KL} \ge 0$, minimizing cross-entropy $H(P, Q)$ with respect to $Q$ is equivalent to minimizing the KL divergence $D_{KL}(P || Q)$. This makes it a very popular loss function in machine learning for classification tasks, where $P$ represents the true distribution (e.g., one-hot encoded labels) and $Q$ represents the model's predicted probability distribution.

\subsection*{Examples}
Multi-class classification with 3 classes. True label is class 2.
True distribution $P = [0, 1, 0]$.
Model's predicted probabilities $Q = [0.1, 0.7, 0.2]$.
Using natural log (nats):
\begin{align*} H(P, Q) &= - [ P(1)\ln Q(1) + P(2)\ln Q(2) + P(3)\ln Q(3) ] \\ &= - [ 0 \cdot \ln(0.1) + 1 \cdot \ln(0.7) + 0 \cdot \ln(0.2) ] \\ &= - \ln(0.7) \approx -(-0.3567) = 0.3567 \text{ nats} \end{align*}
If model predicted $Q' = [0.3, 0.4, 0.3]$:
\[ H(P, Q') = - [ 0 \cdot \ln(0.3) + 1 \cdot \ln(0.4) + 0 \cdot \ln(0.3) ] = - \ln(0.4) \approx 0.9163 \text{ nats} \]
The lower cross-entropy indicates the first prediction $Q$ is better.

% =====================================
% === STRUCTURED PROBABILISTIC MODELS ===
% =====================================
\section{Structured Probabilistic Models}

\subsection*{Symbolism}
These models use graphs (nodes represent RVs, edges represent dependencies) to represent complex joint probability distributions over many variables $X_1, \dots, X_n$. The structure of the graph dictates how the joint distribution $P(X_1, \dots, X_n)$ factorizes into simpler local terms.

\subsection*{Interpretation}
Real-world systems often involve many variables with complex interactions. Structured probabilistic models (also known as graphical models) provide a framework to represent these interactions efficiently by exploiting conditional independence relationships encoded in the graph structure. This allows for:
\begin{itemize}
    \item Compact representation of the joint distribution.
    \item Visualization of the dependency structure.
    \item Efficient algorithms for learning parameters and performing probabilistic inference (calculating marginal or conditional probabilities).
\end{itemize}
There are two main classes based on the type of graph used: Directed Graphs (Bayesian Networks) and Undirected Graphs (Markov Random Fields).

\subsection*{Examples}
\begin{itemize}
    \item Genetics: Modeling inheritance patterns.
    \item Natural Language Processing: Hidden Markov Models for part-of-speech tagging, Conditional Random Fields for named entity recognition.
    \item Computer Vision: Markov Random Fields for image segmentation or denoising.
    \item Medical Diagnosis: Bayesian networks relating diseases and symptoms.
\end{itemize}

% =====================
% === DIRECTED GRAPHS ===
% =====================
\section{Directed Graphs (Bayesian Networks)}

\subsection*{Symbolism}
Represented by a Directed Acyclic Graph (DAG) $G = (V, E)$, where $V$ is a set of nodes (random variables $X_i$) and $E$ is a set of directed edges. An edge $X_i \to X_j$ means $X_i$ is a "parent" of $X_j$. The joint probability distribution factorizes according to the graph structure:
\[ P(X_1, \dots, X_n) = \prod_{i=1}^n P(X_i | \text{Parents}(X_i)) \]
where $\text{Parents}(X_i)$ is the set of nodes with an edge pointing to $X_i$. If $X_i$ has no parents, $P(X_i | \text{Parents}(X_i)) = P(X_i)$.

\subsection*{Interpretation}
Edges typically represent direct probabilistic influence or causal relationships (though not strictly required). \textbf{The factorization implies conditional independence assumptions}. Specifically, a node $X_i$ is conditionally independent of its non-descendants, given its parents. These relationships can be formally checked using the concept of \emph{d-separation} in the graph. Bayesian networks are widely used for causal inference, diagnosis, prediction, etc.

\subsection*{Examples}
\begin{enumerate}
    \item Simple Chain: $A \to B \to C$.
    Factorization: $P(A, B, C) = P(A) P(B|A) P(C|B)$.
    Implies $C \Indep A | B$.
    \item Common Cause (Fork): $B \leftarrow A \to C$.
    Factorization: $P(A, B, C) = P(A) P(B|A) P(C|A)$.
    Implies $B \Indep C | A$.
    \item Common Effect (Collider or v-structure): $A \to C \leftarrow B$.
    Factorization: $P(A, B, C) = P(A) P(B) P(C|A, B)$.
    Implies marginal independence $A \Indep B$. However, they become dependent given $C$: $A \not\Indep B | C$ (explaining away).
\end{enumerate}

% =======================
% === UNDIRECTED GRAPHS ===
% =======================
\section{Undirected Graphs (Markov Random Fields)}

\subsection*{Symbolism}
Represented by an undirected graph $G = (V, E)$. Nodes $V$ are random variables $X_i$, edges $E$ represent probabilistic dependencies (affinities). The joint distribution factorizes over the maximal cliques of the graph. A clique is a subset of nodes where every pair is connected by an edge; a maximal clique is a clique that cannot be extended.
\[ P(X_1, \dots, X_n) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_C(X_C) \]
where:
\begin{itemize}
    \item $\mathcal{C}$ is the set of maximal cliques in $G$.
    \item $X_C$ is the set of variables in clique $C$.
    \item $\psi_C(X_C)$ is a non-negative potential function (or factor) defined over the variables in clique $C$. It measures the "compatibility" or "affinity" of the variables in that clique. Higher values mean higher probability.
    \item $Z$ is the partition function, a normalizing constant ensuring the distribution sums/integrates to 1: $Z = \sum_{x_1, \dots, x_n} \prod_{C \in \mathcal{C}} \psi_C(x_C)$.
\end{itemize}

\subsection*{Interpretation}
Edges represent direct dependencies between variables. Conditional independence is determined by graph separation: If every path between a set of nodes $A$ and a set $B$ passes through a set $C$, then $A \Indep B | C$. Markov Random Fields (MRFs) are often used when dependencies are symmetric, without a clear causal direction, e.g., in spatial models (image pixels) or statistical physics (Ising models). Calculating the partition function $Z$ is often computationally intractable.

\subsection*{Examples}
\begin{enumerate}
    \item Pairwise MRF for Image Denoising:
    Nodes represent pixel intensity values $X_i$ in an image. Edges connect adjacent pixels $(i, j)$. Observed noisy pixel values $Y_i$.
    Joint distribution might be $P(X, Y) = \frac{1}{Z} \prod_{(i,j)} \psi_{ij}(X_i, X_j) \prod_i \phi_i(X_i, Y_i)$.
    $\psi_{ij}(X_i, X_j)$ encourages adjacent pixels $X_i, X_j$ to have similar values (e.g., $\exp(-\beta (X_i-X_j)^2)$).
    $\phi_i(X_i, Y_i)$ encourages the true pixel $X_i$ to be close to the observed noisy value $Y_i$ (e.g., $\exp(-\gamma (X_i-Y_i)^2)$).
    Inference involves finding the most likely true image $X$ given the noisy $Y$.
    \item Simple MRF: $A - B - C$. Maximal cliques are $\{A, B\}$ and $\{B, C\}$.
    Factorization: $P(A, B, C) = \frac{1}{Z} \psi_{AB}(A, B) \psi_{BC}(B, C)$.
    Implies $A \Indep C | B$.
\end{enumerate}




\newpage
\part{Machine Learning Fundamentals}
This section tackles the common machine learning tasks, parts of models, benchmarking, and the theory/framework behind machine learning (ML). This will be focused on getting the reader caught up to speed on the fundamentals so that they can approach deep learning in an intelligent and cohesive manner. You could also teach these models as being drastically different, but that would, on the whole, not allow for common analysis methods to seem as similar as they are in reality. \\
Note: A good chunk of this chapter will be conceptual with some requiring math. Feel free to pick and choose the sections if you don't want to muddle through the math. That's A-OKAY and in fact I have attempted to have sections that are good for both.

\section{Common Machine Learning Tasks}
\subsection{Introduction: Task, Experience, and Model Type}

Machine learning (ML) enables systems to learn from data rather than being explicitly programmed. The design of an ML system can be understood through three key components, following the definition often attributed to Tom M. Mitchell:

\begin{description}
    \item[Task (T):] This defines \emph{what} the system needs to do. It's the problem the ML system is trying to solve. Examples include predicting categories, estimating numerical values, or generating new data. The task specifies the desired input-output mapping or the pattern to be identified.

    \item[Experience (E):] This refers to the \emph{data} the system uses to learn and improve its performance on the task. The nature of the experience dictates the learning paradigm, such as supervised learning (labeled data), unsupervised learning (unlabeled data), or reinforcement learning (rewards/penalties from actions).

    \item[Model Type (and Performance Measure P):] This refers to the \emph{algorithm} or mathematical framework chosen to learn from the experience and perform the task. The model encapsulates the assumptions made about the data and the learning process. Performance (P) on the task, measured by some metric (e.g., accuracy, error rate), should improve with experience. We focus here on the model types.
\end{description}

Below, we discuss several common machine learning tasks framed in terms of these components.

\subsection{Common Machine Learning Tasks}

\subsubsection{Classification}
\begin{description}
    \item[Overview:] Assigning input data points to one of several predefined discrete categories or classes.
    \item[Task:] To learn a mapping function $f: \mathcal{X} \rightarrow \{1, 2, ..., k\}$, where $\mathcal{X}$ is the input space and $\{1, 2, ..., k\}$ represents the $k$ distinct classes.
    \item[Experience:] Typically supervised learning using a dataset of labeled examples $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where $x^{(i)}$ is the feature vector for the $i$-th example, and $y^{(i)}$ is its corresponding class label.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Logistic Regression, Support Vector Machines (SVM), Decision Trees, k-Nearest Neighbors (k-NN).
            \item \textbf{Deep Learning:} Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs, especially for image classification), Recurrent Neural Networks (RNNs, for sequence classification).
        \end{itemize}
\end{description}

\subsubsection{Classification with Missing Inputs}
\begin{description}
    \item[Overview:] A variation of classification where some features in the input vector $x$ might be missing during training or, more commonly, during prediction.
    \item[Task:] Same as classification (map input to one of $k$ classes), but the model must be robust to or explicitly handle missing feature values.
    \item[Experience:] Labeled dataset, potentially containing missing values. Strategies often involve learning how to handle missingness, either through imputation or by designing models that are inherently capable of dealing with it.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Decision Trees (some implementations handle missing values naturally), algorithms used after imputation (e.g., mean/median/mode imputation followed by SVM or Logistic Regression), algorithms based on Expectation-Maximization (EM).
            \item \textbf{Deep Learning:} MLPs or other networks with input masking layers, variational autoencoders (VAEs) adapted for imputation, attention mechanisms that might learn to ignore missing inputs.
        \end{itemize}
\end{description}

\subsubsection{Regression}
\begin{description}
    \item[Overview:] Predicting a continuous numerical value (or a vector of continuous values) for a given input.
    \item[Task:] To learn a mapping function $f: \mathcal{X} \rightarrow \mathbb{R}^d$, where $\mathcal{X}$ is the input space and $\mathbb{R}^d$ is the space of $d$-dimensional real-valued outputs (often $d=1$).
    \item[Experience:] Supervised learning using a dataset of examples $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where $x^{(i)}$ is the feature vector and $y^{(i)}$ is the corresponding continuous target value(s).
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Linear Regression, Polynomial Regression, Support Vector Regression (SVR), Regression Trees, Gradient Boosting Machines (GBMs).
            \item \textbf{Deep Learning:} Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs, e.g., for predicting age from an image), Recurrent Neural Networks (RNNs, for time-series forecasting).
        \end{itemize}
\end{description}

\subsubsection{Transcription}
\begin{description}
    \item[Overview:] Converting unstructured or semi-structured data (like audio or images) into a discrete textual sequence.
    \item[Task:] To map an input sequence (e.g., audio waveform, image pixels) to a corresponding sequence of symbols (e.g., characters, words). Examples include Speech-to-Text and Optical Character Recognition (OCR).
    \item[Experience:] Supervised learning using paired data, such as audio recordings paired with their transcripts, or images of text paired with the actual text content.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Hidden Markov Models (HMMs) often combined with Gaussian Mixture Models (GMMs) for speech recognition; feature extraction pipelines followed by classifiers for OCR.
            \item \textbf{Deep Learning:} Recurrent Neural Networks (RNNs, LSTMs, GRUs) often with Connectionist Temporal Classification (CTC) loss, Sequence-to-Sequence models with Attention, Transformers (e.g., models like OpenAI's Whisper for speech).
        \end{itemize}
\end{description}

\subsubsection{Machine Translation}
\begin{description}
    \item[Overview:] Translating a sequence of symbols (text) from one language (source) to another (target).
    \item[Task:] To map an input sequence of symbols $S = (s_1, s_2, ..., s_m)$ in the source language to an output sequence $T = (t_1, t_2, ..., t_n)$ in the target language.
    \item[Experience:] Supervised learning using large parallel corpora, which are collections of texts aligned sentence-by-sentence (or segment-by-segment) in both source and target languages.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Statistical Machine Translation (SMT), including phrase-based and syntax-based models (e.g., Moses toolkit).
            \item \textbf{Deep Learning:} Sequence-to-Sequence (Seq2Seq) models based on RNNs (LSTMs/GRUs) with attention mechanisms; Transformer networks (now the dominant approach, e.g., Google Translate, DeepL).
        \end{itemize}
\end{description}

\subsubsection{Anomaly Detection}
\begin{description}
    \item[Overview:] Identifying data points, events, or observations that deviate significantly from the majority of the data, often referred to as outliers or anomalies.
    \item[Task:] To determine if a given input data point $x$ is 'normal' or 'anomalous' based on how it conforms to the learned distribution of normal data.
    \item[Experience:] Can be unsupervised (learning from mostly normal data), semi-supervised (learning from labeled normal data, sometimes with a few anomaly examples), or supervised (learning from labeled normal and anomalous data, effectively becoming a classification problem, often highly imbalanced). Unsupervised is common.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Statistical methods (e.g., Z-score, Gaussian mixture models), One-Class SVM, Isolation Forest, Local Outlier Factor (LOF).
            \item \textbf{Deep Learning:} Autoencoders (anomalies often have high reconstruction error), Generative Adversarial Networks (GANs - anomalies might be poorly reconstructed or have low discriminator scores), specialized deep anomaly detection networks.
        \end{itemize}
\end{description}

\subsubsection{Synthesis and Sampling}
\begin{description}
    \item[Overview:] Generating new data samples that are similar to, yet distinct from, the training data. Often involves learning the underlying data distribution.
    \item[Task:] To produce novel data points $x_{\text{new}}$ that appear to be drawn from the same distribution $p_{\text{data}}(x)$ as the training data. Examples include generating realistic images, music, or text.
    \item[Experience:] Typically unsupervised learning using a large dataset of examples representing the distribution to be learned (e.g., a dataset of faces, landscape images, or articles).
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Sampling from fitted probability distributions (e.g., Gaussian Mixture Models), Kernel Density Estimates (KDE) based sampling. (Often limited for high-dimensional data).
            \item \textbf{Deep Learning:} Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flows, Diffusion Models, Autoregressive models (e.g., PixelRNN/CNN for images, GPT for text).
        \end{itemize}
\end{description}

\subsubsection{Denoising}
\begin{description}
    \item[Overview:] Removing noise or corruption from data to recover the underlying clean signal.
    \item[Task:] To learn a function $f$ that maps a corrupted data point $\tilde{x}$ (where $\tilde{x} = x + \text{noise}$) back to the original, clean data point $x$.
    \item[Experience:] Often uses supervised learning where pairs of $(x, \tilde{x})$ are available. Can also be self-supervised, where clean data $x$ is artificially corrupted during training to create $\tilde{x}$.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Filtering techniques (e.g., median filter, Gaussian filter, Wiener filter for images/signals), Principal Component Analysis (PCA) based denoising, Wavelet thresholding.
            \item \textbf{Deep Learning:} Denoising Autoencoders (DAEs), Convolutional Neural Networks specifically trained for denoising tasks (e.g., DnCNN), potentially using GAN structures for realistic reconstruction. Diffusion models are intrinsically linked to denoising.
        \end{itemize}
\end{description}

\subsubsection{Density Estimation}
\begin{description}
    \item[Overview:] Explicitly modeling the underlying probability distribution $p(x)$ from which the training data was sampled.
    \item[Task:] To learn a probability distribution function (or probability density function for continuous data) $p_{\text{model}}(x)$ that accurately represents the true data distribution $p_{\text{data}}(x)$. This allows evaluating the likelihood of new points $p_{\text{model}}(x_{\text{new}})$.
    \item[Experience:] Unsupervised learning using a dataset drawn from the target distribution.
    \item[Model Examples:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Histograms, Kernel Density Estimation (KDE), Gaussian Mixture Models (GMMs), Factor Analysis.
            \item \textbf{Deep Learning:} Variational Autoencoders (VAEs, provide a lower bound on likelihood), Normalizing Flows (allow exact likelihood computation), Energy-Based Models (EBMs), Autoregressive models. Note: While GANs learn to sample from the distribution, they don't typically provide an explicit density function $p(x)$.
        \end{itemize}
\end{description}


\section{Parts of Machine Learning Models}

\subsection{Introduction}

Despite the vast diversity of algorithms and applications within machine learning (ML), most systems, whether traditional or based on deep learning, can be understood through a common set of fundamental components. These components interact to enable the system to learn from experience and perform its designated task. Understanding these core elements provides a framework for analyzing, comparing, and designing ML solutions. The four key components typically involved are: the Data used for learning, the Model that makes predictions or represents patterns, the Cost Function that measures performance, and the Optimization Algorithm that adjusts the model based on the data and cost function.

\subsection{Core Components of a Machine Learning System}

\subsubsection{Data: The Foundation for Learning}
\begin{description}
    \item[Concept:] Data represents the "experience" from which the machine learning system learns. It comprises the observations, measurements, or examples that fuel the learning process. The nature, quality, quantity, and representation of data are paramount to the success of any ML model.

    \item[Role:] Data provides the raw material containing the patterns, relationships, or structures the model aims to identify or approximate. In supervised learning, it includes input features and corresponding target labels or values. In unsupervised learning, it consists of input features without labels. In reinforcement learning, it involves states, actions, and rewards generated through interaction with an environment.

    \item[Traditional ML vs. Deep Learning:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Often designed for structured, tabular data where features are well-defined columns and samples are rows. Feature engineering---manually designing informative features from raw data---is often a critical and labor-intensive step. These methods can sometimes perform well even with relatively small datasets.
            \item \textbf{Deep Learning:} Particularly powerful with large volumes of less structured data, such as images (pixel arrays), text (sequences of words or characters), and audio (waveforms). A key strength is automatic feature learning (representation learning), where hierarchical features are learned directly from the raw data by the network's layers. This typically requires significantly larger datasets compared to traditional methods. Data augmentation (artificially expanding the dataset) is a common technique.
        \end{itemize}
\end{description}

\subsubsection{Model: The Hypothesis Representation}
\begin{description}
    \item[Concept:] The model (or hypothesis class) is the specific mathematical or algorithmic structure chosen to perform the task (e.g., prediction, classification, generation). It defines the space of possible functions or representations that the learning algorithm can select from. The choice of model encodes assumptions about the underlying data generating process.

    \item[Role:] Once trained, the model takes new input data and produces an output (e.g., a class prediction, a numerical value, a generated image). Its internal parameters are adjusted during training to best capture the patterns observed in the data according to the cost function.

    \item[Traditional ML vs. Deep Learning:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Often employs models with more explicit mathematical structures and fewer parameters (relative to deep learning). Examples include:
                \begin{itemize}
                    \item \textit{Linear Models:} Assume linear relationships (Linear Regression, Logistic Regression).
                    \item \textit{Support Vector Machines (SVMs):} Find optimal separating hyperplanes.
                    \item \textit{Tree-based Models:} Use hierarchical decision rules (Decision Trees, Random Forests).
                    \item \textit{Probabilistic Models:} Explicitly model probability distributions, often with assumptions like conditional independence (Naive Bayes) or data arising from a mix of simpler distributions (e.g., \textbf{Gaussian Mixture Models (GMMs)} which model data as a sum of Gaussian distributions). Interpretability of model parameters can sometimes be more straightforward.
                \end{itemize}
            \item \textbf{Deep Learning:} Primarily utilizes Artificial Neural Networks (ANNs) with multiple layers (hence "deep"). These act as powerful, universal function approximators. Examples include:
                \begin{itemize}
                    \item \textit{Multilayer Perceptrons (MLPs):} Basic feedforward networks.
                    \item \textit{Convolutional Neural Networks (CNNs):} Specialized for grid-like data (e.g., images), using convolution operations to capture spatial hierarchies.
                    \item \textit{Recurrent Neural Networks (RNNs):} Designed for sequential data (e.g., text, time series), with internal memory loops.
                    \item \textit{Transformers:} Based on attention mechanisms, highly successful in natural language processing and increasingly other domains.
                \end{itemize}
            Deep learning models typically have millions or billions of parameters, enabling them to learn extremely complex, non-linear patterns and hierarchical representations. However, interpreting the specific role of individual parameters is often challenging ("black box" problem).
        \end{itemize}
\end{description}

\subsubsection{Cost Function: Quantifying Performance}
\begin{description}
    \item[Concept:] The cost function (also known as loss function or objective function) measures the discrepancy between the model's predictions and the desired outcomes (e.g., true labels in supervised learning) or quantifies how well the model meets a specific objective (e.g., maximizing data likelihood in density estimation).

    \item[Role:] It provides a single scalar value indicating how well (or poorly) the current model parameters perform on the training data. The goal of the training process is typically to find the model parameters that minimize this cost function.

    \item[Traditional ML vs. Deep Learning:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Uses well-established cost functions tailored to the task and model. Common examples include Mean Squared Error (MSE) for regression, Cross-Entropy Loss (Log Loss) for classification (e.g., Logistic Regression), Hinge Loss for SVMs. For some model/cost combinations (like linear regression with MSE), the cost function is convex, guaranteeing that a found minimum is the global minimum.
            \item \textbf{Deep Learning:} Also employs standard losses like Cross-Entropy and MSE. However, due to the highly non-linear nature of deep neural networks, the overall cost landscape (the cost function plotted against the vast parameter space) is typically highly complex, high-dimensional, and non-convex, featuring numerous local minima, saddle points, and plateaus. Specialized or custom loss functions are also frequently developed (e.g., perceptual losses for image generation, CTC loss for sequence transcription).
        \end{itemize}
\end{description}

\subsubsection{Optimization Algorithm: Searching for the Best Fit}
\begin{description}
    \item[Concept:] The optimization algorithm is the iterative procedure used to adjust the model's parameters (e.g., weights and biases in a neural network, coefficients in linear regression) to minimize the cost function.

    \item[Role:] It navigates the parameter space, using information from the cost function (often its gradient) calculated on the training data, to find parameter values that yield good performance on the task.

    \item[Traditional ML vs. Deep Learning:]
        \begin{itemize}
            \item \textbf{Traditional ML:} Depending on the model and cost function, optimization might involve:
                \begin{itemize}
                    \item \textit{Closed-form solutions:} Directly calculating the optimal parameters (e.g., the Normal Equation for Ordinary Least Squares).
                    \item \textit{Convex Optimization Solvers:} Specialized algorithms for problems with convex cost functions (e.g., used for SVMs).
                    \item \textit{Iterative Methods:} General-purpose algorithms like Gradient Descent, Conjugate Gradient, L-BFGS, often operating on the entire dataset (batch gradient descent).
                \end{itemize}
            \item \textbf{Deep Learning:} Overwhelmingly relies on variants of Stochastic Gradient Descent (SGD). Because datasets are large and computing the exact gradient over all data is infeasible, SGD and its variants compute the gradient estimate using small subsets of data called mini-batches. Popular optimizers include:
                \begin{itemize}
                    \item \textit{SGD with Momentum:} Helps accelerate convergence and navigate shallow valleys.
                    \item \textit{RMSprop:} Adapts the learning rate per parameter based on the magnitude of recent gradients.
                    \item \textit{Adam (Adaptive Moment Estimation):} Combines ideas from Momentum and RMSprop, often the default choice.
                \end{itemize}
            Tuning optimizer hyperparameters (like learning rate, batch size, momentum coefficients) is crucial for successful deep learning training due to the complex, non-convex nature of the optimization problem.
        \end{itemize}
\end{description}

\subsection{Conclusion}

Understanding these four components---Data, Model, Cost Function, and Optimization Algorithm---provides a unifying lens through which to view machine learning. While the specific implementations and complexities differ significantly between traditional machine learning algorithms (like GMMs or SVMs) and modern deep learning approaches (like CNNs or Transformers), they all operate by defining a model structure, quantifying its performance via a cost function on some data, and using an optimization strategy to find the parameters that minimize that cost, thereby enabling the system to learn. \textbf{When viewing ML as an engineering task, it becomes clear that tweaking each of these parts can provide a model better suited to your needs.} The beauty of these 4 parts is that they can be tweaked independent of one another (largely). 



\section{Shortcomings of Traditional Machine Learning}

Alright, so machine learning was doing pretty well for a while, right? We had algorithms that could classify emails, predict house prices, and group customers. But as datasets got bigger and messier, especially with things like images, audio, and natural language, some real headaches started popping up for these traditional methods. Deep learning really gained traction because it offered potential ways around some of these tough challenges. Let's chat about a few of them.

One of the biggest baddies is the \emph{curse of dimensionality}. It sounds dramatic, but it's a genuine problem. Imagine you have data with just one feature – you can probably get a good sense of it with a handful of data points scattered along a line. Now add a second feature – you're in a 2D square. To get the same 'coverage' density, you need exponentially more points. Now imagine hundreds or thousands of features, like the pixels in an image or the words in a vocabulary. The 'space' your data lives in becomes mind-bogglingly vast. Your data points become incredibly sparse, like lonely stars in an enormous universe. This makes life difficult. How do you estimate statistics reliably? How can algorithms like k-Nearest Neighbors work if *every* point is far away from every other point? The notion of a 'local neighborhood' starts to break down.

This sparsity directly messes with another common assumption: \emph{local consistency or smoothness}. Many classic algorithms are built on the idea that if two input points are similar (close together in the feature space), their outputs should also be similar. Think about smoothly drawing a line or surface through your data points to make predictions. But if everything is far apart due to the curse of dimensionality, how do you know how to interpolate? How do you generalize to a new point if its nearest neighbors are still light-years away in high-dimensional space? The smoothness assumption just doesn't hold up as well when the data points don't densely sample the space.

Here's a weird geometric fact that highlights how strange high dimensions are: If you pick two random vectors (directions) in our familiar 3D space, they can point in all sorts of relative angles. But if you do this in, say, a million-dimensional space, those two random vectors will almost certainly be \emph{nearly perpendicular} (orthogonal) to each other! It's just one example of how our low-dimensional intuition completely fails us, and it hints at why distance metrics and geometric algorithms behave so differently up there.

Now, think about the kind of data we often deal with – images, sounds, text. It's usually not just a random jumble of features. There's structure! Data seems to be generated from \emph{combinations of factors, often in a hierarchy}. An image isn't just pixels; it's edges combining to form textures or simple shapes, which combine to form parts of objects (like an eye or a wheel), which combine to form whole objects (a face, a car). Language has letters forming words, words forming phrases, phrases building sentences according to grammar. Traditional machine learning often required humans to do \emph{feature engineering} – manually figuring out what these intermediate, meaningful features are (like edge detectors). A big hope for deep learning, with its multiple layers, was that it could learn these hierarchical features \emph{automatically}. Maybe the first layer learns simple edges, the next learns combinations of edges into corners or textures, and later layers learn to combine those into object parts and eventually whole objects.

Related to this, but slightly different, is the \emph{manifold hypothesis}. This is a more optimistic take on high-dimensional data. The idea is that even if our data points technically live in a very high-dimensional 'ambient' space (like the space of all possible pixel combinations), the data we actually care about (like pictures of actual faces, or recordings of actual speech) doesn't just fill up that space randomly. Instead, it tends to lie on or close to a much lower-dimensional, possibly wiggly and complicated, surface or structure embedded within that high-dimensional space. Think of a rolled-up sheet of paper (a 2D surface) existing within our 3D world. The data points follow the structure of the paper, not just any random point in 3D. Deep learning models, with their non-linear activation functions and layered structure, seem particularly good at learning functions that are sensitive to variations \emph{along} these complex manifolds while ignoring variations \emph{off} the manifold. They can potentially 'unroll' or 'flatten' these manifolds to make tasks like classification easier.

So, putting it all together, deep learning didn't just appear out of nowhere. It grew out of the need to tackle these challenges: to handle the curse of dimensionality, perhaps by finding the underlying low-dimensional manifolds; to learn the hierarchical, compositional features present in complex data automatically instead of relying on manual feature engineering; and to build models flexible enough to capture the intricate patterns hidden within massive datasets where simple smoothness assumptions fail. It's an attempt to create learning machines that work better on the kind of complex, high-dimensional, structured data that permeates the real world.




\section{Model Architecture}

\subsection{Introduction}

Modern deep learning models, capable of achieving state-of-the-art results on complex tasks, are rarely monolithic inventions. Instead, they are typically constructed by combining, stacking, and adapting a set of powerful, reusable architectural components or "building blocks." Understanding these fundamental blocks is crucial for designing, interpreting, and improving deep learning systems. This document explores some of the most common building blocks from both supervised and unsupervised learning paradigms, touching upon their mathematical transformations, the types of functions they help represent, and example use cases.

\subsection{Linear (Fully Connected / Dense) Layers}

\begin{itemize}
    \item \textbf{Description:} The most basic layer type. Each neuron in a linear layer receives input from \emph{all} neurons in the previous layer (or the input data). It computes a weighted sum of its inputs, adds a bias term, and typically passes the result through an element-wise non-linear activation function.
    \item \textbf{Mathematical Transformation:} For an input vector $\bx$ and output vector $\by$, the transformation is:
        \begin{equation}
            \by = f(\bW \bx + \bb)
        \end{equation}
        where $\bW$ is the weight matrix, $\bb$ is the bias vector, and $f$ is an element-wise activation function (e.g., ReLU, Sigmoid, Tanh, or identity for a purely linear layer). $\bW$ has shape (output\_dim, input\_dim) and $\bb$ has shape (output\_dim, 1).
    \item \textbf{Function Space / Representational Power:} A single linear layer (without non-linearity) can only represent linear functions. A stack of linear layers is still just a linear function. However, when combined with non-linear activation functions in a Multilayer Perceptron (MLP), networks of linear layers become \emph{universal function approximators}, capable of approximating any continuous function to arbitrary accuracy given enough hidden units (capacity). They have minimal inductive bias, meaning they don't make strong assumptions about the input data structure, which makes them flexible but potentially data-hungry. Capacity scales with the number of neurons and layers (parameters).
    \item \textbf{Supervised/Unsupervised Use:} Ubiquitous in both.
    \item \textbf{Example Use Case:} Often used as the final layer in a classification network to map learned features to class scores (logits); also forms the basis of MLPs used for tabular data or as components within more complex architectures like Transformers.
\end{itemize}

\subsection{Convolutional Layers (CNNs)}

\begin{itemize}
    \item \textbf{Description:} Designed primarily for grid-like data (e.g., images). Instead of connecting every input neuron to every output neuron, convolutional layers use small filters (kernels) that slide across the input data's spatial dimensions, computing dot products at each location. This detects local patterns. Multiple filters are used to detect different patterns. Often followed by non-linear activation (e.g., ReLU) and potentially pooling layers (e.g., MaxPooling) to downsample.
    \item \textbf{Mathematical Transformation:} For a 2D input $I$ (e.g., image channel) and a kernel $K$, the discrete convolution output at $(i, j)$ is:
        \begin{equation}
            (I * K)(i, j) = \sum_{m} \sum_{n} I(i+m, j+n) K(m, n) + b
        \end{equation}
        This is computed for each filter $K$ (potentially across multiple input channels) and often involves parameters like stride and padding. The result is then passed through an activation function $f$.
    \item \textbf{Function Space / Representational Power:} CNNs excel at representing functions with \textbf{spatial locality} (nearby inputs are strongly related) and \textbf{translation equivariance} (patterns can appear anywhere). The use of shared kernels (parameter sharing) makes them highly parameter-efficient for grid data compared to fully connected layers. Stacking convolutional layers allows the network to learn a hierarchy of patterns (e.g., edges $\to$ textures $\to$ object parts $\to$ objects). Their strong inductive bias makes them highly effective for image, video, and sometimes sequence processing. Capacity relates to depth, number of filters, and filter size.
    \item \textbf{Supervised/Unsupervised Use:} Widely used in supervised tasks (image classification, object detection, segmentation) but also key components in unsupervised/generative models involving images (GANs, VAEs, Diffusion Models).
    \item \textbf{Example Use Case:} The backbone of most state-of-the-art computer vision models for tasks like identifying objects in photographs.
\end{itemize}

\subsection{Recurrent Layers (RNNs, LSTMs, GRUs)}

\begin{itemize}
    \item \textbf{Description:} Designed for sequential data (e.g., text, time series). RNNs process input sequences one element at a time ($x_t$), maintaining an internal \emph{hidden state} ($h_t$) that summarizes information from previous steps. LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are advanced RNN variants that use gating mechanisms to better control the flow of information and combat the vanishing gradient problem, allowing them to learn longer-range dependencies.
    \item \textbf{Mathematical Transformation (Simple RNN):}
        \begin{align}
            \bh_t &= f(\bW_{hh}\bh_{t-1} + \bW_{xh}\bx_t + \bb_h) \quad \text{(Hidden state update)} \\
            \by_t &= g(\bW_{hy}\bh_t + \bb_y) \quad \text{(Output)}
        \end{align}
        where $f, g$ are activation functions. LSTMs/GRUs have more complex update equations involving input, forget, and output gates (using sigmoid/tanh) to manage $\bh_t$ and potentially a separate cell state $\bc_t$.
    \item \textbf{Function Space / Representational Power:} RNNs can, in theory, represent any function computable by a Turing machine. They are designed to capture \textbf{temporal dependencies} and model functions of sequences where order matters. Their capacity relates to the size of the hidden state. While powerful, simple RNNs struggle with long sequences due to vanishing/exploding gradients. LSTMs/GRUs significantly improve the ability to learn long-range dependencies, expanding the practical function space they can learn. They have a strong inductive bias for sequentiality.
    \item \textbf{Supervised/Unsupervised Use:} Predominantly used in supervised sequence tasks (NLP, time series forecasting) but also for unsupervised sequence modeling (language modeling).
    \item \textbf{Example Use Case:} Machine translation, sentiment analysis of text, speech recognition, predicting the next value in a stock market time series.
\end{itemize}

\subsection{Normalization Layers (Batch Norm, Layer Norm)}

\begin{itemize}
    \item \textbf{Description:} Techniques used to stabilize and accelerate the training of deep neural networks by normalizing the inputs to a layer. Batch Normalization (BatchNorm) normalizes activations across the mini-batch for each feature. Layer Normalization (LayerNorm) normalizes activations across all features for each individual data sample in the batch. Both typically follow normalization with learned scaling ($\gamma$) and shifting ($\beta$) parameters.
    \item \textbf{Mathematical Transformation (Simplified BatchNorm for feature $k$):}
        \begin{enumerate}
            \item Compute mini-batch mean $\mu_{\mathcal{B}}^{(k)}$ and variance $(\sigma_{\mathcal{B}}^{(k)})^2$.
            \item Normalize: $\hat{x}_i^{(k)} = \frac{x_i^{(k)} - \mu_{\mathcal{B}}^{(k)}}{\sqrt{(\sigma_{\mathcal{B}}^{(k)})^2 + \epsilon}}$.
            \item Scale and shift: $y_i^{(k)} = \gamma^{(k)} \hat{x}_i^{(k)} + \beta^{(k)}$.
        \end{enumerate}
        LayerNorm performs steps 1-2 over the feature dimension instead of the batch dimension.
    \item \textbf{Function Space / Representational Power:} Normalization layers do not change the theoretical function space that can be represented, but they dramatically alter the \emph{optimization landscape}. By standardizing the inputs to layers, they reduce "internal covariate shift," allow for higher learning rates, reduce dependence on careful initialization, and can have a slight regularization effect. This makes it practically feasible to train deeper and more complex models that might otherwise fail to converge, effectively expanding the set of functions that can be *learned* in practice.
    \item \textbf{Supervised/Unsupervised Use:} Used widely in both contexts whenever deep architectures are involved (common in CNNs, MLPs, Transformers).
    \item \textbf{Example Use Case:} Applied after convolutional or fully connected layers and before the activation function in ResNets; used extensively within Transformer blocks.
\end{itemize}

\subsection{Residual Connections (ResNets)}

\begin{itemize}
    \item \textbf{Description:} A technique enabling the training of very deep networks. Instead of learning a direct mapping $H(x)$ from input $x$ to output $y$ of a block, the block learns a *residual* mapping $F(x)$, and the output is computed as $y = F(x) + x$ via a "skip connection" that bypasses the block.
    \item \textbf{Mathematical Transformation:}
        \begin{equation}
            \mathbf{y} = F(\mathbf{x}, \{\bW_i\}) + \mathbf{x}
        \end{equation}
        If the dimensions of $\mathbf{x}$ and $F(\mathbf{x})$ differ, a linear projection $W_s$ might be applied to $\mathbf{x}$ in the skip connection: $\mathbf{y} = F(\mathbf{x}, \{\bW_i\}) + W_s\mathbf{x}$.
    \item \textbf{Function Space / Representational Power:} Residual connections don't increase the theoretical capacity (a sufficiently large non-residual network could represent the same function), but they crucially make optimization much easier for deep networks. They allow layers to easily learn the identity function (by driving the weights in $F$ towards zero), ensuring that adding more layers doesn't degrade performance. This facilitates the learning of extremely deep function approximators by improving gradient flow (combating vanishing gradients) and possibly smoothing the loss landscape.
    \item \textbf{Supervised/Unsupervised Use:} Foundational in both, especially in state-of-the-art deep vision models (ResNets and variants) and Transformers.
    \item \textbf{Example Use Case:} Enabling CNNs with 50, 101, or even more layers for ImageNet classification, significantly improving performance over shallower networks.
\end{itemize}

\subsection{Attention Mechanisms}

\begin{itemize}
    \item \textbf{Description:} Allows a model to weigh the importance of different parts of an input sequence (or other representations) when producing an output at a specific position. It computes weights based on the relationship between a "query" element and several "key" elements, then uses these weights to aggregate corresponding "value" elements.
    \item \textbf{Mathematical Transformation (Scaled Dot-Product Attention):}
        Given Queries $\bQ$, Keys $\bK$, Values $\bV$, and key dimension $d_k$:
        \begin{equation}
            \text{Attention}(\bQ, \bK, \bV) = \softmax\left(\frac{\bQ\bK^T}{\sqrt{d_k}}\right)\bV
        \end{equation}
        The $\softmax$ ensures the weights sum to 1. Multi-Head Attention performs this in parallel with different learned linear projections of Q, K, V.
    \item \textbf{Function Space / Representational Power:} Attention mechanisms allow models to represent functions that capture \textbf{long-range dependencies} and context-dependent relationships efficiently, without being constrained by sequential processing (like RNNs) or fixed local receptive fields (like CNNs). They can model arbitrary pairwise interactions between input elements. The expressiveness depends on the dimensionality and the number of attention heads. They provide a powerful way to learn dynamic, input-dependent alignments or weightings.
    \item \textbf{Supervised/Unsupervised Use:} Critical in both supervised (translation, summarization, image captioning) and unsupervised settings (language modeling with Transformers).
    \item \textbf{Example Use Case:} In machine translation, allowing the model to focus on relevant source words when generating each target word. In image captioning, focusing on relevant image regions when generating descriptive words.
\end{itemize}

\subsection{Transformers}

\begin{itemize}
    \item \textbf{Description:} An architecture (introduced in "Attention Is All You Need") that relies almost entirely on attention mechanisms, particularly self-attention (where Q, K, and V come from the same input sequence), eschewing recurrence and often convolution. Standard blocks include multi-head self-attention followed by position-wise feed-forward networks (FFNs), with residual connections and layer normalization throughout. Positional information is typically added via input encodings.
    \item \textbf{Mathematical Transformation:} Composed of sequences of the attention mechanism (Eq. above) and FFNs ($FFN(x) = \text{ReLU}(\bW_1 x + \bb_1)\bW_2 + \bb_2$), combined using residuals and normalization.
    \item \textbf{Function Space / Representational Power:} Represents a highly parallelizable and expressive function class, particularly adept at modeling long-range dependencies in sequences. Its capacity is vast, scaling with depth, width (embedding dimension), and number of attention heads. Has become the dominant architecture for NLP, demonstrating state-of-the-art performance by learning complex context-dependent representations. Its weaker sequential inductive bias (compared to RNNs) is compensated for by positional encodings and its ability to relate any two sequence positions directly via self-attention. Also being successfully adapted for vision and other modalities.
    \item \textbf{Supervised/Unsupervised Use:} Both. Foundational for large language models (GPT, BERT - often pre-trained unsupervised, then fine-tuned supervised), machine translation, etc.
    \item \textbf{Example Use Case:} State-of-the-art models for natural language understanding, text generation, machine translation. Vision Transformers (ViTs) for image classification.
\end{itemize}

\subsection{Autoencoders (AEs)}

\begin{itemize}
    \item \textbf{Description:} An unsupervised learning framework consisting of two parts: an \emph{encoder} network that maps high-dimensional input data $\bx$ to a lower-dimensional latent representation $\bz$, and a \emph{decoder} network that reconstructs the original data $\hat{\bx}$ from $\bz$.
    \item \textbf{Mathematical Transformation:}
        Encoder: $\bz = f_{\phi}(\bx)$
        Decoder: $\hat{\bx} = g_{\psi}(\bz)$
        Objective: $\min_{\phi, \psi} L(\bx, \hat{\bx})$, where $L$ is a reconstruction loss (e.g., MSE or Binary Cross-Entropy). The dimension of $\bz$ is typically smaller than $\bx$.
    \item \textbf{Function Space / Representational Power:} AEs learn functions that perform non-linear dimensionality reduction. The encoder learns to map the data onto a potentially complex, lower-dimensional manifold capturing the main variations in the data. The decoder learns the inverse mapping. The quality of the representation depends on the bottleneck size (capacity of the latent space) and the capacity of the encoder/decoder networks. They don't explicitly model the data probability distribution $p(x)$.
    \item \textbf{Supervised/Unsupervised Use:} Primarily unsupervised (dimensionality reduction, feature learning, data compression, denoising). Latent features can sometimes be used for downstream supervised tasks.
    \item \textbf{Example Use Case:} Learning compressed representations of images for efficient storage or visualization (e.g., using t-SNE on the latent codes $\bz$). Denoising autoencoders are trained to reconstruct clean images from corrupted ones.
\end{itemize}

\subsection{Variational Autoencoders (VAEs)}

\begin{itemize}
    \item \textbf{Description:} A generative, unsupervised model structurally similar to AEs but with a probabilistic approach. The encoder outputs parameters (mean $\bmu_{\bz}$ and log-variance $\log \bsigma_{\bz}^2$) of a distribution over the latent space (typically Gaussian). A latent code $\bz$ is *sampled* from this distribution. The decoder generates data $\hat{\bx}$ from the sampled $\bz$.
    \item \textbf{Mathematical Transformation:}
        Encoder: $\bmu_{\bz}, \log \bsigma_{\bz}^2 = f_{\phi}(\bx)$
        Sampling (Reparameterization Trick): $\bz = \bmu_{\bz} + \bsigma_{\bz} \odot \bepsilon$, where $\bepsilon \sim \mathcal{N}(0, \bI)$
        Decoder: $\hat{\bx} \sim p_{\psi}(\bx | \bz)$ (Outputs parameters of data distribution, e.g., mean for Gaussian or probabilities for Bernoulli)
        Objective: Maximize the Evidence Lower Bound (ELBO):
        $\mathcal{L}(\phi, \psi; \bx) = \mathbb{E}_{\bz \sim q_{\phi}(\bz|\bx)}[\log p_{\psi}(\bx|\bz)] - D_{KL}(q_{\phi}(\bz|\bx) || p(\bz))$
        where $q_{\phi}(\bz|\bx) = \mathcal{N}(\bz | \bmu_{\bz}, \bsigma_{\bz}^2 \bI)$ is the approximate posterior and $p(\bz)$ is the prior (usually $\mathcal{N}(0, \bI)$).
    \item \textbf{Function Space / Representational Power:} VAEs are generative models that learn to approximate the true data distribution $p(x)$. They learn a function (the decoder) that maps a simple latent distribution (the prior $p(z)$) to the complex data distribution. The KL divergence term acts as a regularizer, forcing the learned latent representations $q_\phi(z|x)$ to be close to the prior, resulting in a smooth and structured latent space suitable for generating novel data by sampling $z \sim p(z)$ and decoding. Can represent complex probability distributions, though generated samples are sometimes perceived as blurrier than GANs.
    \item \textbf{Supervised/Unsupervised Use:} Primarily unsupervised (generative modeling, representation learning, latent space interpolation).
    \item \textbf{Example Use Case:} Generating novel images (e.g., faces, digits) by sampling from the prior $\mathcal{N}(0, \bI)$ and decoding. Discovering meaningful latent factors of variation in data.
\end{itemize}

\subsection{Generative Adversarial Networks (GANs)}

\begin{itemize}
    \item \textbf{Description:} An unsupervised generative framework involving two networks trained in competition: a \emph{Generator} ($G$) that tries to create realistic data samples from random noise input ($\bz$), and a \emph{Discriminator} ($D$) that tries to distinguish between real data samples ($\bx$) and fake samples ($\hat{\bx} = G(\bz)$).
    \item \textbf{Mathematical Transformation:}
        Generator: $\hat{\bx} = G_{\psi}(\bz)$, where $\bz \sim p(\bz)$ (e.g., Gaussian noise).
        Discriminator: $p_{real} = D_{\phi}(\bx)$ (outputs probability that input is real).
        Objective (Minimax Game):
        $\min_{\psi} \max_{\phi} V(D, G) = \mathbb{E}_{\bx \sim p_{data}}[\log D_{\phi}(\bx)] + \mathbb{E}_{\bz \sim p_z}[\log(1 - D_{\phi}(G_{\psi}(\bz)))]$
        The discriminator $D$ tries to maximize this (correctly classify real and fake), while the generator $G$ tries to minimize it (fool the discriminator). Many variations on the loss exist (e.g., WGAN, LSGAN).
    \item \textbf{Function Space / Representational Power:} GANs implicitly learn to represent the data distribution $p_{data}(x)$. The generator learns a transformation from a simple noise distribution to the complex data distribution. In theory, given sufficient capacity and perfect optimization, the generator's output distribution can perfectly match the real data distribution. GANs are known for producing sharp, high-fidelity samples but can suffer from training instability (e.g., mode collapse, where the generator produces only limited varieties of outputs). The function space is determined by the capacity of the G and D networks (often deep CNNs).
    \item \textbf{Supervised/Unsupervised Use:} Primarily unsupervised (image generation, style transfer, super-resolution, data augmentation). Conditional GANs incorporate supervised labels.
    \item \textbf{Example Use Case:} Generating highly realistic faces, translating images from one domain to another (e.g., horses to zebras), upscaling low-resolution images.
\end{itemize}

\subsection{Diffusion Models (DDPMs)}

\begin{itemize}
    \item \textbf{Description:} Generative models inspired by non-equilibrium thermodynamics. They involve a fixed \emph{forward process} that gradually adds Gaussian noise to data over $T$ steps, and a learned \emph{reverse process} that starts from pure noise and iteratively denoises it using a neural network to produce a sample.
    \item \textbf{Mathematical Transformation (Reverse Step Core):}
        Given noisy sample $\bx_t$ at step $t$, predict noise $\hat{\bepsilon} = \bepsilon_\theta(\bx_t, t)$ using a neural network (often U-Net based). Compute the previous state $\bx_{t-1}$ via:
        \begin{equation}
             \bx_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \bx_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\bepsilon} \right) + \sigma_t \bz
        \end{equation}
        where $\alpha_t, \beta_t, \bar{\alpha}_t, \sigma_t$ are derived from the forward process noise schedule, and $\bz \sim \mathcal{N}(0, \bI)$ (or $\bz=0$ for $t=1$).
    \item \textbf{Function Space / Representational Power:} Diffusion models learn to approximate the (intractable) score function $\nabla_{\bx_t} \log p(\bx_t)$ of the noisy data distributions, or equivalently, learn to perform the denoising step. By iteratively applying this learned denoising function (parameterized by the flexible $\bepsilon_\theta$ network) over many steps ($T$ is large), they can model extremely complex data distributions. The function space is largely determined by the capacity of the underlying denoising network $\bepsilon_\theta$. They currently achieve state-of-the-art results in high-fidelity image and audio generation.
    \item \textbf{Supervised/Unsupervised Use:} Primarily unsupervised generative modeling. Can be conditioned on inputs (like text) for conditional generation.
    \item \textbf{Example Use Case:} Text-to-image generation (DALL-E 2, Imagen, Stable Diffusion), high-fidelity image generation, audio synthesis.
\end{itemize}

\subsection{Conclusion}

Modern deep learning relies on combining these and other specialized building blocks. Linear, Convolutional, and Recurrent layers provide core processing capabilities tailored to different data types. Normalization, Residual Connections, and Attention enhance training stability, depth, and the ability to model complex dependencies. Unsupervised frameworks like AEs, VAEs, GANs, and Diffusion Models leverage these blocks to learn representations and generate data. Understanding the mathematical transformations and representational capabilities of each block is key to designing effective deep learning architectures for diverse tasks.









\section{Objective Functions}


\subsection{Introduction: Objective Function Derivation}

At its core, much of machine learning involves building models that can learn from data. A crucial part of "learning" is \emph{optimization}: finding the set of parameters for our chosen model that best explains the observed data or performs best on a given task. How we define "best" leads to different optimization frameworks. Two fundamental perspectives dominate: Maximum Likelihood Estimation (MLE) and the Bayesian approach culminating in Maximum A Posteriori (MAP) estimation. These frameworks provide principled ways to derive the objective functions (often called loss functions when minimizing) that we use to train machine learning models. We will explore these, including the specific case of Conditional MLE, and relate them to core concepts from information theory like KL divergence and cross-entropy.

Let $\bm{\theta}$ denote the parameters of our model, and let $\mathcal{D}$ represent our dataset. Our goal is generally to find the optimal $\bm{\theta}^*$.


\subsection{Specific Methods to Find Objective Functions}
You may have come across objective functions used in a variety of fitting applications. A common objective function is the linear least square method which attempts to minimize the error between the models prediction and your dataset. The problem with this is that, when you first learn about it in linear algebra, it is framed as being derived only for linear regression. But it is used quite despite that original derivation requiring some heavy assumptions that would not make it suitable for non-linear functions. So how do we go about deriving an objective function for every ML task? The answer is that we typically do not do this. Instead, we rely on principles like maximum likelihood estimations which can significantly simplify the derivation process. These can also have mathematical proofs done on them showing they have certain convergence properties to the data distribution if you use them to guide your objective function choice.

\subsection{A Broad View of Objective Function Techniques}

While we will wind up focusing in on Maximum Likelihood Estimation, it is important to understand that there are other methods that can be used to generate objective functions. While, this section will likely not be very informative prior to training a model, it should be revisited when you are considering how to \textbf{best represent your data in an ML model.}\\
Besides Maximum Likelihood Estimation (MLE), several other methods exist for estimating parameters or fitting models to data. Here are brief descriptions of some prominent ones:

\begin{enumerate}

    \item \textbf{Maximum A Posteriori (MAP) Estimation:}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} A Bayesian approach that finds the parameter values that are most probable given the observed data \emph{and} prior beliefs about the parameters. It maximizes the posterior distribution $p(\theta | \text{Data}) \propto p(\text{Data} | \theta) p(\theta)$.
            \item \textbf{Difference from MLE:} Incorporates a prior distribution $p(\theta)$, which acts as a regularizer, often preventing overfitting, especially with limited data. MLE is equivalent to MAP with a uniform (flat) prior.
        \end{itemize}

    \item \textbf{Full Bayesian Inference:}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} Instead of finding a single best parameter estimate (a point estimate), this approach aims to determine the entire \emph{posterior probability distribution} $p(\theta | \text{Data})$.
            \item \textbf{Difference from MLE/MAP:} Provides a full characterization of uncertainty about the parameters. Predictions are often made by integrating or averaging over this posterior distribution. It typically requires computationally intensive methods like Markov Chain Monte Carlo (MCMC) or Variational Inference (VI).
        \end{itemize}

    \item \textbf{Method of Moments (MoM):}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} Equates theoretical moments of the assumed probability distribution (which are functions of the parameters) to the corresponding sample moments calculated from the data. The parameters are then found by solving the resulting system of equations.
            \item \textbf{Difference from MLE:} Often computationally simpler but potentially less statistically efficient (i.e., estimates might have higher variance). Doesn't require specifying the full likelihood function, only moments.
        \end{itemize}

    \item \textbf{Generalized Method of Moments (GMM):}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} An extension of MoM, particularly useful when you have more moment conditions (theoretical expectations that should equal zero) than parameters. It finds parameters that minimize a weighted quadratic form of the differences between sample moments and theoretical moments.
            \item \textbf{Difference from MoM/MLE:} More flexible than MoM. Often used when the likelihood function is difficult or impossible to specify, common in econometrics.
        \end{itemize}

    \item \textbf{Minimum Distance Estimation:}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} Selects parameters that minimize a chosen statistical distance (e.g., Hellinger distance, Cramér-von Mises distance) between the probability distribution implied by the model and the empirical distribution of the data.
            \item \textbf{Difference from MLE:} MLE minimizes the KL divergence, which is one type of statistical distance. Other distance measures can lead to different estimators, sometimes with better robustness properties.
        \end{itemize}

    \item \textbf{Robust Estimation Methods (e.g., M-estimators):}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} Designed to be less sensitive to outliers or violations of model assumptions compared to methods like MLE (which can be heavily influenced by outliers, e.g., least squares). They typically involve modifying the objective function to down-weight the influence of extreme data points (e.g., using Huber loss instead of squared error).
            \item \textbf{Difference from MLE:} Prioritizes robustness over statistical efficiency under ideal conditions.
        \end{itemize}

    \item \textbf{Empirical Risk Minimization (ERM):}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} A general framework dominant in machine learning. It defines a \emph{loss function} that measures the error between a model's prediction and the true outcome. ERM finds the parameters that minimize the average loss over the training dataset (the "empirical risk").
            \item \textbf{Difference from MLE:} More general. MLE is a specific case of ERM where the loss function is the negative log-likelihood. ERM allows using other loss functions (like hinge loss in SVMs) that may not correspond directly to maximizing the likelihood of a simple probabilistic model but have desirable properties (e.g., sparsity, margin maximization).
        \end{itemize}

    \item \textbf{Approximate Bayesian Computation (ABC):}
        \begin{itemize}[label=\textbullet, leftmargin=*]
            \item \textbf{Concept:} A class of simulation-based, likelihood-free methods used when the likelihood function $p(\text{Data} | \theta)$ is intractable. It involves simulating data from the model under different parameters drawn from the prior and accepting parameters whose simulated data closely matches the observed data (based on summary statistics).
            \item \textbf{Difference from MLE/Full Bayes:} Avoids direct computation of the likelihood, approximating the posterior distribution instead.
        \end{itemize}

\end{enumerate}

These methods offer different trade-offs in terms of computational complexity, statistical efficiency, robustness, and the type of information they provide (point estimates vs. full distributions). The choice often depends on the specific problem, data characteristics, model complexity, and goals of the analysis.




\subsection{Maximum Likelihood Estimation (MLE)}

\subsubsection{Concept and Goal}
The principle behind MLE is intuitive: we want to find the parameter values $\bm{\theta}$ that make the observed data $\mathcal{D}$ \emph{most probable} or \emph{most likely} under our model $p(\cdot | \bm{\theta})$. It asks: "Given our model structure, what parameters maximize the probability of seeing the data we actually saw?" This approach treats the parameters $\bm{\theta}$ as fixed, unknown quantities to be estimated solely based on the data.

\subsubsection{Mathematical Formulation}
Let the dataset be $\mathcal{D} = \{ \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(N)} \}$, assuming the data points are independent and identically distributed (i.i.d.). The probability of observing this entire dataset given the parameters $\bm{\theta}$ is the \emph{likelihood function}:
\begin{equation}
    L(\bm{\theta}; \mathcal{D}) = p(\mathcal{D} | \bm{\theta}) = \prod_{i=1}^{N} p(\mathbf{x}^{(i)} | \bm{\theta})
\end{equation}
The MLE estimate $\hat{\bm{\theta}}_{MLE}$ is the value of $\bm{\theta}$ that maximizes this likelihood:
\begin{equation}
    \hat{\bm{\theta}}_{MLE} = \argmax_{\bm{\theta}} L(\bm{\theta}; \mathcal{D}) = \argmax_{\bm{\theta}} \prod_{i=1}^{N} p(\mathbf{x}^{(i)} | \bm{\theta})
\end{equation}
In practice, working with products is numerically unstable and mathematically inconvenient. Since the logarithm is a monotonically increasing function, maximizing the likelihood is equivalent to maximizing the \emph{log-likelihood}. This is only the case for the location of the maximum which in our instance corresponds to the parameters. Taking the log does in fact change the value of the maximum or peak, but since we are only worried about what parameters maximize our likelihood of seeing the data, we do not need to worry about this:
\begin{equation}
    \ell(\bm{\theta}; \mathcal{D}) = \log L(\bm{\theta}; \mathcal{D}) = \log \prod_{i=1}^{N} p(\mathbf{x}^{(i)} | \bm{\theta}) = \sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} | \bm{\theta})
\end{equation}
So, the MLE estimate is typically found by solving:
\begin{equation} \label{eq:mle_opt}
    \hat{\bm{\theta}}_{MLE} = \argmax_{\bm{\theta}} \sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} | \bm{\theta})
\end{equation}
This is often reformulated as minimizing the \emph{negative log-likelihood} (NLL):
\begin{equation}
    \hat{\bm{\theta}}_{MLE} = \argmin_{\bm{\theta}} -\sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} | \bm{\theta})
\end{equation}

\subsubsection{Connection to KL Divergence and Cross-Entropy}
The MLE objective is deeply connected to minimizing the difference between the true data distribution and the distribution represented by our model. Let $\hat{p}_{data}(\mathbf{x})$ be the empirical distribution defined by the training data (essentially, a distribution that places probability $1/N$ on each observed data point $\mathbf{x}^{(i)}$ and 0 elsewhere). Let $p_{model}(\mathbf{x} | \bm{\theta})$ be the distribution defined by our model with parameters $\bm{\theta}$.

We want to find $\bm{\theta}$ such that $p_{model}(\mathbf{x} | \bm{\theta})$ is "close" to $\hat{p}_{data}(\mathbf{x})$. A standard measure of closeness between two distributions $P$ and $Q$ is the Kullback-Leibler (KL) divergence:
\begin{equation}
    D_{KL}(P || Q) = \mathbb{E}_{\mathbf{x} \sim P} \left[ \log \frac{P(\mathbf{x})}{Q(\mathbf{x})} \right] = \sum_{\mathbf{x}} P(\mathbf{x}) \log \frac{P(\mathbf{x})}{Q(\mathbf{x})} \quad (\text{or } \int P(\mathbf{x}) \log \frac{P(\mathbf{x})}{Q(\mathbf{x})} d\mathbf{x})
\end{equation}
Minimizing the KL divergence between the empirical distribution and the model distribution corresponds to finding the best model approximation:
\begin{align}
    \argmin_{\bm{\theta}} D_{KL}(\hat{p}_{data} || p_{model}(\cdot | \bm{\theta})) &= \argmin_{\bm{\theta}} \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} \left[ \log \frac{\hat{p}_{data}(\mathbf{x})}{p_{model}(\mathbf{x} | \bm{\theta})} \right] \\
    &= \argmin_{\bm{\theta}} \left( \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} [\log \hat{p}_{data}(\mathbf{x})] - \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} [\log p_{model}(\mathbf{x} | \bm{\theta})] \right)
\end{align}
The first term, $\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} [\log \hat{p}_{data}(\mathbf{x})]$, is the negative entropy of the empirical distribution, $-H(\hat{p}_{data})$, which does not depend on $\bm{\theta}$. Since it does not change with $\bm{\theta}$, it is constant for any choice of $\bm{\theta}$. Constants in our landscape simply move the value of our maxima or minima up and down and do not affect its location. Therefore, minimizing the KL divergence is equivalent to minimizing the second term:
\begin{equation}
    \argmin_{\bm{\theta}} - \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} [\log p_{model}(\mathbf{x} | \bm{\theta})]
\end{equation}
This term is precisely the definition of the \emph{cross-entropy} between $\hat{p}_{data}$ and $p_{model}(\cdot|\bm{\theta})$, denoted $H(\hat{p}_{data}, p_{model}(\cdot|\bm{\theta}))$. Evaluating the expectation under the empirical distribution gives:
\begin{equation}
    - \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} [\log p_{model}(\mathbf{x} | \bm{\theta})] = - \sum_{i=1}^{N} \frac{1}{N} \log p_{model}(\mathbf{x}^{(i)} | \bm{\theta}) = \frac{1}{N} \left( -\sum_{i=1}^{N} \log p_{model}(\mathbf{x}^{(i)} | \bm{\theta}) \right)
\end{equation}
Minimizing this is equivalent to minimizing the negative log-likelihood objective we derived for MLE in Equation \ref{eq:mle_opt} (up to a constant factor $1/N$). The clever part to note is that the sum over $1/N$ in this equation is the fill in for our probability of $\mathbf{x}$. 

\textbf{In short: Maximizing the likelihood (MLE) is equivalent to minimizing the KL divergence between the empirical data distribution and the model distribution, which is also equivalent to minimizing the cross-entropy between them.}


\subsection{Conditional Maximum Likelihood Estimation (C-MLE)}

\subsubsection{Concept and Goal}
Often, particularly in supervised learning, we are not interested in modeling the distribution of inputs $\mathbf{x}$, but rather in predicting outputs $\mathbf{y}$ given inputs $\mathbf{x}$. Conditional Maximum Likelihood Estimation focuses on finding parameters $\bm{\theta}$ that maximize the probability of the observed outputs $\mathbf{y}^{(i)}$ given the corresponding inputs $\mathbf{x}^{(i)}$.

\subsubsection{Mathematical Formulation}
Let the dataset be $\mathcal{D} = \{ (\mathbf{x}^{(1)}, \mathbf{y}^{(1)}), ..., (\mathbf{x}^{(N)}, \mathbf{y}^{(N)}) \}$. The conditional likelihood is the probability of observing the outputs $Y = \{\mathbf{y}^{(1)}, ..., \mathbf{y}^{(N)}\}$ given the inputs $X = \{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(N)}\}$ and parameters $\bm{\theta}$:
\begin{equation}
    L(\bm{\theta}; Y|X) = p(Y | X; \bm{\theta}) = \prod_{i=1}^{N} p(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}; \bm{\theta})
\end{equation}
The C-MLE estimate $\hat{\bm{\theta}}_{C-MLE}$ maximizes this conditional likelihood, or equivalently, the conditional log-likelihood:
\begin{equation}
    \ell(\bm{\theta}; Y|X) = \log L(\bm{\theta}; Y|X) = \sum_{i=1}^{N} \log p(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}; \bm{\theta})
\end{equation}
\begin{equation} \label{eq:cmle_opt}
    \hat{\bm{\theta}}_{C-MLE} = \argmax_{\bm{\theta}} \sum_{i=1}^{N} \log p(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}; \bm{\theta})
\end{equation}
Again, this is usually framed as minimizing the negative conditional log-likelihood.

\subsubsection{Relationship to Cross-Entropy}
The connection to cross-entropy is very direct here, especially in classification tasks. Consider a classification problem where $\mathbf{y}^{(i)}$ is a one-hot encoded vector representing the true class for input $\mathbf{x}^{(i)}$, and $p(\mathbf{y} | \mathbf{x}^{(i)}; \bm{\theta})$ represents the vector of predicted probabilities for each class given $\mathbf{x}^{(i)}$. The negative conditional log-likelihood for a single data point is:
\begin{equation}
    -\log p(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}; \bm{\theta}) = -\sum_{k=1}^{K} y^{(i)}_k \log p(y_k=1 | \mathbf{x}^{(i)}; \bm{\theta})
\end{equation}
where $K$ is the number of classes, $y^{(i)}_k$ is 1 if the true class is $k$ and 0 otherwise, and $p(y_k=1 | \mathbf{x}^{(i)}; \bm{\theta})$ is the model's predicted probability for class $k$. This is exactly the definition of the \emph{cross-entropy loss} between the true label distribution (one-hot vector $\mathbf{y}^{(i)}$) and the model's predicted probability distribution $p(\mathbf{y} | \mathbf{x}^{(i)}; \bm{\theta})$. Minimizing the sum over all data points, as in Equation \ref{eq:cmle_opt} (when negated), is precisely minimizing the average cross-entropy loss, a standard objective for training classifiers.

\subsection{MLE Example on Linear Regression}
Everything we have talked about so far is fine and dandy, but we have yet to come up with any real loss or objective function. We have only really talked about probabilities. So how do we make this connection between our predictive model and a probability? The answer lies in making the realization that often what we are predicting is an average value for some distribution. To make this clear lets switch to a concrete example of linear regression.\\
When we perform linear regression, we are interested in finding a line of best fit for our data. But what exactly do we mean by "best" line. Lets think critically about what properties might qualify as a "best" line. First at each $\mathbf{x}^{(i)}$ in our dataset, our output in our data is not guaranteed to be a single value since noise always exists. So instead lets switch to modeling our...

\subsection{Bayesian Approach: Maximum A Posteriori (MAP) Estimation}

\subsubsection{Concept and Goal}
While MLE finds parameters that best explain the data, it doesn't incorporate any prior knowledge we might have about the parameters. The Bayesian approach treats parameters $\bm{\theta}$ not as fixed unknowns, but as random variables themselves, having a \emph{prior distribution} $p(\bm{\theta})$ that encodes our beliefs before seeing any data. After observing data $\mathcal{D}$, we update our beliefs using Bayes' theorem to obtain the \emph{posterior distribution} $p(\bm{\theta} | \mathcal{D})$.

Maximum A Posteriori (MAP) estimation seeks to find the single point estimate for $\bm{\theta}$ that is most probable given the data and our prior beliefs, i.e., the mode of the posterior distribution.

\subsubsection{Mathematical Formulation}
Bayes' theorem states:
\begin{equation}
    p(\bm{\theta} | \mathcal{D}) = \frac{p(\mathcal{D} | \bm{\theta}) p(\bm{\theta})}{p(\mathcal{D})}
\end{equation}
Here:
\begin{itemize}
    \item $p(\bm{\theta} | \mathcal{D})$ is the \textbf{posterior} distribution (our belief about $\bm{\theta}$ after seeing data).
    \item $p(\mathcal{D} | \bm{\theta})$ is the \textbf{likelihood} (same as in MLE, how probable the data is given $\bm{\theta}$).
    \item $p(\bm{\theta})$ is the \textbf{prior} distribution (our belief about $\bm{\theta}$ before seeing data).
    \item $p(\mathcal{D}) = \int p(\mathcal{D} | \bm{\theta}') p(\bm{\theta}') d\bm{\theta}'$ is the \textbf{evidence} or marginal likelihood of the data, acting as a normalization constant.
\end{itemize}
The MAP estimate $\hat{\bm{\theta}}_{MAP}$ maximizes the posterior probability:
\begin{equation}
    \hat{\bm{\theta}}_{MAP} = \argmax_{\bm{\theta}} p(\bm{\theta} | \mathcal{D}) = \argmax_{\bm{\theta}} \frac{p(\mathcal{D} | \bm{\theta}) p(\bm{\theta})}{p(\mathcal{D})}
\end{equation}
Since $p(\mathcal{D})$ does not depend on $\bm{\theta}$, we can ignore it in the maximization:
\begin{equation}
    \hat{\bm{\theta}}_{MAP} = \argmax_{\bm{\theta}} p(\mathcal{D} | \bm{\theta}) p(\bm{\theta})
\end{equation}
As with MLE, it's often easier to work with the logarithm. So we can apply the $log$ to the full expression and move it inside the $argmax$ since it only impacts the magnitude and not the location of maximization:
\begin{equation} \label{eq:map_opt}
    \hat{\bm{\theta}}_{MAP} = \argmax_{\bm{\theta}} \left[ \log p(\mathcal{D} | \bm{\theta}) + \log p(\bm{\theta}) \right]
\end{equation}
This clearly shows the MAP objective is the sum of the log-likelihood (the MLE objective) and the log-prior.

\subsubsection{Relationship to Regularization}
The log-prior term $\log p(\bm{\theta})$ often acts as a \emph{regularization} term in the optimization objective, preventing the parameters from overfitting to the training data by penalizing overly complex models (as defined by the prior). This basically allows us to input what we expect $\log p(\bm{\theta})$ to be and favor distributions that are closer to the expectation.
\begin{itemize}
    \item \textbf{L2 Regularization (Weight Decay):} If we assume a Gaussian prior on the parameters, centered at zero, $p(\bm{\theta}) \propto \exp\left(-\frac{1}{2\sigma^2} ||\bm{\theta}||_2^2\right)$, then the log-prior is $\log p(\bm{\theta}) = -\frac{1}{2\sigma^2} ||\bm{\theta}||_2^2 + \text{constant}$. Maximizing the MAP objective (Equation \ref{eq:map_opt}) becomes equivalent to minimizing:
    \begin{equation}
        -\log p(\mathcal{D} | \bm{\theta}) + \lambda ||\bm{\theta}||_2^2 \quad (\text{where } \lambda = \frac{1}{2\sigma^2})
    \end{equation}
    This is the standard objective for models trained with L2 regularization. This winds up penalizing large parameter values and favors small ones. You can vary the value of $\lambda$ to modulate this effect.
    \item \textbf{L1 Regularization (Lasso):} If we assume a Laplace prior, $p(\bm{\theta}) \propto \exp\left(-\frac{1}{b} ||\bm{\theta}||_1\right)$, then the log-prior is $\log p(\bm{\theta}) = -\frac{1}{b} ||\bm{\theta}||_1 + \text{constant}$. Maximizing the MAP objective becomes equivalent to minimizing:
    \begin{equation}
        -\log p(\mathcal{D} | \bm{\theta}) + \lambda ||\bm{\theta}||_1 \quad (\text{where } \lambda = \frac{1}{b})
    \end{equation}
    This is the objective for models trained with L1 regularization, known for promoting sparsity in $\bm{\theta}$. Unlike the L2 regularization, this regularization technique does not squish our values further with a squared operation. This is what promotes sparsity, and small values do not get squished and instead need to almost entirely vanish.
\end{itemize}
Thus, MAP estimation provides a probabilistic justification for common regularization techniques.

\subsubsection{Comparison with MLE}
\begin{itemize}
    \item MAP incorporates prior beliefs, while MLE does not (or can be seen as MAP with a uniform, improper prior).
    \item The prior acts as a regularizer, potentially preventing overfitting, especially with limited data.
    \item As the amount of data $N$ increases, the influence of the log-likelihood term $\log p(\mathcal{D}|\bm{\theta}) = \sum \log p(\mathbf{x}^{(i)}|\bm{\theta})$ typically dominates the log-prior term $\log p(\bm{\theta})$. Consequently, the MAP estimate $\hat{\bm{\theta}}_{MAP}$ usually converges to the MLE estimate $\hat{\bm{\theta}}_{MLE}$ as $N \to \infty$.
\end{itemize}

\subsubsection{Conclusion}

Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation represent two fundamental philosophies for deriving optimization objectives in machine learning. MLE seeks parameters that make the observed data most probable, and is equivalent to minimizing the KL divergence or cross-entropy between the empirical data distribution and the model distribution. Conditional MLE applies this principle to supervised learning tasks, directly motivating the use of cross-entropy loss for classification. MAP incorporates prior beliefs about parameters, finding the most probable parameters given both data and prior. This Bayesian perspective provides a theoretical foundation for common regularization techniques like L1 and L2, showing how they arise from specific prior assumptions about the parameters. Understanding these frameworks is crucial for interpreting the loss functions used to train countless machine learning models.


\section{Optimization}

\subsection{Introduction: Optimizing Models}

Machine learning models learn by minimizing a \emph{loss function} (or cost function), $J(\btheta)$, which measures how poorly the model performs on the training data given its current parameters $\btheta$. The goal of training is to find the parameter values $\btheta^*$ that result in the minimum loss. Gradient Descent is a family of iterative optimization algorithms that form the backbone of how most machine learning models, especially deep neural networks, are trained. They work by repeatedly taking steps in the direction opposite to the gradient of the loss function with respect to the parameters. This document explores the evolution of gradient descent, from the basic batch method to more sophisticated adaptive techniques. We denote the learning rate by $\eta$.

\subsection{(Batch) Gradient Descent (GD)}

\subsubsection{Historical Context/Motivation}
This is the original and most conceptually straightforward gradient descent algorithm. It aligns directly with the mathematical definition of minimizing a function using its gradient. It was needed as a fundamental way to iteratively approach the minimum of a function when a closed-form solution is unavailable or impractical.

\subsubsection{Intuition}
Imagine standing on a hillside (the loss surface) and wanting to get to the bottom (minimum loss). You look around in all directions (calculate the gradient over the entire dataset), find the direction of steepest descent (opposite the gradient), and take a step in that direction. Repeat until you reach the valley floor. Because you use the entire dataset to determine the direction, each step is taken in the objectively "best" direction according to the overall loss landscape.

\subsubsection{Mathematical Formulation}
The loss $J(\btheta)$ is typically the average loss over the entire dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^N$:
\begin{equation}
    J(\btheta) = \frac{1}{N} \sum_{i=1}^{N} L(f(x^{(i)}; \btheta), y^{(i)})
\end{equation}
where $L$ is the loss for a single example and $f(x^{(i)}; \btheta)$ is the model's prediction. The update rule involves computing the gradient $\nabla_{\btheta} J(\btheta)$ using all $N$ samples:
\begin{equation}
    \btheta_{\text{new}} = \btheta_{\text{old}} - \eta \nabla_{\btheta} J(\btheta_{\text{old}}) = \btheta_{\text{old}} - \eta \left( \frac{1}{N} \sum_{i=1}^{N} \nabla_{\btheta} L(f(x^{(i)}; \btheta_{\text{old}}), y^{(i)}) \right)
\end{equation}

\subsubsection{Pros}
\begin{itemize}
    \item Calculates the true gradient of the loss function over the entire dataset, leading to stable convergence towards a minimum (local or global, depending on convexity).
    \item Convergence path is generally smooth.
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item Computationally very expensive and slow for large datasets, as the \textbf{entire dataset} must be processed for \emph{each single parameter update}.
    \item Requires loading the entire dataset into memory, which can be infeasible (I'm looking at you, Jeff).
    \item Can get stuck in poor local minima for non-convex loss landscapes (common in deep learning).
\end{itemize}

\subsubsection{Basic Calculation Example}
Let's minimize $J(\theta) = \theta^2$ starting at $\theta_0 = 4$ with $\eta = 0.1$. The gradient is $\nabla_{\theta} J(\theta) = 2\theta$. (Here, the "batch" is trivial as the function doesn't depend on data).
\begin{itemize}
    \item \textbf{Iteration 1:} $\nabla J(4) = 2 \times 4 = 8$. $\theta_1 = 4 - 0.1 \times 8 = 3.2$.
    \item \textbf{Iteration 2:} $\nabla J(3.2) = 2 \times 3.2 = 6.4$. $\theta_2 = 3.2 - 0.1 \times 6.4 = 2.56$.
    \item \textbf{Iteration 3:} $\nabla J(2.56) = 2 \times 2.56 = 5.12$. $\theta_3 = 2.56 - 0.1 \times 5.12 = 2.048$.
\end{itemize}
The parameter $\theta$ moves towards the minimum at 0.

\subsection{Stochastic Gradient Descent (SGD)}

\subsubsection{Historical Context/Motivation}
Batch GD's computational burden with large datasets motivated SGD. Why wait to process millions of examples for one small step? SGD updates parameters based on the gradient computed from just \emph{one} randomly chosen training example at a time. This dramatically speeds up the number of updates per unit of time and allows for learning from data streams (online learning). The term Incremental Gradient Descent (IGD) is often used synonymously, emphasizing the update after each sample.

\subsubsection{Intuition}
Instead of carefully surveying the entire landscape for the best direction down, you just look at the slope under your feet based on \emph{one single example} and take a step downhill according to that limited view. The steps are very noisy and might not always go directly toward the true minimum, but they are very fast. Over many steps, the path tends towards the minimum, like a person stumbling downhill – lots of variation, but overall progress downwards. The noise can help escape shallow local minima.

\subsubsection{Mathematical Formulation}
In each iteration $t$, select one sample $(x^{(i)}, y^{(i)})$ randomly from the dataset. Compute the gradient of the loss $L$ for \emph{that single sample}: $g_t = \nabla_{\btheta} L(f(x^{(i)}; \btheta_t), y^{(i)})$. The update rule is:
\begin{equation}
    \btheta_{t+1} = \btheta_t - \eta g_t
\end{equation}

\subsubsection{Pros}
\begin{itemize}
    \item Much faster computation per update (only one sample).
    \item Lower memory requirement (no need to store full dataset gradients).
    \item Suitable for very large datasets and online learning scenarios.
    \item The noise in updates can help escape sharp/poor local minima.
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item High variance in parameter updates due to noisy gradient estimates.
    \item Convergence path oscillates significantly.
    \item May require more iterations (epochs) to converge compared to batch GD, although each iteration is faster.
    * Loss function may not decrease smoothly.
    \item Sensitive to learning rate; often requires careful tuning and learning rate decay schedules.
\end{itemize}

\subsubsection{Basic Calculation Example}
Using $J(\theta) = \theta^2$, imagine this represents the loss for sample $i$. If $\theta_t = 2.56$, then $g_t = \nabla J(2.56) = 5.12$. The update would be $\theta_{t+1} = 2.56 - 0.1 \times 5.12 = 2.048$. The next update would use the gradient based on the *next randomly selected sample*.

\subsection{Mini-batch Gradient Descent (MBGD)}

\subsubsection{Historical Context/Motivation}
This method emerged as a practical compromise between the stability of Batch GD and the speed of SGD. It computes the gradient and updates parameters based on a small, randomly selected batch of $b$ samples (e.g., $b=32, 64, 128$). This approach leverages hardware optimizations (vectorization, parallelism on GPUs) much better than SGD while being significantly faster than Batch GD. It's the de facto standard for training deep neural networks.

\subsubsection{Intuition}
Instead of using the whole dataset (Batch GD) or just one sample (SGD), use a small group (mini-batch). This gives a gradient estimate that is less noisy than SGD but much faster to compute than Batch GD. It provides a good balance, allowing for efficient computation via vectorization while maintaining reasonably stable convergence.

\subsubsection{Mathematical Formulation}
In each iteration $t$, select a mini-batch of $b$ samples $\mathcal{B}_t = \{(x^{(j)}, y^{(j)})\}_{j=i}^{i+b-1}$. Compute the average gradient over this batch: $g_t = \frac{1}{b} \sum_{j \in \mathcal{B}_t} \nabla_{\btheta} L(f(x^{(j)}; \btheta_t), y^{(j)})$. The update rule is:
\begin{equation}
    \btheta_{t+1} = \btheta_t - \eta g_t
\end{equation}

\subsubsection{Pros}
\begin{itemize}
    \item Reduces the variance of parameter updates compared to SGD, leading to more stable convergence.
    \item Computationally efficient; takes advantage of optimized matrix operations (vectorization) on modern hardware (CPUs/GPUs).
    \item Represents a good balance between Batch GD and SGD.
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item Introduces a new hyperparameter: the mini-batch size $b$.
    \item Still requires careful tuning of the learning rate $\eta$.
    * Convergence can still oscillate, though less than SGD.
\end{itemize}

\subsubsection{Basic Calculation Example}
Imagine sampling a batch of $b$ examples. Compute the loss gradient for each example in the batch. Average these gradients to get $g_t$. Apply the update rule $\btheta_{t+1} = \btheta_t - \eta g_t$.

\subsection{Adaptive Learning Rate Methods}
A major challenge with the methods above is choosing and tuning the learning rate $\eta$. A single, fixed learning rate might be too slow for some parameters or too fast for others, especially in high-dimensional or sparse problems. Adaptive methods dynamically adjust the learning rate during training, often on a per-parameter basis.

\subsection{AdaGrad (Adaptive Gradient Algorithm)}

\subsubsection{Historical Context/Motivation}
Developed by Duchi, Hazan, and Singer (2011) primarily to handle sparse data effectively (common in NLP). The idea was to give parameters associated with infrequent features larger updates (higher effective learning rate) and parameters associated with frequent features smaller updates.

\subsubsection{Intuition}
It adapts the learning rate for each parameter $\theta_j$ individually. It keeps track of the sum of squared historical gradients for each parameter. Parameters that have experienced large gradients in the past will have their learning rate reduced significantly. This helps infrequent parameters catch up by allowing them larger updates initially when their squared gradient sum is small.

\subsubsection{Mathematical Formulation}
Let $g_{t,j} = \nabla_{\theta_j} J(\theta_{t,j})$ be the gradient for parameter $j$ at step $t$. Accumulate squared gradients element-wise:
\begin{equation}
    G_{t,jj} = G_{t-1,jj} + g_{t,j}^2 \quad (\text{Diagonal matrix } G_t \text{ stores sums of squares})
\end{equation}
The update rule for each parameter $\theta_j$ is:
\begin{equation}
    \theta_{t+1, j} = \theta_{t, j} - \frac{\eta}{\sqrt{G_{t,jj} + \epsilon}} g_{t,j}
\end{equation}
where $\epsilon$ is a small smoothing constant (e.g., $10^{-8}$) to prevent division by zero. Note the per-parameter scaling $1/\sqrt{G_{t,jj}+\epsilon}$.

\subsubsection{Pros}
\begin{itemize}
    \item Automatically adapts learning rates per parameter.
    \item Performs well on problems with sparse gradients (e.g., NLP, recommendation systems).
    \item Less need to manually tune the learning rate $\eta$ (often default works ok).
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item The accumulation of squared gradients $G_t$ grows monotonically.
    \item This causes the learning rate to shrink continuously and eventually become infinitesimally small, potentially stopping learning prematurely before convergence.
\end{itemize}

\subsection{RMSProp (Root Mean Square Propagation)}

\subsubsection{Historical Context/Motivation}
Developed independently around the same time as AdaDelta, often attributed to Geoff Hinton in his Coursera lectures (unpublished). It directly addresses AdaGrad's problem of the monotonically decreasing learning rate by using an *exponentially decaying average* of squared gradients instead of summing them indefinitely.

\subsubsection{Intuition}
Similar to AdaGrad, it scales the learning rate per parameter. However, instead of letting the sum of squares grow forever, it uses a moving average, giving more weight to recent gradients. This means if gradients become small recently, the effective learning rate can increase again, preventing the learning from halting prematurely. It divides the learning rate by a root-mean-square of recent gradients.

\subsubsection{Mathematical Formulation}
Maintain an exponentially decaying average of squared gradients $E[g^2]_t$ for each parameter (element-wise):
\begin{equation}
    E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma) g_t^2
\end{equation}
where $g_t = \nabla_{\btheta} J(\btheta_t)$ and $\gamma$ is the decay rate (e.g., 0.9). The update rule is:
\begin{equation}
    \btheta_{t+1} = \btheta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
\end{equation}
(The operation $g_t^2$ and the division/sqrt are element-wise).

\subsubsection{Pros}
\begin{itemize}
    \item Resolves AdaGrad's vanishing learning rate issue by using a moving average.
    \item Adapts learning rates per parameter effectively.
    \item Often converges faster than AdaGrad.
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item Still requires tuning hyperparameters $\eta$ and $\gamma$.
\end{itemize}

\subsection{Adam (Adaptive Moment Estimation)}

\subsubsection{Historical Context/Motivation}
Introduced by Kingma and Ba (2014), Adam combines the ideas of adaptive learning rates (like RMSProp) with \emph{momentum}. Momentum methods maintain a moving average of the gradients themselves to help accelerate convergence along consistent directions and dampen oscillations. Adam computes adaptive learning rates for each parameter based on estimates of both the first moment (the mean, like momentum) and the second moment (the uncentered variance, like RMSProp) of the gradients.

\subsubsection{Intuition}
Adam is like a combination of Momentum and RMSProp. It keeps track of the average *direction* the gradients have been pointing recently (momentum, $m_t$) and the average *magnitude* (squared value) of recent gradients (variance estimate, $v_t$). It uses the momentum part to help push the parameters faster in consistent directions and the variance estimate part to adapt the learning rate per parameter (making smaller steps for parameters with high variance/magnitude). It also includes a bias-correction step to account for the fact that the moving averages start at zero, which is especially important early in training.

\subsubsection{Mathematical Formulation}
Maintain exponentially decaying averages of past gradients ($m_t$) and past squared gradients ($v_t$):
\begin{align}
    m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(First moment estimate)} \\
    v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(Second moment estimate)}
\end{align}
where $g_t = \nabla_{\btheta} J(\btheta_t)$, and $\beta_1, \beta_2$ are decay rates (e.g., $\beta_1=0.9, \beta_2=0.999$). Compute bias-corrected estimates:
\begin{align}
    \hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
    \hat{v}_t &= \frac{v_t}{1 - \beta_2^t}
\end{align}
The update rule is:
\begin{equation}
    \btheta_{t+1} = \btheta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{equation}
(Operations involving $g_t^2$, $\sqrt{\hat{v}_t}$, and the final update are element-wise).

\subsubsection{Pros}
\begin{itemize}
    \item Combines advantages of RMSProp (adaptive LR) and momentum (acceleration).
    \item Bias correction improves initial stability.
    \item Often converges quickly and works well on a wide variety of problems.
    \item Relatively insensitive to the choice of hyperparameters (default values often work well).
\end{itemize}

\subsubsection{Cons}
\begin{itemize}
    \item More complex algorithm with more moving parts.
    \item Introduces more hyperparameters ($\beta_1, \beta_2, \epsilon$, though defaults are often used).
    \item Some studies suggest Adam can sometimes converge to worse local minima than SGD with momentum on certain tasks, although it often gets there faster. Convergence properties are still an area of research.
\end{itemize}

\subsection{Gradient Decent Summary}
Gradient descent algorithms are fundamental to training machine learning models. Starting from Batch GD, limitations in computation and convergence led to SGD and Mini-batch GD. The challenge of tuning learning rates spurred the development of adaptive methods like AdaGrad, RMSProp, and Adam, which dynamically adjust updates based on gradient history. Adam, combining adaptive rates with momentum, is currently one of the most widely used and effective general-purpose optimizers, particularly in deep learning. However, the choice of optimizer can still depend significantly on the specific problem, dataset, and model architecture, and optimization remains an active area of research.

\subsection{Backpropagation Introduction}
Backpropagation, short for "backward propagation of errors," is the cornerstone algorithm for training artificial neural networks, particularly deep networks. It isn't an optimization algorithm itself (like Gradient Descent), but rather a highly efficient method for computing the gradients of a loss function with respect to all the weights and biases in the network. These gradients are then used by optimization algorithms to update the network's parameters and improve its performance. This document delves into the motivation, intuition, mathematical derivation, symbolism, and limitations of backpropagation.

\subsection{Motivation for Backpropagation}
Training a neural network typically involves minimizing a loss function $J(\btheta)$ that quantifies the difference between the network's predictions and the true target values. The parameters $\btheta$ (all the weights $\bW$ and biases $\bb$ in the network) are adjusted using gradient-based optimization methods like Stochastic Gradient Descent (SGD) or its variants (Adam, RMSProp). These methods require the gradient of the loss function with respect to every parameter in the network, $\nabla_{\btheta} J(\btheta)$.

Consider a deep neural network with potentially millions of parameters. Calculating the partial derivative $\partial J / \partial \theta_i$ individually for each parameter $\theta_i$ using standard numerical differentiation or symbolic differentiation for the entire complex function is computationally infeasible. Many parameters influence the final loss through shared intermediate computations in the network's layers. A naive approach would lead to immense redundant calculations. Backpropagation provides a way to compute all necessary gradients efficiently by leveraging the structure of the network and the chain rule of calculus, avoiding redundant computations.

\subsection{Intuition and Interpretation}

\subsubsection{Core Idea: Propagating Error Signals Backward}
Imagine the network performing a forward pass: input data flows through the layers, activations are computed, and finally, a prediction is made and compared to the true target, resulting in a loss value. Backpropagation takes this final loss value and works backward through the network:
\begin{itemize}
    \item It calculates how much the output layer's pre-activation values contributed to the loss.
    \item It then calculates how much the weights and biases of the output layer contributed to that error signal.
    \item Crucially, it propagates this error signal further backward, calculating how much the activations of the *previous* hidden layer contributed to the output layer's error signal.
    \item This process repeats layer by layer, propagating the error signal backward and calculating the gradients for weights and biases at each layer based on the error signal it receives from the layer ahead and the activations it received from the layer behind during the forward pass.
\end{itemize}

\subsubsection{Interpretation: Assigning Blame/Credit}
The gradient $\partial J / \partial w$ for a specific weight $w$ tells you how much the final loss $J$ would change for a tiny change in that weight $w$. Backpropagation effectively computes this sensitivity for all weights and biases. It quantifies the "blame" or "responsibility" of each parameter for the final error. Parameters that contributed significantly to the error will have larger magnitude gradients, indicating they need to be adjusted more by the optimization algorithm.

\subsubsection{Computational Graph View}
A neural network can be viewed as a large computational graph where nodes represent operations (matrix multiplication, bias addition, activation functions, loss calculation) and edges represent the flow of tensors (data, parameters, activations, gradients). The forward pass calculates the value of each node up to the final loss node. Backpropagation is essentially an application of \emph{reverse-mode automatic differentiation} on this graph. It starts from the final node (loss) and computes the gradient of the loss with respect to each node's output, then uses the chain rule locally at each node to compute the gradient with respect to its inputs, passing these gradients backward along the edges.

\subsection{Mathematical Derivation and Symbolism of Backpropagation}

Let's define the notation for a feedforward neural network with $L$ layers:
\begin{itemize}
    \item $l$: Layer index ($l=1, ..., L$). Layer $L$ is the output layer.
    \item $\bW^{(l)}$: Weight matrix for layer $l$.
    \item $\bb^{(l)}$: Bias vector for layer $l$.
    \item $\bz^{(l)}$: Pre-activation (logit) vector for layer $l$.
    \item $\ba^{(l)}$: Activation vector for layer $l$. $\ba^{(0)} = \bx$ (input).
    \item $f$: Activation function (e.g., Sigmoid, ReLU, Tanh). Applied element-wise.
    \item $J$: Loss function (scalar).
\end{itemize}
The forward pass is defined by:
\begin{align}
    \bz^{(l)} &= \bW^{(l)} \ba^{(l-1)} + \bb^{(l)} \\
    \ba^{(l)} &= f(\bz^{(l)})
\end{align}
The goal is to compute $\frac{\partial J}{\partial \bW^{(l)}}$ and $\frac{\partial J}{\partial \bb^{(l)}}$ for all $l=1, ..., L$.

The key insight of backpropagation is to first compute the gradient of the loss $J$ with respect to the pre-activation $\bz^{(l)}$ at each layer $l$. We denote this error signal as $\bdelta^{(l)}$:
\begin{equation}
    \bdelta^{(l)} \equiv \frac{\partial J}{\partial \bz^{(l)}}
\end{equation}
We compute these $\bdelta^{(l)}$ values backward, starting from the last layer $L$.

\textbf{1. Error at the Output Layer ($l=L$):}
Using the chain rule, the error signal $\bdelta^{(L)}$ depends on the derivative of the loss with respect to the final activation $\ba^{(L)}$ and the derivative of the activation function $f$ at $\bz^{(L)}$:
\begin{equation} \label{eq:delta_L}
    \bdelta^{(L)} = \frac{\partial J}{\partial \ba^{(L)}} \odot f'(\bz^{(L)}) = \frac{\partial J}{\partial \ba^{(L)}} \odot \frac{\partial \ba^{(L)}}{\partial \bz^{(l)}}
\end{equation}
Here, $\odot$ denotes element-wise multiplication (Hadamard product), and $f'(\bz^{(L)})$ is the element-wise derivative of the activation function evaluated at $\bz^{(L)}$. $\frac{\partial J}{\partial \ba^{(L)}}$ depends on the specific loss function used.

\textbf{2. Error at Hidden Layers ($l < L$):}
To find the error $\bdelta^{(l)}$ at layer $l$, we need to know how $\bz^{(l)}$ affects the loss $J$. It does so only through $\bz^{(l+1)}$ in the next layer. Applying the chain rule:
\begin{equation}
    \bdelta^{(l)} = \frac{\partial J}{\partial \bz^{(l)}} = \frac{\partial J}{\partial \bz^{(l+1)}} \frac{\partial \bz^{(l+1)}}{\partial \ba^{(l)}} \frac{\partial \ba^{(l)}}{\partial \bz^{(l)}}
\end{equation}
Recognizing the terms:
\begin{itemize}
    \item $\frac{\partial J}{\partial \bz^{(l+1)}} = \bdelta^{(l+1)}$ (error from the next layer).
    \item $\frac{\partial \bz^{(l+1)}}{\partial \ba^{(l)}} = \frac{\partial}{\partial \ba^{(l)}} (\bW^{(l+1)} \ba^{(l)} + \bb^{(l+1)}) = (\bW^{(l+1)})^T$ (transpose of weights projecting error back).
    \item $\frac{\partial \ba^{(l)}}{\partial \bz^{(l)}} = f'(\bz^{(l)})$ (element-wise activation derivative).
\end{itemize}
Combining these gives the recursive formula for the error signal:
\begin{equation} \label{eq:delta_l}
    \bdelta^{(l)} = ((\bW^{(l+1)})^T \bdelta^{(l+1)}) \odot f'(\bz^{(l)})
\end{equation}

\textbf{3. Gradients with Respect to Parameters ($\bW^{(l)}, \bb^{(l)}$):}
Once we have the error signal $\bdelta^{(l)}$ for a layer, we can find the gradients for its parameters using the chain rule again:
\begin{align}
    \frac{\partial J}{\partial \bW^{(l)}} &= \frac{\partial J}{\partial \bz^{(l)}} \frac{\partial \bz^{(l)}}{\partial \bW^{(l)}} = \bdelta^{(l)} (\ba^{(l-1)})^T \label{eq:grad_W} \\
    \frac{\partial J}{\partial \bb^{(l)}} &= \frac{\partial J}{\partial \bz^{(l)}} \frac{\partial \bz^{(l)}}{\partial \bb^{(l)}} = \bdelta^{(l)} \cdot 1 = \bdelta^{(l)} \label{eq:grad_b}
\end{align}
(Note: $\frac{\partial \bz^{(l)}}{\partial \bW^{(l)}} = (\ba^{(l-1)})^T$ and $\frac{\partial \bz^{(l)}}{\partial \bb^{(l)}} = 1$ using matrix calculus rules).

The backpropagation algorithm involves a forward pass to compute activations $a^{(l)}$ and pre-activations $z^{(l)}$ (storing them), followed by a backward pass starting with Eq.~\ref{eq:delta_L} to compute $\delta^{(L)}$, then using Eq.~\ref{eq:delta_l} recursively for $l=L-1, ..., 1$ to find all $\delta^{(l)}$, and simultaneously using Eq.~\ref{eq:grad_W} and Eq.~\ref{eq:grad_b} to find the desired parameter gradients at each layer.

\subsection{Simple Backpropagation Calculation Example (Linear Layers)}
Let's use a network with linear activations ($f(z)=z$, so $f'(z)=1$) to focus on the backprop flow.
\begin{itemize}
    \item Network: 1 input ($x$) $\to$ 2 hidden neurons ($a^{(1)}$) $\to$ 1 output ($a^{(2)}$).
    \item Input: $\bx = [1.0]$ (so $a^{(0)} = [1.0]$)
    \item Target: $y_{true} = 0.5$
    \item Parameters (column vectors for biases):
        $\bW^{(1)} = \begin{bmatrix} 0.2 \\ 0.3 \end{bmatrix}$, $\bb^{(1)} = \begin{bmatrix} 0.1 \\ -0.1 \end{bmatrix}$
        $\bW^{(2)} = \begin{bmatrix} 0.5 & -0.5 \end{bmatrix}$, $\bb^{(2)} = [0.2]$
    \item Loss: $J = \frac{1}{2} (a^{(2)} - y_{true})^2$
\end{itemize}

\textbf{Forward Pass:}
\begin{enumerate}
    \item $\bz^{(1)} = \bW^{(1)} \ba^{(0)} + \bb^{(1)} = \begin{bmatrix} 0.2 \\ 0.3 \end{bmatrix} [1.0] + \begin{bmatrix} 0.1 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.3 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 0.3 \\ 0.2 \end{bmatrix}$
    \item $\ba^{(1)} = f(\bz^{(1)}) = \bz^{(1)} = \begin{bmatrix} 0.3 \\ 0.2 \end{bmatrix}$ (since $f$ is linear)
    \item $\bz^{(2)} = \bW^{(2)} \ba^{(1)} + \bb^{(2)} = \begin{bmatrix} 0.5 & -0.5 \end{bmatrix} \begin{bmatrix} 0.3 \\ 0.2 \end{bmatrix} + [0.2] = [(0.5)(0.3) + (-0.5)(0.2)] + [0.2] = [0.15 - 0.10] + [0.2] = [0.05] + [0.2] = [0.25]$
    \item $\ba^{(2)} = f(\bz^{(2)}) = \bz^{(2)} = [0.25]$ (prediction $y_{pred}$)
    \item $J = \frac{1}{2} (0.25 - 0.5)^2 = \frac{1}{2} (-0.25)^2 = \frac{1}{2} (0.0625) = 0.03125$
\end{enumerate}

\textbf{Backward Pass:}
\begin{enumerate}
    \item Calculate $\frac{\partial J}{\partial a^{(2)}} = (a^{(2)} - y_{true}) = (0.25 - 0.5) = [-0.25]$
    \item Calculate $\bdelta^{(2)} = \frac{\partial J}{\partial a^{(2)}} \odot f'(z^{(2)}) = [-0.25] \odot [1] = [-0.25]$ (since $f'(z)=1$)
    \item Calculate gradients for layer 2:
        $\frac{\partial J}{\partial \bW^{(2)}} = \bdelta^{(2)} (\ba^{(1)})^T = [-0.25] \begin{bmatrix} 0.3 & 0.2 \end{bmatrix} = \begin{bmatrix} (-0.25)(0.3) & (-0.25)(0.2) \end{bmatrix} = \begin{bmatrix} -0.075 & -0.05 \end{bmatrix}$
        $\frac{\partial J}{\partial \bb^{(2)}} = \bdelta^{(2)} = [-0.25]$
    \item Calculate $\bdelta^{(1)} = ((\bW^{(2)})^T \bdelta^{(2)}) \odot f'(\bz^{(1)})$
        $(\bW^{(2)})^T \bdelta^{(2)} = \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix} [-0.25] = \begin{bmatrix} (0.5)(-0.25) \\ (-0.5)(-0.25) \end{bmatrix} = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix}$
        $\bdelta^{(1)} = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix} \odot \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix}$ (since $f'(z)=1$)
    \item Calculate gradients for layer 1:
        $\frac{\partial J}{\partial \bW^{(1)}} = \bdelta^{(1)} (\ba^{(0)})^T = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix} [1.0]^T = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix} [1.0] = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix}$
        $\frac{\partial J}{\partial \bb^{(1)}} = \bdelta^{(1)} = \begin{bmatrix} -0.125 \\ 0.125 \end{bmatrix}$
\end{enumerate}
Now we have the gradients $\frac{\partial J}{\partial \bW^{(1)}}, \frac{\partial J}{\partial \bb^{(1)}}, \frac{\partial J}{\partial \bW^{(2)}}, \frac{\partial J}{\partial \bb^{(2)}}$, which can be used by an optimizer (like SGD) to update the parameters.

\subsection{Applicability Beyond Fully Connected Layers}
The power of backpropagation lies in its generality, stemming from the chain rule. It works as long as the operations within the network layers are (almost everywhere) differentiable.

\begin{itemize}
    \item \textbf{Convolutional Layers:} Convolution is a linear operation (a sum of products involving input patches and filter weights). Operations like padding and stride selection structure the computation, but the core operation is differentiable with respect to both the input and the filter weights. Pooling layers (like MaxPooling) are non-linear but piece-wise linear; sub-gradients can be used where the function is not strictly differentiable (in practice, the gradient is typically passed through the element that was the maximum). Backpropagation computes gradients for the filters (kernels) and propagates the error signal back to the layer's input (sometimes conceptualized via related operations like transposed convolutions).
    \item \textbf{Attention Blocks:} Self-attention mechanisms (common in Transformers) involve operations like creating Query, Key, and Value vectors via matrix multiplication, calculating attention scores (e.g., scaled dot-product), applying masking, using Softmax to get attention weights, and computing a weighted sum of Value vectors. All these core operations (matrix multiplication, scaling, addition, Softmax, element-wise multiplication for weighting) are differentiable. Backpropagation applies the chain rule through this sequence of operations, allowing the network to learn the parameters involved in the Q, K, V projections and any subsequent feedforward layers.
\end{itemize}

\subsection{Limitations of Backpropagation}
While powerful, backpropagation has limitations and scenarios where it doesn't directly apply or faces challenges:

\begin{itemize}
    \item \textbf{Non-Differentiable Operations:} If a network includes components that are fundamentally non-differentiable (e.g., hard thresholding, sampling from a discrete distribution, operations involving true `argmax`), the gradient flow stops. Workarounds like using "straight-through estimators" for discrete variables or sub-gradients for functions like ReLU/MaxPool exist but can be heuristics.
    \item \textbf{Discrete States/Actions:} In areas like Reinforcement Learning (RL) or models with discrete latent variables, the decisions or states themselves are discrete. Backpropagation cannot directly compute gradients through these discrete choices. Techniques like policy gradients (REINFORCE in RL), which use sampling and likelihood ratios, or reparameterization tricks combined with continuous relaxations (like Gumbel-Softmax) are needed.
    \item \textbf{Vanishing/Exploding Gradients:} In very deep networks or with certain activation functions (like sigmoid/tanh), the repeated multiplication during the backward pass can cause gradients to shrink exponentially towards zero (vanishing) or grow exponentially towards infinity (exploding). This hinders learning. This isn't a failure of the backprop *algorithm* itself, but a numerical challenge it faces. Modern techniques like ReLU activations, residual connections, normalization layers (Batch Norm, Layer Norm), and careful initialization are crucial for mitigating these issues.
    \item \textbf{Requirement for a Differentiable Loss \& Supervision:}** Standard backprop relies on having a differentiable loss function computed based on readily available supervisory signals (labels, target values) or reconstruction targets. It's less straightforward to apply in unsupervised settings without a clear reconstruction or probabilistic objective, or in RL where rewards are often sparse, delayed, and not directly differentiable with respect to actions.
    \item \textbf{Biological Plausibility:}** While neural networks draw inspiration from the brain, the backpropagation mechanism (requiring precise, symmetric backward transmission of error signals) is generally not considered a plausible model of how biological neurons learn.
\end{itemize}

\subsection{Backpropagation Conclusion}
Backpropagation is a remarkably elegant and efficient algorithm based on the chain rule of calculus, enabling the computation of gradients required to train complex, deep neural networks using gradient-based optimization. Its ability to systematically assign credit or blame to millions of parameters by propagating error signals backward makes large-scale deep learning feasible. While applicable to a wide range of differentiable architectures, including convolutional and attention-based networks, it faces challenges with non-differentiable components, discrete choices, and numerical stability, necessitating specialized techniques or architectural modifications in certain domains. Understanding backpropagation remains crucial for anyone working deeply with neural network training and development.
















\section{Common Model Benchmarking}

\subsection{Introduction: Why Benchmark?}

In the rapidly evolving field of deep learning, **benchmarking** refers to the standardized process of evaluating and comparing the performance of different models or algorithms on specific tasks using common datasets and metrics. It serves several critical purposes:
\begin{itemize}
    \item \textbf{Measuring Progress:} Provides a quantitative way to track improvements in model capabilities over time.
    \item \textbf{Model Selection:} Helps researchers and practitioners choose the most suitable model architecture or training strategy for a given task.
    \item \textbf{Assessing Generalization:} Evaluates how well models perform on new, unseen data, which is the ultimate goal of most machine learning applications.
    \item \textbf{Identifying Overfitting:} Helps detect when a model has learned the training data too well at the expense of generalization.
    \item \textbf{Reproducibility and Comparison:} Creates a common ground for fairly comparing results across different research groups and publications.
\end{itemize}
Understanding benchmarking practices requires familiarity with core machine learning concepts related to model evaluation.

\subsection{Core Concepts in Model Evaluation}

Benchmarking is intrinsically linked to assessing several fundamental properties of machine learning models:

\begin{itemize}
    \item \textbf{Generalization:} This is arguably the most crucial concept. Generalization refers to a model's ability to perform well on previously unseen data drawn from the same underlying distribution as the training data. A good benchmark score on a held-out test set is our primary measure of generalization ability.
    \item \textbf{Overfitting:} A model overfits when it learns spurious patterns or noise specific to the training data, rather than the underlying generalizable patterns. Such a model performs very well on the training data but poorly on new data (i.e., it fails to generalize). Benchmarking, specifically using separate validation and test sets, is essential for detecting overfitting (indicated by a large gap between training performance and validation/test performance).
    \item \textbf{Underfitting:} This occurs when a model is too simple or lacks the necessary complexity (\textbf{model capacity}) to capture the underlying structure of the data. An underfit model performs poorly on both the training data and new data. Low scores on benchmark datasets usually indicate underfitting (or other training issues).
    \item \textbf{Model Capacity:} Refers to the flexibility of a model or the complexity of the function space it can represent (influenced by factors like the number of parameters, layers, type of architecture). Higher capacity models can fit more complex patterns but are also more prone to overfitting. Benchmarking helps find a model with appropriate capacity – one that minimizes generalization error, not just training error.
    \item \textbf{Error and the Bias-Variance Trade-off:} The generalization error of a model can be conceptually decomposed (though hard to measure precisely) into bias, variance, and irreducible error.
        \begin{itemize}
            \item \textbf{Bias:} Error due to incorrect assumptions in the model (e.g., assuming a linear relationship when it's non-linear). High bias leads to underfitting.
            \item \textbf{Variance:} Error due to the model's sensitivity to small fluctuations in the training data. High variance leads to overfitting.
        \end{itemize}
    Benchmarking aims to estimate the total generalization error. Observing performance on training, validation, and test sets helps diagnose whether poor performance is due to high bias (low scores everywhere) or high variance (good training score, poor validation/test score).
\end{itemize}

\subsection{Common Benchmarking Practices}

To obtain reliable estimates of model performance and generalization ability, several standard practices are employed:

\subsubsection{Train/Validation/Test Split}
\begin{itemize}
    \item \textbf{Motivation:} Using the same data to train a model and evaluate its final performance gives an overly optimistic and biased estimate of how it will perform on new data. We need separate datasets for different stages of model development.
    \item \textbf{Process:} The available dataset is typically split into three disjoint subsets:
        \begin{enumerate}
            \item \textbf{Training Set:} Used to fit the model parameters (weights and biases) by minimizing the loss function (e.g., via backpropagation and gradient descent). (Commonly 60-80\% of the data).
            \item \textbf{Validation Set (or Development Set):} Used during training to tune hyperparameters (e.g., learning rate, network architecture choices, regularization strength). The model's performance on the validation set guides these choices. It provides an unbiased estimate of performance *during* development. (Commonly 10-20\% of the data).
            \item \textbf{Test Set:} Used \emph{only once} after all training and hyperparameter tuning is complete to obtain a final, unbiased estimate of the model's generalization performance on unseen data. This is the number typically reported in papers as the benchmark result. (Commonly 10-20\% of the data).
        \end{enumerate}
    \item \textbf{Mathematical Idea:} We aim to estimate the expected loss on new data drawn from the true data distribution $P_{\text{data}}$: $E_{(\bx,\by) \sim P_{\text{data}}}[L(f(\bx;\btheta^*), \by)]$. The training set is used to find candidate parameters $\btheta$. The validation set helps select the best hyperparameters or model checkpoint ($\btheta^*$) without directly optimizing for the test set. The test set provides an unbiased sample estimate of the true generalization error for the chosen $\btheta^*$.
\end{itemize}

\subsubsection{Cross-Validation (k-fold)}
\begin{itemize}
    \item \textbf{Motivation:} When data is limited, a single train/validation/test split might be sensitive to how the split was made. k-fold cross-validation provides a more robust estimate of generalization performance by using data more efficiently.
    \item \textbf{Process:}
        \begin{enumerate}
            \item Randomly partition the data (excluding any final held-out test set) into $k$ equal-sized folds (e.g., $k=5$ or $k=10$).
            \item For each fold $i$ from 1 to $k$:
                \begin{itemize}
                    \item Use fold $i$ as the validation set.
                    \item Use the remaining $k-1$ folds as the training set.
                    \item Train the model and evaluate its performance on the validation fold $i$.
                \end{itemize}
            \item The overall performance is typically reported as the average performance across the $k$ validation folds. Hyperparameters can be chosen based on this average performance.
        \end{enumerate}
    \item \textbf{Mathematical Idea:} By averaging the performance over $k$ different validation sets, cross-validation reduces the variance associated with a single random split, giving a more reliable estimate of how the model is likely to perform on average on unseen data. The estimate is $\text{Score}_{CV} = \frac{1}{k} \sum_{i=1}^k \text{ValidationScore}_{\text{fold } i}$.
\end{itemize}

\subsubsection{Standardized Datasets and Tasks}
\begin{itemize}
    \item \textbf{Motivation:} To meaningfully compare different models or algorithms, they must be evaluated on the same task using the same data and evaluation procedures.
    \item \textbf{Examples:} The field relies heavily on widely accepted benchmark datasets like \texttt{MNIST}, \texttt{CIFAR-10/100}, \texttt{ImageNet} (image classification), \texttt{COCO} (object detection), \texttt{SQuAD} (question answering), \texttt{GLUE}/\texttt{SuperGLUE} (natural language understanding), \texttt{LibriSpeech} (speech recognition), etc.
    \item \textbf{Importance:} These datasets come with predefined splits (or protocols for creating them), specific tasks (e.g., classify images into 1000 categories), and established evaluation metrics, allowing for direct comparison of reported results. Leaderboards often track state-of-the-art performance on these benchmarks.
\end{itemize}

\subsubsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{Motivation:} Need quantitative ways to measure performance that are relevant to the task objectives.
    \item \textbf{Examples \& Mathematical Ideas:} The choice depends heavily on the task:
        \begin{itemize}
            \item \textbf{Accuracy (Classification):} $\text{Acc} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}$. Simple, but can be misleading if classes are imbalanced.
            \item \textbf{Precision, Recall, F1-Score (Classification):} Used when distinguishing between types of errors (False Positives FP, False Negatives FN) is important, especially with imbalanced data. Let TP = True Positives.
                \begin{itemize}
                    \item Precision $= \frac{TP}{TP + FP}$
                    \item Recall (Sensitivity) $= \frac{TP}{TP + FN}$
                    \item F1-Score $= 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ (Harmonic mean)
                \end{itemize}
            \item \textbf{AUC-ROC (Binary Classification):} Area Under the Receiver Operating Characteristic Curve plots True Positive Rate (Recall) vs. False Positive Rate at various decision thresholds. Represents the probability that the model ranks a random positive example higher than a random negative example. Value between 0 and 1 (0.5 is random guessing, 1 is perfect). Calculated via integration.
            \item \textbf{Mean Squared Error (MSE) (Regression):} $MSE = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$. Penalizes large errors quadratically. Corresponds to MLE under Gaussian noise.
            \item \textbf{Mean Absolute Error (MAE) (Regression):} $MAE = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|$. Less sensitive to outliers than MSE. Corresponds to MLE under Laplacian noise.
            \item \textbf{Perplexity (PPL) (Language Modeling):} Often defined as $PPL = \exp(H)$, where $H$ is the cross-entropy loss per word. $H = -\frac{1}{N} \sum_{i=1}^N \log p(w_i | \text{context}_i)$. Lower PPL indicates the model assigns higher probability to the true sequence of words.
            \item \textbf{BLEU, ROUGE (Translation, Summarization):} Compare generated text to one or more reference texts based on n-gram overlap, often incorporating brevity penalties. Formulas involve counts of matching n-grams.
        \end{itemize}
\end{itemize}

\subsection{Considerations and Caveats}

While essential, benchmark results should be interpreted with caution:
\begin{itemize}
    \item \textbf{Benchmark Saturation:} Achieving very high scores on a benchmark doesn't always mean the underlying task is "solved". Models might exploit statistical artifacts or biases in the dataset rather than learning the intended generalizable skill.
    \item \textbf{Data Leakage:} It's crucial to ensure no information from the validation or test sets inadvertently influences the training process (e.g., using test set statistics for normalization).
    \item \textbf{Hyperparameter Tuning on Test Set:} A common mistake is to tune hyperparameters based on test set performance. This invalidates the test set as an unbiased estimate of generalization. Any tuning must use only the training and validation sets.
    \item \textbf{Computational Cost \& Resources:} Comparing models purely on final metrics can be misleading if one model requires vastly more computational resources (time, hardware, energy) to train or run. Reporting computational budgets or efficiency metrics is increasingly important.
    \item \textbf{Distribution Shift:} Performance on a standardized benchmark dataset (drawn from one distribution) may not directly reflect performance on a specific real-world application where the data distribution might differ (domain shift, covariate shift). Further testing on in-domain data is often necessary.
\end{itemize}

\subsection{Benchmarking Conclusion}

Benchmarking is a vital practice in deep learning for measuring progress, comparing models, and understanding generalization. It relies on core concepts like generalization vs. overfitting and leverages standard methodologies like train/validation/test splits or cross-validation, standardized datasets, and appropriate evaluation metrics. While benchmark scores provide valuable insights, it's important to perform evaluations rigorously, avoid common pitfalls like data leakage or improper use of the test set, and be aware that benchmark performance may not always perfectly predict real-world applicability due to potential distribution shifts and dataset limitations.




\newpage

\part{Examples: Deep Dives}

\section{Denoise Diffusion Models}

This section breaks down the forward/reverse process sampling step $p_\theta(x_{t-1} | x_t)$ in Denoising Diffusion Probabilistic Models (DDPMs) into its detailed mathematical manipulations. The goal is, given the noisy sample $\bx_t$ at timestep $t$ and our trained noise prediction network $\bepsilon_\theta$, how do we calculate the sample $\bx_{t-1}$ for the previous timestep? I want to quickly note that DDPMs can be used as data manipulation to change the target of our model. This means that we can use any kind of nueral network architecture we want to try to learn this data manipulation. This will be important to keep in mind as you follow along.

\subsection{Preliminaries and Goal}

We start with the definitions from the forward process:
\begin{itemize}
    \item $\beta_t$: Variance schedule for $t=1, ..., T$. $T$ corresponds to the fully noised state.
    \item $\alpha_t = 1 - \beta_t$.
    \item $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.
\end{itemize}
Our goal is to sample $\bx_{t-1}$ from the learned reverse transition distribution $p_\theta(\bx_{t-1} | \bx_t)$.

\subsection{The Forward Process}
The idea is to take advantage of a latent space (continuous unseen variable that encodes structure) to randomly sample noise and them produce a new sample that shares commonalities with the actual dataset. To do this, we need to get it into the latent space. This could be done in one jump like in variantional autoencoders, but instead, we do it slowly over the course of many little steps by adding small amounts of noise. The reason for this is we are going to utilize ideas from non-EQ thermodynamics to produce our latent space. We are basically going to start with an unnoised sample pulled from a distribution which is akin to our non-equilibrium state (low entropy). We will progressively add a small amount of noise over iterations until it becomes just Gaussian noise (high entropy). This is akin to a non-equilibrium thermodynamics system that we allow to equilibrate. The path it will take will increase its entropy (second law of thermodynamics) and end in equilibrium (which follows a Guassian distribution). This is not the only connection to diffusion. Diffusion, which is the spreading out of molecules from higher concentration to lower, is driven by the molecules bumping into one another which is a kinetic process. If you remember back to your kinetics class from physics, you can completely describe where all the particles in a system (molecules) will be if you know all their current positions and velocities. If however, you only know $99\%$ of the positions and velocities of the particles then you can only probabilistically talk about where each particle will be a short time later. This can be modelled as a Markov Process (meaning that the next condition is only influenced by the previous position). We will use this type of process to describe 

\subsection{Defining the Learned Reverse Transition}

Remember that we are trying to utilize either maximum likelihood estimation or MAP. So we want to get a probability function to work with so we can use these cost function generating principles. We model the reverse transition as a Gaussian distribution, primarily because the true (but intractable) reverse posterior $q(\bx_{t-1}|\bx_t, \bx_0)$ is known to be Gaussian under the forward process definition (this is utilizing the assumption that the change in the Shannon entropy between steps is small - think non-EQ thermo dynamics):
\begin{equation}
    p_\theta(\bx_{t-1} | \bx_t) = \mathcal{N}(\bx_{t-1} \,|\, \bmu_\theta(\bx_t, t), \sigma_t^2 \bI)
\end{equation}
The task boils down to defining the mean $\bmu_\theta(\bx_t, t)$ and variance $\sigma_t^2 \bI$.

\subsection{Determining the Variance $\sigma_t^2$}

The variance is typically chosen as a fixed (non-learned) value based on the forward process variance schedule. Common choices are:
\begin{itemize}
    \item \textbf{Choice 1 (Matching True Posterior Variance):} Set $\sigma_t^2$ equal to the variance of $q(\bx_{t-1} | \bx_t, \bx_0)$, which is $\tilde{\beta}_t$:
      \begin{equation}
          \sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
      \end{equation}
      Notice how the fraction before the $\beta_t$ term is nearly $1$ since we do not expect the scheduler to vary much between points.
    \item \textbf{Choice 2 (Simpler Forward Process Variance):} Set $\sigma_t^2$ equal to the variance added in the corresponding forward step $\beta_t$:
      \begin{equation}
          \sigma_t^2 = \beta_t
      \end{equation}
\end{itemize}
Both choices work well in practice. The DDPM paper showed that learning the variance provides minimal benefit, so fixing it is standard. Let's denote the chosen standard deviation as $\sigma_t$.

\subsection{Parameterizing and Learning the Mean}

This is the core part where the neural network comes in. As derived previously (by linking the desired learned mean to the form of the true posterior mean $\tilde{\bmu}_t(\bx_t, \bx_0)$), the mean $\bmu_\theta$ is parameterized in terms of the current noisy sample $\bx_t$ and the output of the noise prediction network $\bepsilon_\theta(\bx_t, t)$:
\begin{equation} \label{eq:mean_param}
    \bmu_\theta(\bx_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \bx_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bepsilon_\theta(\bx_t, t) \right)
\end{equation}
The network $\bepsilon_\theta(\bx_t, t)$ is trained (as described before) to predict the noise $\bepsilon$ that was added during the forward process to get $\bx_t$ from the original $\bx_0$.

\subsection{The Sampling Step (Putting it Together)}

To sample $\bx_{t-1}$ from the distribution $\mathcal{N}(\bx_{t-1} \,|\, \bmu_\theta(\bx_t, t), \sigma_t^2 \bI)$, we use the standard method for sampling from a Gaussian: take the mean and add noise scaled by the standard deviation.

First, sample a random noise vector from a standard Gaussian distribution:
\begin{equation}
    \bz \sim \mathcal{N}(0, \bI)
\end{equation}
However, for the very last step ($t=1$), we typically don't add noise, so we set $\bz = \mathbf{0}$ if $t=1$.

Then, the sample $\bx_{t-1}$ is calculated as:
\begin{equation} \label{eq:sampling_step}
    \bx_{t-1} = \bmu_\theta(\bx_t, t) + \sigma_t \bz
\end{equation}

\subsection{Explicit Calculation for One Reverse Step}

Let's substitute the formula for $\bmu_\theta(\bx_t, t)$ (Eq.~\ref{eq:mean_param}) into the sampling step (Eq.~\ref{eq:sampling_step}). This gives the full calculation performed at each timestep $t$ (from $T$ down to 1) during generation:

\begin{enumerate}
    \item \textbf{Input:} Current noisy sample $\bx_t$, current timestep $t$.
    \item \textbf{Predict Noise:} Use the trained neural network $\bepsilon_\theta$ to predict the noise component in $\bx_t$:
        \begin{equation*}
            \hat{\bepsilon} = \bepsilon_\theta(\bx_t, t)
        \end{equation*}
    \item \textbf{Calculate Mean:} Compute the mean $\bmu$ using the predicted noise $\hat{\bepsilon}$ and the known schedule parameters:
        \begin{equation*}
            \bmu = \frac{1}{\sqrt{\alpha_t}} \left( \bx_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\bepsilon} \right)
        \end{equation*}
    \item \textbf{Get Standard Deviation:} Determine the standard deviation $\sigma_t$ based on the chosen variance schedule (e.g., $\sigma_t = \sqrt{\tilde{\beta}_t}$ or $\sigma_t = \sqrt{\beta_t}$).
    \item \textbf{Sample Random Noise:} Sample $\bz \sim \mathcal{N}(0, \bI)$ if $t > 1$, otherwise set $\bz = \mathbf{0}$.
    \item \textbf{Compute $\bx_{t-1}$:} Combine the mean and the scaled random noise:
        \begin{equation*}
             \bx_{t-1} = \bmu + \sigma_t \bz
        \end{equation*}
        Substituting $\bmu$:
        \begin{equation} \label{eq:full_reverse_step}
             \boxed{\bx_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \bx_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bepsilon_\theta(\bx_t, t) \right) + \sigma_t \bz}
        \end{equation}
\end{enumerate}
This final boxed equation (Eq.~\ref{eq:full_reverse_step}) represents the complete mathematical manipulation performed at each step of the reverse diffusion process to generate the sample $\bx_{t-1}$ from $\bx_t$. It uses the neural network's noise prediction to estimate the mean of the slightly less noisy previous state and then samples from the distribution around that mean.




% ------------------------------------------------------------------------------

\end{document}
